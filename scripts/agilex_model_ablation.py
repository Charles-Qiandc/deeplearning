import os, sys

import numpy as np
import torch
from PIL import Image
from torchvision import transforms

from configs.state_vec import STATE_VEC_IDX_MAPPING

from pathlib import Path

# get current workspace
current_file = Path(__file__)
sys.path.append(os.path.join(current_file.parent.parent, "models"))

from multimodal_encoder.siglip_encoder import SiglipVisionTower
from multimodal_encoder.t5_encoder import T5Embedder
from multimodal_encoder.dinov2_encoder import create_dinov2_encoder
from multimodal_encoder.depth_encoder import create_depth_encoder
from rdt_runner_ablation import RDTRunnerAblation


def create_model(args, **kwargs):
    """åˆ›å»ºæ¶ˆèå®éªŒç‰ˆæœ¬çš„RDTæ¨¡å‹"""
    left_arm_dim, right_arm_dim = (
        args["arm_dim"]["left_arm_dim"],
        args["arm_dim"]["right_arm_dim"],
    )
    model = RoboticDiffusionTransformerModelAblation(args, **kwargs)
    pretrained = kwargs.get("pretrained", None)
    if pretrained is not None and os.path.isfile(pretrained):
        model.load_pretrained_weights(pretrained)

    return model


class RoboticDiffusionTransformerModelAblation(object):
    """
    æ¶ˆèå®éªŒç‰ˆRDTæ¨¡å‹åŒ…è£…å™¨
    é›†æˆè¾“å…¥ä¾§å¤šæ¨¡æ€è§†è§‰ç‰¹å¾èåˆ
    """

    def __init__(
        self,
        args,
        device="cuda",
        dtype=torch.bfloat16,
        image_size=None,
        control_frequency=25,
        pretrained=None,
        pretrained_vision_encoder_name_or_path=None,
    ):
        self.args = args
        self.dtype = dtype
        self.image_size = image_size
        self.device = device
        self.control_frequency = control_frequency
        self.image_processor, self.vision_model = self.get_vision_encoder(pretrained_vision_encoder_name_or_path)
        
        # ğŸ†• åˆ›å»ºé¢å¤–çš„è§†è§‰ç¼–ç å™¨
        self.dinov2_encoder = None
        self.depth_encoder = None
        
        # ä»é…ç½®ä¸­è¯»å–æ˜¯å¦ä½¿ç”¨é¢å¤–ç‰¹å¾
        self.use_dinov2_features = args.get("use_dinov2_features", False)
        self.use_depth_features = args.get("use_depth_features", False)
        
        if self.use_dinov2_features:
            print("ğŸ”§ åŠ è½½DINOv2ç¼–ç å™¨ç”¨äºæ¨ç†...")
            self.dinov2_encoder = create_dinov2_encoder(model_size="large", select_feature="cls_only")
            self.dinov2_encoder.to(device, dtype=dtype)
            
        if self.use_depth_features:
            print("ğŸ”§ åŠ è½½DepthAnythingç¼–ç å™¨ç”¨äºæ¨ç†...")
            self.depth_encoder = create_depth_encoder(
                model_size="metric_large",
                feature_dim=1024,
                device=device,
                use_metric_model=True
            )
            self.depth_encoder.to(device, dtype=dtype)
        
        self.policy = self.get_policy(pretrained)
        
        self.left_arm_dim, self.right_arm_dim = (
            args["arm_dim"]["left_arm_dim"],
            args["arm_dim"]["right_arm_dim"],
        )

        self.reset()

    def get_policy(self, pretrained):
        """åˆå§‹åŒ–æ¶ˆèå®éªŒç‰ˆç­–ç•¥æ¨¡å‹"""
        if pretrained is None or os.path.isfile(pretrained):
            img_cond_len = (self.args["common"]["img_history_size"] * self.args["common"]["num_cameras"] *
                            self.vision_model.num_patches)

            _model = RDTRunnerAblation(
                action_dim=self.args["common"]["state_dim"],
                pred_horizon=self.args["common"]["action_chunk_size"],
                config=self.args["model"],
                lang_token_dim=self.args["model"]["lang_token_dim"],
                img_token_dim=self.args["model"]["img_token_dim"],
                state_token_dim=self.args["model"]["state_token_dim"],
                max_lang_cond_len=self.args["dataset"]["tokenizer_max_length"],
                img_cond_len=img_cond_len,
                img_pos_embed_config=[
                    (
                        "image",
                        (
                            self.args["common"]["img_history_size"],
                            self.args["common"]["num_cameras"],
                            -self.vision_model.num_patches,
                        ),
                    ),
                ],
                lang_pos_embed_config=[
                    ("lang", -self.args["dataset"]["tokenizer_max_length"]),
                ],
                dtype=self.dtype,
                # ğŸ†• æ¶ˆèå®éªŒå‚æ•°
                use_dinov2_features=self.use_dinov2_features,
                dinov2_feature_dim=1024,
                use_depth_features=self.use_depth_features,
                depth_feature_dim=1024,
                visual_fusion_strategy=self.args.get("visual_fusion_strategy", "concat"),
            )
        else:
            raise NotImplementedError("ä»hubåŠ è½½æ¶ˆèç‰ˆæ¨¡å‹æš‚æœªå®ç°")

        return _model

    def get_vision_encoder(self, pretrained_vision_encoder_name_or_path):
        vision_encoder = SiglipVisionTower(vision_tower=pretrained_vision_encoder_name_or_path, args=None)
        image_processor = vision_encoder.image_processor
        return image_processor, vision_encoder

    def reset(self):
        """è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼"""
        device = self.device
        weight_dtype = self.dtype
        self.policy.eval()
        self.vision_model.eval()
        
        if self.dinov2_encoder is not None:
            self.dinov2_encoder.eval()
        if self.depth_encoder is not None:
            self.depth_encoder.eval()

        self.policy = self.policy.to(device, dtype=weight_dtype)
        self.vision_model = self.vision_model.to(device, dtype=weight_dtype)
        
        if self.dinov2_encoder is not None:
            self.dinov2_encoder = self.dinov2_encoder.to(device, dtype=weight_dtype)
        if self.depth_encoder is not None:
            self.depth_encoder = self.depth_encoder.to(device, dtype=weight_dtype)

    def load_pretrained_weights(self, pretrained):
        """å®‰å…¨åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆæ¶ˆèç‰ˆæœ¬ï¼‰"""
        try:
            print(f"ğŸ”§ å¼€å§‹åŠ è½½æ¶ˆèå®éªŒæƒé‡: {pretrained}")
            checkpoint = torch.load(pretrained, map_location='cpu')
            print(f"âœ… CheckpointåŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(checkpoint)} ä¸ªé¡¶çº§é”®")
            
            # è·å–å½“å‰æ¨¡å‹çš„å‚æ•°å
            model_state_dict = self.policy.state_dict()
            model_keys = set(model_state_dict.keys())
            
            print(f"ğŸ“‹ å½“å‰æ¨¡å‹åŒ…å« {len(model_keys)} ä¸ªå‚æ•°")
            
            # è¿‡æ»¤checkpointä¸­çš„å‚æ•°
            filtered_checkpoint = {}
            skipped_params = []
            
            for key, value in checkpoint.items():
                if key in model_keys:
                    # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦åŒ¹é…
                    if value.shape == model_state_dict[key].shape:
                        filtered_checkpoint[key] = value
                    else:
                        skipped_params.append(f"{key} (å½¢çŠ¶ä¸åŒ¹é…: {value.shape} vs {model_state_dict[key].shape})")
                else:
                    # è¯†åˆ«è¢«ç§»é™¤çš„REPAç›¸å…³å‚æ•°
                    if any(keyword in key for keyword in [
                        'dual_teacher_model', 'soft_router', 'routing_network', 
                        'dinov2_to_action_projector', 'depth_to_action_projector',
                        'routing_temperature', 'critical_annotator', 'repa']):
                        skipped_params.append(f"{key} (REPAç»„ä»¶ï¼Œå·²ç§»é™¤)")
                    else:
                        skipped_params.append(f"{key} (å‚æ•°ä¸å­˜åœ¨)")
            
            # åŠ è½½è¿‡æ»¤åçš„å‚æ•°
            missing_keys, unexpected_keys = self.policy.load_state_dict(filtered_checkpoint, strict=False)
            
            # ç»Ÿè®¡ç»“æœ
            print(f"âœ… æˆåŠŸåŠ è½½ {len(filtered_checkpoint)} ä¸ªå‚æ•°")
            
            if skipped_params:
                repa_skipped = [p for p in skipped_params if 'REPAç»„ä»¶' in p]
                shape_skipped = [p for p in skipped_params if 'å½¢çŠ¶ä¸åŒ¹é…' in p]
                other_skipped = [p for p in skipped_params if p not in repa_skipped and p not in shape_skipped]
                
                if repa_skipped:
                    print(f"âš ï¸  è·³è¿‡ {len(repa_skipped)} ä¸ªREPAç›¸å…³å‚æ•°ï¼ˆæ¶ˆèå®éªŒå·²ç§»é™¤ï¼‰")
                if shape_skipped:
                    print(f"âš ï¸  è·³è¿‡ {len(shape_skipped)} ä¸ªå½¢çŠ¶ä¸åŒ¹é…å‚æ•°")
                if other_skipped:
                    print(f"âš ï¸  è·³è¿‡ {len(other_skipped)} ä¸ªå…¶ä»–å‚æ•°")
                    for param in other_skipped[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        print(f"     - {param}")
            
            if missing_keys:
                print(f"âš ï¸  {len(missing_keys)} ä¸ªæ¨¡å‹å‚æ•°æœªæ‰¾åˆ°å¯¹åº”æƒé‡ï¼ˆä¿æŒé»˜è®¤åˆå§‹åŒ–ï¼‰")
                # è¿™äº›é€šå¸¸æ˜¯æ–°åŠ å…¥çš„è§†è§‰ç‰¹å¾æŠ•å½±å™¨å‚æ•°
                for key in missing_keys[:5]:
                    if any(keyword in key for keyword in ['dinov2', 'depth', 'visual']):
                        print(f"     - {key} (æ–°å¢è§†è§‰ç‰¹å¾ç»„ä»¶)")
            
            print("âœ… æ¶ˆèå®éªŒæƒé‡åŠ è½½å®Œæˆï¼Œæ¨¡å‹å¯ä»¥æ­£å¸¸è¯„ä¼°")
            return True
            
        except Exception as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            print("â„¹ï¸  å°†ä½¿ç”¨é»˜è®¤åˆå§‹åŒ–ç»§ç»­...")
            import traceback
            traceback.print_exc()
            return False

    def _format_joint_to_state(self, joints):
        """æ ¼å¼åŒ–å…³èŠ‚çŠ¶æ€"""
        AGILEX_STATE_INDICES = ([STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"]
                                 for i in range(self.left_arm_dim)] + [STATE_VEC_IDX_MAPPING["left_gripper_open"]] +
                                [STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"]
                                 for i in range(self.right_arm_dim)] + [STATE_VEC_IDX_MAPPING[f"right_gripper_open"]])
        
        # é‡æ–°ç¼©æ”¾å¤¹çˆªåˆ°[0,1]èŒƒå›´
        joints = joints / torch.tensor(
            [[[1 for i in range(self.left_arm_dim + 1 + self.right_arm_dim + 1)]]],
            device=joints.device,
            dtype=joints.dtype,
        )

        B, N, _ = joints.shape
        state = torch.zeros(
            (B, N, self.args["model"]["state_token_dim"]),
            device=joints.device,
            dtype=joints.dtype,
        )
        # å¡«å……ç»Ÿä¸€çŠ¶æ€å‘é‡
        state[:, :, AGILEX_STATE_INDICES] = joints
        # ç»„è£…æ©ç 
        state_elem_mask = torch.zeros(
            (B, self.args["model"]["state_token_dim"]),
            device=joints.device,
            dtype=joints.dtype,
        )
        state_elem_mask[:, AGILEX_STATE_INDICES] = 1
        return state, state_elem_mask

    def _unformat_action_to_joint(self, action):
        """å°†ç»Ÿä¸€åŠ¨ä½œå‘é‡è½¬æ¢ä¸ºå…³èŠ‚åŠ¨ä½œ"""
        AGILEX_STATE_INDICES = ([STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"]
                                 for i in range(self.left_arm_dim)] + [STATE_VEC_IDX_MAPPING["left_gripper_open"]] +
                                [STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"]
                                 for i in range(self.right_arm_dim)] + [STATE_VEC_IDX_MAPPING[f"right_gripper_open"]])
        action_indices = AGILEX_STATE_INDICES
        joints = action[:, :, action_indices]

        # é‡æ–°ç¼©æ”¾å¤¹çˆªå›åŠ¨ä½œèŒƒå›´
        joints = joints * torch.tensor(
            [[[1 for i in range(self.left_arm_dim + 1 + self.right_arm_dim + 1)]]],
            device=joints.device,
            dtype=joints.dtype,
        )

        return joints

    @torch.no_grad()
    def step(self, proprio, images, text_embeds):
        """
        æ¶ˆèå®éªŒç‰ˆé¢„æµ‹æ­¥éª¤ï¼šé›†æˆå¤šæ¨¡æ€è§†è§‰ç‰¹å¾
        """
        device = self.device
        dtype = self.dtype

        # èƒŒæ™¯å›¾åƒç”¨äºå¡«å……
        background_color = np.array([int(x * 255) for x in self.image_processor.image_mean],
                                    dtype=np.uint8).reshape(1, 1, 3)
        background_image = (np.ones(
            (
                self.image_processor.size["height"],
                self.image_processor.size["width"],
                3,
            ),
            dtype=np.uint8,
        ) * background_color)

        # ğŸ†• åˆ†ç¦»å›¾åƒç”¨äºä¸åŒçš„è§†è§‰ç¼–ç å™¨
        # imagesé¡ºåº: [å‰t-1, å³t-1, å·¦t-1, å‰t, å³t, å·¦t]
        current_frame_images = images[3:6]  # å½“å‰å¸§çš„ä¸‰ä¸ªè§†è§’
        
        # é¢„å¤„ç†SigLIPå›¾åƒ
        image_tensor_list = []
        for image in images:
            if image is None:
                image = Image.fromarray(background_image)

            if self.image_size is not None:
                image = transforms.Resize(self.image_size)(image)

            if self.args["dataset"].get("auto_adjust_image_brightness", False):
                pixel_values = list(image.getdata())
                average_brightness = sum(sum(pixel) for pixel in pixel_values) / (len(pixel_values) * 255.0 * 3)
                if average_brightness <= 0.15:
                    image = transforms.ColorJitter(brightness=(1.75, 1.75))(image)

            if self.args["dataset"].get("image_aspect_ratio", "pad") == "pad":
                def expand2square(pil_img, background_color):
                    width, height = pil_img.size
                    if width == height:
                        return pil_img
                    elif width > height:
                        result = Image.new(pil_img.mode, (width, width), background_color)
                        result.paste(pil_img, (0, (width - height) // 2))
                        return result
                    else:
                        result = Image.new(pil_img.mode, (height, height), background_color)
                        result.paste(pil_img, ((height - width) // 2, 0))
                        return result

                image = expand2square(image, tuple(int(x * 255) for x in self.image_processor.image_mean))
            image = self.image_processor.preprocess(image, return_tensors="pt")["pixel_values"][0]
            image_tensor_list.append(image)

        image_tensor = torch.stack(image_tensor_list, dim=0).to(device, dtype=dtype)

        # SigLIPç¼–ç 
        image_embeds = self.vision_model(image_tensor).detach()
        image_embeds = image_embeds.reshape(-1, self.vision_model.hidden_size).unsqueeze(0)

        # ğŸ†• DINOv2ç‰¹å¾æå–
        dinov2_features = None
        if self.dinov2_encoder is not None and self.use_dinov2_features:
            # ä½¿ç”¨å½“å‰å¸§çš„å‰è§†å›¾åƒ
            front_image = current_frame_images[0]  # å‰è§†ç›¸æœº
            if front_image is not None:
                # é¢„å¤„ç†DINOv2å›¾åƒ
                dinov2_input = Image.fromarray(front_image)
                dinov2_input = dinov2_input.resize((518, 518), Image.BILINEAR)
                dinov2_array = np.array(dinov2_input).astype(np.float32) / 255.0
                dinov2_tensor = torch.from_numpy(dinov2_array).permute(2, 0, 1).to(device, dtype=dtype)
                
                # ImageNetæ ‡å‡†åŒ–
                mean = torch.tensor([0.485, 0.456, 0.406], device=device, dtype=dtype).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225], device=device, dtype=dtype).view(3, 1, 1)
                dinov2_tensor = (dinov2_tensor - mean) / std
                
                # æ·»åŠ batchç»´åº¦å¹¶æå–ç‰¹å¾
                dinov2_tensor = dinov2_tensor.unsqueeze(0)  # (1, 3, 518, 518)
                dinov2_features = self.dinov2_encoder(dinov2_tensor)  # (1, 1, 1024)

        # ğŸ†• DepthAnythingç‰¹å¾æå–  
        depth_features = None
        if self.depth_encoder is not None and self.use_depth_features:
            # ä½¿ç”¨å½“å‰å¸§çš„å‰è§†å›¾åƒ
            front_image = current_frame_images[0]  # å‰è§†ç›¸æœº
            if front_image is not None:
                # é¢„å¤„ç†æ·±åº¦å›¾åƒ
                depth_input = Image.fromarray(front_image)
                depth_input = depth_input.resize((518, 518), Image.BILINEAR)
                depth_array = np.array(depth_input).astype(np.float32) / 255.0
                depth_tensor = torch.from_numpy(depth_array).permute(2, 0, 1).to(device, dtype=dtype)
                
                # ImageNetæ ‡å‡†åŒ–
                mean = torch.tensor([0.485, 0.456, 0.406], device=device, dtype=dtype).view(3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225], device=device, dtype=dtype).view(3, 1, 1)
                depth_tensor = (depth_tensor - mean) / std
                
                # æ·»åŠ batchç»´åº¦å¹¶æå–ç‰¹å¾
                depth_tensor = depth_tensor.unsqueeze(0)  # (1, 3, 518, 518)
                depth_features_raw, _ = self.depth_encoder(depth_tensor)  # (1, 1370, 1024)
                # åªå–CLS token
                depth_features = depth_features_raw[:, 0:1, :]  # (1, 1, 1024)

        # å‡†å¤‡æœ¬ä½“æ„Ÿè§‰çŠ¶æ€å’Œæ§åˆ¶é¢‘ç‡
        joints = proprio.to(device).unsqueeze(0)  # (1, 1, 14)
        states, state_elem_mask = self._format_joint_to_state(joints)  # (1, 1, 128), (1, 128)
        states, state_elem_mask = states.to(device, dtype=dtype), state_elem_mask.to(device, dtype=dtype)
        states = states[:, -1:, :]  # (1, 1, 128)
        ctrl_freqs = torch.tensor([self.control_frequency]).to(device)

        text_embeds = text_embeds.to(device, dtype=dtype)

        # ğŸ†• ä½¿ç”¨æ¶ˆèç‰ˆç­–ç•¥é¢„æµ‹ï¼ˆä¼ å…¥é¢å¤–çš„è§†è§‰ç‰¹å¾ï¼‰
        trajectory = self.policy.predict_action(
            lang_tokens=text_embeds,
            lang_attn_mask=torch.ones(text_embeds.shape[:2], dtype=torch.bool, device=text_embeds.device),
            img_tokens=image_embeds,
            state_tokens=states,
            action_mask=state_elem_mask.unsqueeze(1),
            ctrl_freqs=ctrl_freqs,
            dinov2_features=dinov2_features,
            depth_features=depth_features,
        )
        trajectory = self._unformat_action_to_joint(trajectory).to(torch.float32)

        return trajectory