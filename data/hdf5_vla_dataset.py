import os
import fnmatch
import json

import h5py
import yaml
import cv2
import numpy as np
import torch

from configs.state_vec import STATE_VEC_IDX_MAPPING

# ğŸ†• å¯¼å…¥å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨
try:
    from data.critical_timestep_annotator import TaskType, create_silent_task_annotator
    CRITICAL_ANNOTATION_AVAILABLE = True
except ImportError:
    print("âš ï¸ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨ä¸å¯ç”¨ï¼Œå°†è·³è¿‡å…³é”®æ—¶é—´æ®µç”Ÿæˆ")
    CRITICAL_ANNOTATION_AVAILABLE = False


class HDF5VLADataset:
    """
    This class is used to sample episodes from the embododiment dataset
    stored in HDF5.
    ğŸ†• é›†æˆå…³é”®æ—¶é—´æ®µæ ‡æ³¨åŠŸèƒ½
    
    æ•°æ®ç»“æ„: æ¯ä¸ªHDF5æ–‡ä»¶åŒ…å«ä¸€ä¸ªå®Œæ•´episode
    """

    def __init__(self, model_config_path) -> None:
        # [Modify] The path to the HDF5 dataset directory
        # Each HDF5 file contains one episode
        with open(model_config_path, "r") as f:
            model_config = yaml.safe_load(f)
        HDF5_DIR = model_config["data_path"]
        self.DATASET_NAME = model_config.get("dataset_name", "agilex")

        self.file_paths = []
        for root, _, files in os.walk(HDF5_DIR):
            for filename in fnmatch.filter(files, "*.hdf5"):
                file_path = os.path.join(root, filename)
                self.file_paths.append(file_path)

        if not self.file_paths:
            raise ValueError(f"æœªåœ¨ {HDF5_DIR} ä¸­æ‰¾åˆ°HDF5æ–‡ä»¶")

        # Load the config
        with open("configs/base.yaml", "r") as file:
            config = yaml.safe_load(file)
        self.CHUNK_SIZE = config["common"]["action_chunk_size"]
        self.IMG_HISORY_SIZE = config["common"]["img_history_size"]
        self.STATE_DIM = config["common"]["state_dim"]

        # ğŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨é…ç½®
        self.enable_critical_annotation = model_config.get("enable_critical_annotation", False)
        self.task_type = TaskType(model_config.get("task_type", 1)) if CRITICAL_ANNOTATION_AVAILABLE else None
        
        if self.enable_critical_annotation and CRITICAL_ANNOTATION_AVAILABLE:
            # åˆ›å»ºå…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨
            annotation_config = model_config.get("critical_annotation_config", {})
            self.critical_annotator = create_silent_task_annotator(self.task_type)
            
            # å¦‚æœæœ‰è‡ªå®šä¹‰é…ç½®ï¼Œæ›´æ–°æ ‡æ³¨å™¨å‚æ•°
            if annotation_config:
                for key, value in annotation_config.items():
                    if hasattr(self.critical_annotator, key):
                        setattr(self.critical_annotator, key, value)
                        
            print(f"ğŸ¯ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨å·²å¯ç”¨ (ä»»åŠ¡ç±»å‹: {self.task_type.name})")
        else:
            self.critical_annotator = None
            if self.enable_critical_annotation:
                print("âš ï¸ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å·²ç¦ç”¨ (æ ‡æ³¨å™¨ä¸å¯ç”¨)")

        # Get each episode's len
        episode_lens = []
        valid_file_paths = []
        
        print(f"ğŸ“‚ æ‰«æHDF5æ–‡ä»¶: æ‰¾åˆ° {len(self.file_paths)} ä¸ªæ–‡ä»¶")
        
        for i, file_path in enumerate(self.file_paths):
            try:
                valid, res = self.parse_hdf5_file_state_only(file_path)
                if valid:
                    _len = res["state"].shape[0]
                    episode_lens.append(_len)
                    valid_file_paths.append(file_path)
                else:
                    print(f"âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ä»¶: {os.path.basename(file_path)}")
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")
                
        if not valid_file_paths:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„episodeæ•°æ®")
            
        self.file_paths = valid_file_paths
        self.episode_sample_weights = np.array(episode_lens) / np.sum(episode_lens)
        
        print(f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ: {len(self.file_paths)} ä¸ªæœ‰æ•ˆepisodes")
        print(f"   - å…³é”®æ—¶é—´æ®µæ ‡æ³¨: {'å¯ç”¨' if self.enable_critical_annotation and self.critical_annotator else 'ç¦ç”¨'}")

    def __len__(self):
        return len(self.file_paths)

    def get_dataset_name(self):
        return self.DATASET_NAME

    def _generate_critical_labels(self, qpos_trajectory: np.ndarray, action_horizon: int = None) -> torch.Tensor:
        """
        ğŸ†• ç”Ÿæˆå…³é”®æ—¶é—´æ®µæ ‡ç­¾
        
        Args:
            qpos_trajectory: (T, N) numpyæ•°ç»„ï¼Œå®Œæ•´çš„qposè½¨è¿¹
            action_horizon: åŠ¨ä½œé¢„æµ‹èŒƒå›´ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨CHUNK_SIZE
            
        Returns:
            critical_labels: (action_horizon,) torch tensorï¼Œå…³é”®æ—¶é—´æ®µæ ‡ç­¾
        """
        if not self.enable_critical_annotation or not self.critical_annotator:
            # å¦‚æœæ²¡æœ‰å¯ç”¨æˆ–æ ‡æ³¨å™¨ä¸å¯ç”¨ï¼Œè¿”å›å…¨0æ ‡ç­¾
            horizon = action_horizon or self.CHUNK_SIZE
            return torch.zeros(horizon, dtype=torch.long)
        
        if action_horizon is None:
            action_horizon = self.CHUNK_SIZE
            
        try:
            # ä½¿ç”¨æ ‡æ³¨å™¨ç”Ÿæˆæ ‡ç­¾
            critical_labels, analysis_info = self.critical_annotator.annotate(qpos_trajectory)
            
            # è½¬æ¢ä¸ºtorch tensor
            critical_labels = torch.from_numpy(critical_labels).long()
            
            # ç¡®ä¿æ ‡ç­¾é•¿åº¦åŒ¹é…action_horizon
            if len(critical_labels) > action_horizon:
                # å¦‚æœè½¨è¿¹æ¯”action_horizoné•¿ï¼Œæˆªå–å¯¹åº”éƒ¨åˆ†
                critical_labels = critical_labels[:action_horizon]
            elif len(critical_labels) < action_horizon:
                # å¦‚æœè½¨è¿¹æ¯”action_horizonçŸ­ï¼Œç”¨0å¡«å……
                padding = torch.zeros(action_horizon - len(critical_labels), dtype=torch.long)
                critical_labels = torch.cat([critical_labels, padding])
                
            return critical_labels
            
        except Exception as e:
            print(f"âš ï¸ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è¿”å›å…¨0æ ‡ç­¾
            return torch.zeros(action_horizon, dtype=torch.long)

    def get_item(self, index: int = None, state_only=False):
        """Get a training sample at a random timestep.

        Args:
            index (int, optional): the index of the episode.
                If not provided, a random episode will be selected.
            state_only (bool, optional): Whether to return only the state.
                In this way, the sample will contain a complete trajectory rather
                than a single timestep. Defaults to False.

        Returns:
           sample (dict): a dictionary containing the training sample.
        """
        while True:
            if index is None:
                file_path = np.random.choice(self.file_paths, p=self.episode_sample_weights)
            else:
                file_path = self.file_paths[index]
            valid, sample = (self.parse_hdf5_file(file_path)
                             if not state_only else self.parse_hdf5_file_state_only(file_path))
            if valid:
                return sample
            else:
                index = np.random.randint(0, len(self.file_paths))

    def parse_hdf5_file(self, file_path):
        """ğŸ”„ ä¿®æ”¹ç°æœ‰æ–¹æ³•ï¼Œæ·»åŠ å…³é”®æ—¶é—´æ®µæ ‡ç­¾ç”Ÿæˆ"""
        try:
            with h5py.File(file_path, "r") as f:
                qpos = f["observations"]["qpos"][:]
                left_arm_dim = f["observations"]["left_arm_dim"][:]
                right_arm_dim = f["observations"]["right_arm_dim"][:]
                num_steps = qpos.shape[0]
                
                # [Optional] We drop too-short episode
                # if num_steps < 128:
                #     return False, None

                # [Optional] We skip the first few still steps
                EPS = 1e-2
                # Get the idx of the first qpos whose delta exceeds the threshold
                qpos_delta = np.abs(qpos - qpos[0:1])
                indices = np.where(np.any(qpos_delta > EPS, axis=1))[0]
                if len(indices) > 0:
                    first_idx = indices[0]
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿åŠ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç´¢å¼•
                    first_idx = 0
                    print(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file_path)} ä¸­æœªæ£€æµ‹åˆ°æ˜æ˜¾è¿åŠ¨")

                # ğŸ†• ä¿å­˜å®Œæ•´çš„qposè½¨è¿¹ç”¨äºå…³é”®æ—¶é—´æ®µåˆ†æ
                qpos_trajectory = qpos.copy()

                # We randomly sample a timestep
                step_id = np.random.randint(max(first_idx - 1, 0), num_steps)

                # Load the instruction
                dir_path = os.path.dirname(file_path)

                # You can also use precomputed language embeddings (recommended)
                instruction = "perform manipulation task"  # é»˜è®¤æŒ‡ä»¤
                
                instructions_path = os.path.join(dir_path, "instructions")
                if os.path.exists(instructions_path):
                    instructions_names = []
                    for filename in os.listdir(instructions_path):
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä»¥.ptç»“å°¾
                        if filename.endswith(".pt"):
                            instructions_names.append(os.path.join(instructions_path, filename))
                    
                    if instructions_names:
                        instruction = np.random.choice(instructions_names)

                # Assemble the meta
                meta = {
                    "dataset_name": self.DATASET_NAME,
                    "#steps": num_steps,
                    "step_id": step_id,
                    "instruction": instruction,
                }

                # Rescale gripper to [0, 1]
                total_joints = left_arm_dim[0] + 1 + right_arm_dim[0] + 1
                qpos = qpos / np.array([[1 for i in range(total_joints)]])
                target_qpos = f["action"][step_id:step_id + self.CHUNK_SIZE] / np.array(
                    [[1 for i in range(total_joints)]])

                # Parse the state and action
                state = qpos[step_id:step_id + 1]
                state_std = np.std(qpos, axis=0)
                state_mean = np.mean(qpos, axis=0)
                state_norm = np.sqrt(np.mean(qpos**2, axis=0))
                actions = target_qpos
                
                if actions.shape[0] < self.CHUNK_SIZE:
                    # Pad the actions using the last action
                    actions = np.concatenate(
                        [
                            actions,
                            np.tile(actions[-1:], (self.CHUNK_SIZE - actions.shape[0], 1)),
                        ],
                        axis=0,
                    )

                # ğŸ†• ç”Ÿæˆå…³é”®æ—¶é—´æ®µæ ‡ç­¾
                # ä¸ºå½“å‰é‡‡æ ·çš„åŠ¨ä½œåºåˆ—ç”Ÿæˆæ ‡ç­¾
                action_start_idx = step_id
                action_end_idx = min(step_id + self.CHUNK_SIZE, qpos_trajectory.shape[0])
                
                # æå–å¯¹åº”æ—¶é—´æ®µçš„qposç”¨äºæ ‡æ³¨
                qpos_segment = qpos_trajectory[max(0, action_start_idx-10):action_end_idx+10]  # ç¨å¾®æ‰©å±•ä¸Šä¸‹æ–‡
                critical_labels = self._generate_critical_labels(qpos_segment, self.CHUNK_SIZE)

                # Fill the state/action into the unified vector
                def fill_in_state(values):
                    # Target indices corresponding to your state space
                    # In this example: 6 joints + 1 gripper for each arm
                    UNI_STATE_INDICES = (
                        [STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"]
                         for i in range(left_arm_dim[0])] + [STATE_VEC_IDX_MAPPING["left_gripper_open"]] +
                        [STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"]
                         for i in range(right_arm_dim[0])] + [STATE_VEC_IDX_MAPPING["right_gripper_open"]])
                    uni_vec = np.zeros(values.shape[:-1] + (self.STATE_DIM, ))
                    uni_vec[..., UNI_STATE_INDICES] = values
                    return uni_vec

                state = fill_in_state(state)
                state_indicator = fill_in_state(np.ones_like(state_std))
                state_std = fill_in_state(state_std)
                state_mean = fill_in_state(state_mean)
                state_norm = fill_in_state(state_norm)
                # If action's format is different from state's,
                # you may implement fill_in_action()
                actions = fill_in_state(actions)

                # Parse the images
                def parse_img(key):
                    try:
                        imgs = []
                        for i in range(max(step_id - self.IMG_HISORY_SIZE + 1, 0), step_id + 1):
                            if i < f["observations"]["images"][key].shape[0]:
                                img_bits = f["observations"]["images"][key][i]
                                img = cv2.imdecode(np.frombuffer(img_bits, np.uint8), cv2.IMREAD_COLOR)
                                imgs.append(img)
                            else:
                                # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨æœ€åä¸€å¼ å›¾åƒ
                                if imgs:
                                    imgs.append(imgs[-1])
                                else:
                                    # åˆ›å»ºé»‘è‰²å›¾åƒä½œä¸ºå ä½ç¬¦
                                    imgs.append(np.zeros((224, 224, 3), dtype=np.uint8))
                        
                        imgs = np.stack(imgs) if imgs else np.zeros((self.IMG_HISORY_SIZE, 224, 224, 3), dtype=np.uint8)
                        
                        if imgs.shape[0] < self.IMG_HISORY_SIZE:
                            # Pad the images using the first image
                            imgs = np.concatenate(
                                [
                                    np.tile(
                                        imgs[:1],
                                        (self.IMG_HISORY_SIZE - imgs.shape[0], 1, 1, 1),
                                    ),
                                    imgs,
                                ],
                                axis=0,
                            )
                        return imgs
                    except Exception as e:
                        print(f"âš ï¸ è§£æå›¾åƒ {key} å¤±è´¥: {e}")
                        return np.zeros((self.IMG_HISORY_SIZE, 224, 224, 3), dtype=np.uint8)

                # `cam_high` is the external camera image
                cam_high = parse_img("cam_high")
                # For step_id = first_idx - 1, the valid_len should be one
                valid_len = min(step_id - max(first_idx - 1, 0) + 1, self.IMG_HISORY_SIZE)
                cam_high_mask = np.array([False] * (self.IMG_HISORY_SIZE - valid_len) + [True] * valid_len)
                cam_left_wrist = parse_img("cam_left_wrist")
                cam_left_wrist_mask = cam_high_mask.copy()
                cam_right_wrist = parse_img("cam_right_wrist")
                cam_right_wrist_mask = cam_high_mask.copy()

                # Return the resulting sample
                # For unavailable images, return zero-shape arrays, i.e., (IMG_HISORY_SIZE, 0, 0, 0)
                # E.g., return np.zeros((self.IMG_HISORY_SIZE, 0, 0, 0)) for the key "cam_left_wrist",
                # if the left-wrist camera is unavailable on your robot
                sample = {
                    "meta": meta,
                    "state": state,
                    "state_std": state_std,
                    "state_mean": state_mean,
                    "state_norm": state_norm,
                    "actions": actions,
                    "state_indicator": state_indicator,
                    "cam_high": cam_high,
                    "cam_high_mask": cam_high_mask,
                    "cam_left_wrist": cam_left_wrist,
                    "cam_left_wrist_mask": cam_left_wrist_mask,
                    "cam_right_wrist": cam_right_wrist,
                    "cam_right_wrist_mask": cam_right_wrist_mask,
                    "qpos_trajectory": qpos_trajectory,  # ğŸ†• æ·»åŠ å®Œæ•´qposè½¨è¿¹
                    "critical_labels": critical_labels,  # ğŸ†• æ·»åŠ å…³é”®æ—¶é—´æ®µæ ‡ç­¾
                }
                
                return True, sample

        except Exception as e:
            print(f"âŒ è§£æHDF5æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")
            return False, None

    def parse_hdf5_file_state_only(self, file_path):
        """ğŸ”„ ä¿®æ”¹ç°æœ‰æ–¹æ³•ï¼Œæ·»åŠ å…³é”®æ—¶é—´æ®µæ ‡ç­¾ç”Ÿæˆ"""
        try:
            with h5py.File(file_path, "r") as f:
                qpos = f["observations"]["qpos"][:]
                left_arm_dim = f["observations"]["left_arm_dim"][:]
                right_arm_dim = f["observations"]["right_arm_dim"][:]

                num_steps = qpos.shape[0]
                # [Optional] We drop too-short episode
                if num_steps < 10:  # è‡³å°‘éœ€è¦10æ­¥
                    return False, None

                # [Optional] We skip the first few still steps
                EPS = 1e-2
                # Get the idx of the first qpos whose delta exceeds the threshold
                qpos_delta = np.abs(qpos - qpos[0:1])
                indices = np.where(np.any(qpos_delta > EPS, axis=1))[0]
                if len(indices) > 0:
                    first_idx = indices[0]
                else:
                    first_idx = 0  # å¦‚æœæ²¡æœ‰è¿åŠ¨ï¼Œä»ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥å¼€å§‹

                # ğŸ†• ä¿å­˜å®Œæ•´çš„qposè½¨è¿¹
                qpos_trajectory = qpos.copy()

                # ğŸ†• ç”Ÿæˆå®Œæ•´è½¨è¿¹çš„å…³é”®æ—¶é—´æ®µæ ‡ç­¾
                full_critical_labels = self._generate_critical_labels(qpos_trajectory)

                # Rescale gripper to [0, 1]
                total_joints = left_arm_dim[0] + right_arm_dim[0] + 2
                qpos = qpos / np.array([[1 for i in range(total_joints)]])
                target_qpos = f["action"][:] / np.array([[1 for i in range(total_joints)]])

                # Parse the state and action
                state = qpos[max(first_idx - 1, 0):]
                action = target_qpos[max(first_idx - 1, 0):]

                # Fill the state/action into the unified vector
                def fill_in_state(values):
                    # Target indices corresponding to your state space
                    # In this example: 6 joints + 1 gripper for each arm
                    UNI_STATE_INDICES = (
                        [STATE_VEC_IDX_MAPPING[f"left_arm_joint_{i}_pos"]
                         for i in range(left_arm_dim[0])] + [STATE_VEC_IDX_MAPPING["left_gripper_open"]] +
                        [STATE_VEC_IDX_MAPPING[f"right_arm_joint_{i}_pos"]
                         for i in range(right_arm_dim[0])] + [STATE_VEC_IDX_MAPPING["right_gripper_open"]])
                    uni_vec = np.zeros(values.shape[:-1] + (self.STATE_DIM, ))
                    uni_vec[..., UNI_STATE_INDICES] = values
                    return uni_vec

                state = fill_in_state(state)
                action = fill_in_state(action)

                # è°ƒæ•´å…³é”®æ—¶é—´æ®µæ ‡ç­¾é•¿åº¦ä»¥åŒ¹é…actioné•¿åº¦
                if len(full_critical_labels) > len(action):
                    adjusted_critical_labels = full_critical_labels[:len(action)]
                elif len(full_critical_labels) < len(action):
                    # ç”¨0å¡«å……
                    padding = torch.zeros(len(action) - len(full_critical_labels), dtype=torch.long)
                    adjusted_critical_labels = torch.cat([full_critical_labels, padding])
                else:
                    adjusted_critical_labels = full_critical_labels

                # Return the resulting sample
                return True, {
                    "state": state, 
                    "action": action,
                    "qpos_trajectory": qpos_trajectory,  # ğŸ†• æ·»åŠ å®Œæ•´qposè½¨è¿¹
                    "critical_labels": adjusted_critical_labels,  # ğŸ†• æ·»åŠ å…³é”®æ—¶é—´æ®µæ ‡ç­¾
                }

        except Exception as e:
            print(f"âŒ è§£æHDF5æ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")
            return False, None


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import tempfile
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    test_config = {
        "data_path": "/path/to/your/hdf5/data",  # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
        "dataset_name": "agilex",
        "enable_critical_annotation": True,
        "task_type": 1,  # 1=æŠ“å–ç±», 2=ç‚¹å‡»ç±»
        "critical_annotation_config": {
            "relative_low_speed_ratio": 0.15,
            "min_deceleration_threshold": -0.0008,
            "gripper_close_delta_threshold": -0.01,
            "verbose": True  # æµ‹è¯•æ—¶å¯ç”¨è¯¦ç»†è¾“å‡º
        }
    }
    
    # ä¿å­˜ä¸´æ—¶é…ç½®
    with tempfile.NamedTemporaryFile(mode='w', suffix='.yml', delete=False) as f:
        import yaml
        yaml.dump(test_config, f)
        config_path = f.name
    
    try:
        print("ğŸ§ª æµ‹è¯•HDF5VLAæ•°æ®é›†...")
        ds = HDF5VLADataset(config_path)
        
        print(f"æ•°æ®é›†é•¿åº¦: {len(ds)}")
        print(f"æ•°æ®é›†åç§°: {ds.get_dataset_name()}")
        
        # æµ‹è¯•è·å–æ ·æœ¬
        if len(ds) > 0:
            sample = ds.get_item(0)
            print("\næ ·æœ¬ä¿¡æ¯:")
            for key, value in sample.items():
                if isinstance(value, np.ndarray):
                    print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
                elif isinstance(value, torch.Tensor):
                    print(f"  {key}: shape={value.shape}, dtype={value.dtype}")
                elif isinstance(value, dict):
                    print(f"  {key}: dict with keys {list(value.keys())}")
                else:
                    print(f"  {key}: {type(value)}")
            
            # æ£€æŸ¥å…³é”®æ—¶é—´æ®µæ ‡ç­¾
            if "critical_labels" in sample:
                critical_labels = sample["critical_labels"]
                critical_ratio = critical_labels.float().mean().item() if hasattr(critical_labels, 'float') else 0
                print(f"\nğŸ¯ å…³é”®æ—¶é—´æ®µä¿¡æ¯:")
                print(f"  æ ‡ç­¾å½¢çŠ¶: {critical_labels.shape}")
                print(f"  å…³é”®æ—¶é—´æ®µæ¯”ä¾‹: {critical_ratio:.3f}")
                print(f"  å…³é”®æ—¶é—´æ­¥æ•°: {critical_labels.sum()}")
                
        print("âœ… æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(config_path)