#!/usr/bin/env python3
"""
REPAé›†æˆéªŒè¯æµ‹è¯•è„šæœ¬ï¼ˆå…¨å±€GPUé€‚é…ç‰ˆï¼‰
æµ‹è¯•RDT + REPAå¯¹é½æŸå¤±çš„å®Œæ•´åŠŸèƒ½
"""

import sys
import os
import yaml
import torch
import torch.nn.functional as F
import traceback
from pathlib import Path

# ========== å…¨å±€ device ==========
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"å½“å‰æµ‹è¯•è®¾å¤‡: {device}")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_dinov2_encoder():
    """æµ‹è¯•1: DINOv2ç¼–ç å™¨åŠŸèƒ½"""
    print("ğŸ” æµ‹è¯•1: DINOv2ç¼–ç å™¨åŠ è½½å’ŒåŠŸèƒ½")
    try:
        from models.multimodal_encoder.dinov2_encoder import DinoV2VisionTower, create_dinov2_encoder
        
        dinov2_encoder = create_dinov2_encoder(
            model_size="large", 
            select_feature="patch"
        ).to(device)
        
        dinov2_encoder.print_model_info()
        
        batch_size = 2
        test_images = torch.randn(batch_size, 3, 224, 224, device=device)
        features = dinov2_encoder(test_images)
        print(f"âœ… DINOv2å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   - è¾“å…¥å½¢çŠ¶: {test_images.shape}")
        print(f"   - è¾“å‡ºå½¢çŠ¶: {features.shape}")
        expected_shape = (batch_size, 256, 1024)
        print(f"   - æœŸæœ›å½¢çŠ¶: {expected_shape}")
        assert features.shape == expected_shape, f"å½¢çŠ¶ä¸åŒ¹é…: {features.shape} vs {expected_shape}"
        return dinov2_encoder, features.to(device)
    except Exception as e:
        print(f"âŒ DINOv2æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None, None

def test_rdt_model():
    """æµ‹è¯•2: RDTæ¨¡å‹åˆå§‹åŒ–"""
    print("\nğŸ” æµ‹è¯•2: RDTæ¨¡å‹åˆå§‹åŒ–")
    try:
        from models.rdt.model import RDT
        
        rdt_model = RDT(
            output_dim=128,
            horizon=32,
            hidden_size=1152,
            depth=8,
            num_heads=16,
            enable_repa_loss=True,
            repa_activation_layer=4,
            dinov2_feature_dim=1024,
            dtype=torch.bfloat16,
        ).to(device)
        
        print(f"âœ… RDTæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        print(f"   - æ€»å‚æ•°é‡: {sum(p.numel() for p in rdt_model.parameters()):,}")
        print(f"   - Transformerå±‚æ•°: {len(rdt_model.blocks)}")
        print(f"   - REPAå¯ç”¨: {rdt_model.enable_repa_loss}")
        print(f"   - REPAæ¿€æ´»å±‚: {rdt_model.repa_activation_layer + 1}")
        print(f"   - æ•°æ®ç±»å‹: {next(rdt_model.parameters()).dtype}")
        
        batch_size = 2
        x = torch.randn(batch_size, 33, 1152, dtype=torch.bfloat16, device=device)
        freq = torch.tensor([25, 30], dtype=torch.long, device=device)
        t = torch.tensor([100, 200], dtype=torch.long, device=device)
        lang_c = torch.randn(batch_size, 512, 1152, dtype=torch.bfloat16, device=device)
        img_c = torch.randn(batch_size, 4096, 1152, dtype=torch.bfloat16, device=device)
        
        pred, activations = rdt_model(x, freq, t, lang_c, img_c)
        print(f"âœ… RDTå‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   - é¢„æµ‹å½¢çŠ¶: {pred.shape}")
        print(f"   - ä¸­é—´æ¿€æ´»é”®: {list(activations.keys())}")
        if 'action_tokens_for_repa' in activations:
            action_tokens = activations['action_tokens_for_repa']
            print(f"   - åŠ¨ä½œtokenå½¢çŠ¶: {action_tokens.shape}")
            assert action_tokens.shape == (batch_size, 32, 1152), f"åŠ¨ä½œtokenå½¢çŠ¶é”™è¯¯: {action_tokens.shape}"
        return rdt_model, activations
    except Exception as e:
        print(f"âŒ RDTæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None, None

def test_rdt_runner():
    """æµ‹è¯•3: RDTRunnerå®Œæ•´åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•3: RDTRunnerå®Œæ•´åŠŸèƒ½")
    try:
        config = {
            'rdt': {
                'hidden_size': 1152,
                'num_heads': 16
            },
            'lang_adaptor': 'linear',
            'img_adaptor': 'linear',
            'state_adaptor': 'linear',
            'noise_scheduler': {
                'num_train_timesteps': 1000,
                'beta_schedule': 'linear',
                'prediction_type': 'epsilon',
                'clip_sample': False,
                'num_inference_timesteps': 50
            }
        }
        from models.rdt_runner import RDTRunner
        rdt_runner = RDTRunner(
            action_dim=128,
            pred_horizon=32,
            config=config,
            lang_token_dim=1024,
            img_token_dim=1024,
            state_token_dim=96,
            max_lang_cond_len=1024,
            img_cond_len=4096,
            enable_repa_loss=True,
            repa_loss_weight=0.2,
            dtype=torch.bfloat16
        ).to(device)
        print(f"âœ… RDTRunneråˆå§‹åŒ–æˆåŠŸ")
        print(f"   - REPAæƒé‡: {rdt_runner.repa_loss_weight}")
        print(f"   - æ¨¡å‹å±‚æ•°: {len(rdt_runner.model.blocks)}")
        if hasattr(rdt_runner.model, 'action_to_vision_projector'):
            print(f"   - åŠ¨ä½œæŠ•å½±å™¨å·²åˆ›å»º")
            test_input = torch.randn(2, 1152, dtype=torch.bfloat16, device=device)
            test_output = rdt_runner.model.action_to_vision_projector(test_input)
            print(f"   - æŠ•å½±å™¨æµ‹è¯•: {test_input.shape} -> {test_output.shape}")
            assert test_output.shape == (2, 1024), f"æŠ•å½±å™¨è¾“å‡ºç»´åº¦é”™è¯¯: {test_output.shape}"
        return rdt_runner
    except Exception as e:
        print(f"âŒ RDTRunneræµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_repa_loss_computation(rdt_runner, dinov2_features):
    """æµ‹è¯•4: REPAæŸå¤±è®¡ç®—"""
    print("\nğŸ” æµ‹è¯•4: REPAæŸå¤±è®¡ç®—")
    try:
        batch_size = 2
        action_tokens = torch.randn(batch_size, 32, 1152, dtype=torch.bfloat16, device=device)
        vision_features = dinov2_features.to(torch.bfloat16).to(device)
        print(f"   - åŠ¨ä½œtokenså½¢çŠ¶: {action_tokens.shape}")
        print(f"   - è§†è§‰ç‰¹å¾å½¢çŠ¶: {vision_features.shape}")
        repa_loss = rdt_runner.compute_repa_loss(action_tokens, vision_features)
        print(f"âœ… REPAæŸå¤±è®¡ç®—æˆåŠŸ: {repa_loss.item():.4f}")
        assert repa_loss.item() <= 0, f"REPAæŸå¤±åº”è¯¥æ˜¯è´Ÿå€¼ï¼ˆè´Ÿä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ï¼Œä½†å¾—åˆ°: {repa_loss.item()}"
        assert repa_loss.item() > -2.0, f"REPAæŸå¤±è¿‡å°ï¼Œå¯èƒ½æœ‰é—®é¢˜: {repa_loss.item()}"
        repa_loss_no_vision = rdt_runner.compute_repa_loss(action_tokens, None)
        print(f"âœ… æ— è§†è§‰ç‰¹å¾æƒ…å†µ: {repa_loss_no_vision.item():.4f}")
        assert repa_loss_no_vision.item() == 0.0, "æ— è§†è§‰ç‰¹å¾æ—¶åº”è¿”å›0æŸå¤±"
        rdt_runner.enable_repa_loss = False
        repa_loss_disabled = rdt_runner.compute_repa_loss(action_tokens, vision_features)
        print(f"âœ… REPAå…³é—­æƒ…å†µ: {repa_loss_disabled.item():.4f}")
        assert repa_loss_disabled.item() == 0.0, "REPAå…³é—­æ—¶åº”è¿”å›0æŸå¤±"
        rdt_runner.enable_repa_loss = True
        return repa_loss.item()
    except Exception as e:
        print(f"âŒ REPAæŸå¤±æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return None

def test_full_forward_pass(rdt_runner, dinov2_features):
    """æµ‹è¯•5: å®Œæ•´å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—"""
    print("\nğŸ” æµ‹è¯•5: å®Œæ•´å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—")
    try:
        batch_size = 2
        lang_tokens = torch.randn(batch_size, 512, 1024, dtype=torch.bfloat16, device=device)
        lang_attn_mask = torch.ones(batch_size, 512, dtype=torch.bool, device=device)
        img_tokens = torch.randn(batch_size, 4096, 1024, dtype=torch.bfloat16, device=device)
        state_tokens = torch.randn(batch_size, 1, 128, dtype=torch.bfloat16, device=device)
        action_gt = torch.randn(batch_size, 32, 128, dtype=torch.bfloat16, device=device)
        action_mask = torch.ones(batch_size, 1, 64, dtype=torch.bfloat16, device=device)
        ctrl_freqs = torch.tensor([25, 30], dtype=torch.long, device=device)
        vision_features = dinov2_features.to(torch.bfloat16).to(device)
        print(f"   - è§†è§‰ç‰¹å¾å½¢çŠ¶: {vision_features.shape}")
        total_loss, diffusion_loss, repa_loss = rdt_runner.compute_loss(
            lang_tokens=lang_tokens,
            lang_attn_mask=lang_attn_mask,
            img_tokens=img_tokens,
            state_tokens=state_tokens,
            action_gt=action_gt,
            action_mask=action_mask,
            ctrl_freqs=ctrl_freqs,
            vision_features=vision_features
        )
        print(f"âœ… å®Œæ•´å‰å‘ä¼ æ’­æˆåŠŸ")
        print(f"   - æ€»æŸå¤±: {total_loss.item():.4f}")
        print(f"   - æ‰©æ•£æŸå¤±: {diffusion_loss.item():.4f}")
        print(f"   - REPAæŸå¤±: {repa_loss.item():.4f}")
        print(f"   - REPAè´¡çŒ®: {(repa_loss.item() * rdt_runner.repa_loss_weight):.4f}")
        print(f"   - æŸå¤±æ¯”ä¾‹: {abs(repa_loss.item() * rdt_runner.repa_loss_weight / diffusion_loss.item() * 100):.2f}%")
        assert total_loss.item() > 0, "æ€»æŸå¤±åº”ä¸ºæ­£å€¼"
        assert diffusion_loss.item() > 0, "æ‰©æ•£æŸå¤±åº”ä¸ºæ­£å€¼"
        assert repa_loss.item() <= 0, "REPAæŸå¤±åº”ä¸ºè´Ÿå€¼æˆ–é›¶"
        expected_total = diffusion_loss + rdt_runner.repa_loss_weight * repa_loss
        assert torch.allclose(total_loss, expected_total, atol=1e-5), f"æ€»æŸå¤±è®¡ç®—é”™è¯¯: {total_loss} vs {expected_total}"
        return True
    except Exception as e:
        print(f"âŒ å®Œæ•´å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_gradient_flow(rdt_runner, dinov2_features):
    """æµ‹è¯•6: æ¢¯åº¦æµéªŒè¯"""
    print("\nğŸ” æµ‹è¯•6: æ¢¯åº¦æµéªŒè¯")
    try:
        rdt_runner.train()
        batch_size = 2
        lang_tokens = torch.randn(batch_size, 512, 1024, dtype=torch.bfloat16, device=device, requires_grad=True)
        lang_attn_mask = torch.ones(batch_size, 512, dtype=torch.bool, device=device)
        img_tokens = torch.randn(batch_size, 4096, 1024, dtype=torch.bfloat16, device=device, requires_grad=True)
        state_tokens = torch.randn(batch_size, 1, 128, dtype=torch.bfloat16, device=device, requires_grad=True)
        action_gt = torch.randn(batch_size, 32, 128, dtype=torch.bfloat16, device=device)
        action_mask = torch.ones(batch_size, 1, 64, dtype=torch.bfloat16, device=device)
        ctrl_freqs = torch.tensor([25, 30], dtype=torch.long, device=device)
        vision_features = dinov2_features.to(torch.bfloat16).to(device).detach()
        total_loss, _, _ = rdt_runner.compute_loss(
            lang_tokens=lang_tokens,
            lang_attn_mask=lang_attn_mask,
            img_tokens=img_tokens,
            state_tokens=state_tokens,
            action_gt=action_gt,
            action_mask=action_mask,
            ctrl_freqs=ctrl_freqs,
            vision_features=vision_features
        )
        total_loss.backward()
        components_to_check = [
            ('RDTç¬¬4å±‚', rdt_runner.model.blocks[3].norm1.weight),
            ('åŠ¨ä½œæŠ•å½±å™¨', rdt_runner.model.action_to_vision_projector[0].weight),
            ('æœ€ç»ˆå±‚', rdt_runner.model.final_layer.ffn_final.fc1.weight),
            ('è¯­è¨€é€‚é…å™¨', rdt_runner.lang_adaptor.weight),
        ]
        gradient_ok = True
        for name, param in components_to_check:
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                print(f"âœ… {name} æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
                if grad_norm < 1e-8:
                    print(f"âš ï¸  {name} æ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±")
                elif grad_norm > 100:
                    print(f"âš ï¸  {name} æ¢¯åº¦è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸")
            else:
                print(f"âŒ {name} æ— æ¢¯åº¦")
                gradient_ok = False
        if gradient_ok:
            print("âœ… æ¢¯åº¦æµæ­£å¸¸")
        else:
            print("âš ï¸  éƒ¨åˆ†ç»„ä»¶æ¢¯åº¦å¼‚å¸¸")
        return gradient_ok
    except Exception as e:
        print(f"âŒ æ¢¯åº¦æµæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_alignment_quality(rdt_runner, dinov2_features):
    """æµ‹è¯•7: å¯¹é½è´¨é‡åˆ†æï¼ˆæ–°å¢ï¼‰"""
    print("\nğŸ” æµ‹è¯•7: å¯¹é½è´¨é‡åˆ†æ")
    try:
        batch_size = 2
        horizon = 32
        action_tokens = torch.randn(batch_size, horizon, 1152, dtype=torch.bfloat16, device=device)
        action_tokens_flat = action_tokens.reshape(-1, 1152)
        projected_actions = rdt_runner.model.action_to_vision_projector(action_tokens_flat)
        projected_actions = projected_actions.reshape(batch_size, horizon, 1024)
        projected_actions = F.normalize(projected_actions, dim=-1)
        vision_features_norm = F.normalize(dinov2_features.to(torch.bfloat16).to(device), dim=-1)
        similarities = []
        for b in range(batch_size):
            sim_matrix = torch.mm(projected_actions[b], vision_features_norm[b].t())
            max_sims = sim_matrix.max(dim=1)[0]
            similarities.extend(max_sims.tolist())
        similarities = torch.tensor(similarities, device=device)
        print(f"âœ… ç›¸ä¼¼åº¦ç»Ÿè®¡:")
        print(f"   - å¹³å‡å€¼: {similarities.mean():.4f}")
        print(f"   - æ ‡å‡†å·®: {similarities.std():.4f}")
        print(f"   - æœ€å°å€¼: {similarities.min():.4f}")
        print(f"   - æœ€å¤§å€¼: {similarities.max():.4f}")
        print(f"   - ä¸­ä½æ•°: {similarities.median():.4f}")
        if similarities.mean() < -0.5:
            print("âš ï¸  ç›¸ä¼¼åº¦è¿‡ä½ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æŠ•å½±å™¨åˆå§‹åŒ–")
        elif similarities.mean() > 0.5:
            print("âš ï¸  ç›¸ä¼¼åº¦è¿‡é«˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")
        else:
            print("âœ… ç›¸ä¼¼åº¦åˆ†å¸ƒåˆç†")
        return True
    except Exception as e:
        print(f"âŒ å¯¹é½è´¨é‡æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹REPAé›†æˆæµ‹è¯•")
    print("="*50)
    test_results = {}
    dinov2_encoder, dinov2_features = test_dinov2_encoder()
    test_results['dinov2'] = dinov2_encoder is not None
    if not test_results['dinov2']:
        print("âŒ DINOv2æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    rdt_model, activations = test_rdt_model()
    test_results['rdt_model'] = rdt_model is not None
    rdt_runner = test_rdt_runner()
    test_results['rdt_runner'] = rdt_runner is not None
    if not test_results['rdt_runner']:
        print("âŒ RDTRunneræµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    repa_loss = test_repa_loss_computation(rdt_runner, dinov2_features)
    test_results['repa_loss'] = repa_loss is not None
    forward_ok = test_full_forward_pass(rdt_runner, dinov2_features)
    test_results['full_forward'] = forward_ok
    gradient_ok = test_gradient_flow(rdt_runner, dinov2_features)
    test_results['gradient_flow'] = gradient_ok
    alignment_ok = test_alignment_quality(rdt_runner, dinov2_features)
    test_results['alignment_quality'] = alignment_ok
    print("\n" + "="*50)
    print("ğŸ æµ‹è¯•ç»“æœæ±‡æ€»:")
    passed_tests = 0
    total_tests = len(test_results)
    for test_name, passed in test_results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"   {test_name:20} {status}")
        if passed:
            passed_tests += 1
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed_tests}/{total_tests} æµ‹è¯•é€šè¿‡")
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æ ¸å¿ƒæµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹é›†æˆåˆ°è®­ç»ƒæµç¨‹ã€‚")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("   1. ä¿®æ”¹è®­ç»ƒè„šæœ¬æ·»åŠ DINOv2ç¼–ç å™¨")
        print("   2. è°ƒæ•´è¶…å‚æ•°(Î»=0.2)")
        print("   3. è¿è¡Œå®Œæ•´è®­ç»ƒæµ‹è¯•")
        print("\nâš ï¸  æ³¨æ„äº‹é¡¹:")
        print("   - DINOv2-large è¾“å‡º 256 ä¸ª patches (ä¸æ˜¯ 196)")
        print("   - æŠ•å½±å™¨ç»´åº¦: 1152 -> 2304 -> 1024")
        print("   - REPA æŸå¤±ä¸ºè´Ÿå€¼ï¼ˆè´Ÿä½™å¼¦ç›¸ä¼¼åº¦ï¼‰")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤ã€‚")
    return test_results

if __name__ == "__main__":
    main()
