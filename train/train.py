import copy
import logging
import math
import os
from pathlib import Path

import diffusers
import torch
import torch.utils.checkpoint
import transformers
import yaml
from accelerate import Accelerator
from accelerate.utils import DeepSpeedPlugin, ProjectConfiguration, set_seed
from diffusers.optimization import get_scheduler
from diffusers.utils import is_wandb_available
from huggingface_hub import create_repo, upload_folder
from tqdm.auto import tqdm
from safetensors.torch import load_model

from models.ema_model import EMAModel
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.multimodal_encoder.t5_encoder import T5Embedder
from models.rdt_runner import RDTRunner
from train.dataset import DataCollatorForVLAConsumerDataset, VLAConsumerDataset
from train.sample import log_sample_res

# å¯¼å…¥DINOv2å’ŒDepthAnythingV2ç¼–ç å™¨
from models.multimodal_encoder.dinov2_encoder import create_dinov2_encoder
from models.multimodal_encoder.depth_encoder import create_depth_encoder

# å¯¼å…¥å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨
from data.critical_timestep_annotator import TaskType

if is_wandb_available():
    import wandb


def save_model_card(repo_id: str, base_model=str, repo_folder=None):
    yaml_header = f"""
---
license: mit
base_model: {base_model}
language:
- en
pipeline_tag: robotics
library_name: transformers
tags:
- robotics
- pytorch
- multimodal
- pretraining
- vla
- diffusion
- rdt
- soft-routing
- dual-teachers
- critical-timestep
- binary-labels
---
    """
    model_card = f"""
# RDT with Soft Routing Dual-Teacher REPA - {repo_id}

This is a RDT model with soft routing dual-teacher REPA alignment loss, task-driven critical timestep annotation 
derived from {base_model}. The weights were trained using [RDT](https://rdt-robotics.github.io/rdt-robotics/) 
with advanced soft routing multi-modal alignment strategies.

## Key Features
- **Soft Routing Strategy**: Rule-driven weight allocation based on binary critical timestep labels
- **Critical Timestep Annotation**: Task-driven annotation for precise temporal alignment
- **Dual Visual Teachers**: DINOv2 (global semantic) + DepthAnythingV2 (depth geometric)
- **Neural Weight Adjustment**: Optional fine-tuning with temporal smoothing
- **Contrastive Learning**: Enhanced feature alignment with contrastive loss

## Weight Allocation Strategy
- **Critical Timesteps (1)**: Global 25%, Depth 75% - Focus on precise manipulation
- **Non-Critical Timesteps (0)**: Global 75%, Depth 25% - Focus on scene understanding

## Architecture Components
1. **Binary Label Soft Router**: Rule-driven weight allocation with optional neural adjustment
2. **Dual Visual Teachers**: DINOv2 (global) + DepthAnythingV2 (geometric)
3. **Temporal Smoothing**: Prevents sudden weight transitions
4. **Contrastive Learning**: Enhances feature alignment quality

## Task Types Supported
- **Grasp Tasks (task_type=1)**: Deceleration â†’ Gripper closing alignment
- **Click Tasks (task_type=2)**: Gripper closing â†’ Deceleration alignment
"""
    with open(os.path.join(repo_folder, "README.md"), "w") as f:
        f.write(yaml_header + model_card)


def check_critical_alerts(metrics, global_step, logger):
    """
    æ£€æŸ¥å…³é”®å¼‚å¸¸æƒ…å†µå¹¶å‘å‡ºé¢„è­¦
    """
    alerts = []
    
    # 1. ç‰¹å¾å¯¹é½å¤±æ•ˆ
    if metrics.get('global_similarity', 1.0) < 0.3 and global_step > 1000:
        alerts.append("WARNING: Low global similarity - feature alignment may be failing")
    
    # 2. æƒé‡åˆ†é…ä¸¥é‡åç¦»
    if 'critical_global_weight' in metrics:
        expected_critical_global = 0.25
        actual = metrics['critical_global_weight']
        if abs(actual - expected_critical_global) > 0.2:  # åç¦»è¶…è¿‡20%
            alerts.append(f"WARNING: Critical weight deviation - expected {expected_critical_global}, got {actual:.3f}")
    
    # 3. æƒé‡è°ƒæ•´è¿‡åº¦
    if metrics.get('weight_drift', 0) > 0.15:
        alerts.append("WARNING: Excessive weight drift - neural adjustment may be too aggressive")
    
    # 4. æ•°æ®ä¸å¹³è¡¡
    critical_ratio = metrics.get('critical_ratio', 0.3)
    if critical_ratio < 0.1 or critical_ratio > 0.6:
        alerts.append(f"WARNING: Critical ratio imbalance - {critical_ratio:.3f}")
    
    # è®°å½•è­¦å‘Š
    for alert in alerts:
        logger.warning(alert)
    
    return alerts


def train(args, logger):
    # Read the config
    with open(args.config_path, "r") as fp:
        config = yaml.safe_load(fp)

    with open(args.model_config_path, "r") as f:
        model_config = yaml.safe_load(f)
    
    args.output_dir = model_config["checkpoint_path"]
    logging_dir = Path(args.output_dir, args.logging_dir)

    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)
    accelerator = Accelerator(
        deepspeed_plugin=(DeepSpeedPlugin(hf_ds_config=args.deepspeed) if args.deepspeed is not None else None),
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_dir=logging_dir,
        project_config=accelerator_project_config,
    )

    if args.report_to == "wandb":
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # Make one log on every process with the configuration for debugging.
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # If passed along, set the training seed now.
    if args.seed is not None:
        set_seed(args.seed)

    # Handle the repository creation
    if accelerator.is_main_process:
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
                token=args.hub_token,
            ).repo_id

    # For mixed precision training
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16

    # â­ è·å–åŒæ•™å¸ˆé…ç½®
    global_teacher_type = model_config.get("global_teacher_type", "dinov2")
    depth_teacher_type = model_config.get("depth_teacher_type", "depth_anything_v2")  # â­ æ–°å¢
    
    # å‘åå…¼å®¹ï¼šä»æ—§æ ‡å¿—æ¨æ–­
    if "global_teacher_type" not in model_config:
        if model_config.get("use_dinov2_features", False):
            global_teacher_type = "dinov2"
        elif model_config.get("use_siglip_global_features", False):
            global_teacher_type = "siglip"
        else:
            global_teacher_type = "dinov2"
    
    # â­ å‘åå…¼å®¹ï¼šä»æ—§æ ‡å¿—æ¨æ–­æ·±åº¦æ•™å¸ˆ
    if "depth_teacher_type" not in model_config:
        if model_config.get("use_depth_features", True):
            depth_teacher_type = "depth_anything_v2"
        elif model_config.get("use_siglip_depth_features", False):
            depth_teacher_type = "siglip"
        else:
            depth_teacher_type = "depth_anything_v2"
    
    # â­ è‡ªåŠ¨è®¾ç½®ç‰¹å¾æ ‡å¿—
    use_dinov2_features = (global_teacher_type == "dinov2")
    use_siglip_global_features = (global_teacher_type == "siglip")
    use_depth_anything_v2 = (depth_teacher_type == "depth_anything_v2")
    use_siglip_depth_features = (depth_teacher_type == "siglip")
    
    # ç¡®å®šç‰¹å¾ç»´åº¦
    if global_teacher_type == "dinov2":
        global_feature_dim = 1024
    elif global_teacher_type == "siglip":
        global_feature_dim = 1152
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å…¨å±€æ•™å¸ˆç±»å‹: {global_teacher_type}")
    
    # â­ ç¡®å®šæ·±åº¦ç‰¹å¾ç»´åº¦
    if depth_teacher_type == "depth_anything_v2":
        depth_feature_dim = 1024
    elif depth_teacher_type == "siglip":
        depth_feature_dim = 1152
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ·±åº¦æ•™å¸ˆç±»å‹: {depth_teacher_type}")
    
    logger.info("=" * 70)
    logger.info(f"ğŸ¯ åŒæ•™å¸ˆé…ç½®ï¼ˆè‡ªåŠ¨è®¾ç½®ï¼‰:")
    logger.info(f"   ğŸ“Š å…¨å±€æ•™å¸ˆ:")
    logger.info(f"      - ç±»å‹: {global_teacher_type.upper()}")
    logger.info(f"      - ç‰¹å¾ç»´åº¦: {global_feature_dim}")
    logger.info(f"      - use_dinov2_features: {use_dinov2_features}")
    logger.info(f"      - use_siglip_global_features: {use_siglip_global_features}")
    logger.info(f"   ğŸ“Š æ·±åº¦æ•™å¸ˆ:")
    logger.info(f"      - ç±»å‹: {depth_teacher_type.upper()}")
    logger.info(f"      - ç‰¹å¾ç»´åº¦: {depth_feature_dim}")
    logger.info(f"      - use_depth_anything_v2: {use_depth_anything_v2}")
    logger.info(f"      - use_siglip_depth_features: {use_siglip_depth_features}")
    logger.info("=" * 70)
    
    # è·å–è½¯è·¯ç”±REPAé…ç½®
    enable_soft_routing_repa = model_config.get("enable_soft_routing_repa", True)
    soft_routing_repa_weight = model_config.get("soft_routing_repa_weight", 0.2)
    
    # å…³é”®æ—¶é—´æ®µæ ‡æ³¨é…ç½®
    enable_critical_annotation = model_config.get("enable_critical_annotation", True)
    task_type = model_config.get("task_type", 1)
    critical_annotation_config = model_config.get("critical_annotation_config", {})
    
    # è½¯è·¯ç”±é…ç½®
    soft_routing_config = model_config.get("soft_routing_config", {})
    # â­ è‡ªåŠ¨è®¾ç½®ç»´åº¦ï¼ˆå¦‚æœæœªé…ç½®ï¼‰
    if 'global_dim' not in soft_routing_config:
        soft_routing_config['global_dim'] = global_feature_dim
    if 'depth_dim' not in soft_routing_config:
        soft_routing_config['depth_dim'] = depth_feature_dim

    # æ–‡æœ¬ç¼–ç å™¨
    if args.precomp_lang_embed:
        tokenizer, text_encoder = None, None
    else:
        text_embedder = T5Embedder(
            from_pretrained=args.pretrained_text_encoder_name_or_path,
            model_max_length=config["dataset"]["tokenizer_max_length"],
            device=accelerator.device,
        )
        tokenizer, text_encoder = text_embedder.tokenizer, text_embedder.model

    # SigLIPè§†è§‰ç¼–ç å™¨ï¼ˆä¸»å¹²ï¼‰
    vision_encoder = SiglipVisionTower(
        vision_tower=args.pretrained_vision_encoder_name_or_path, 
        args=None
    )
    image_processor = vision_encoder.image_processor

    # â­ åˆ›å»ºå…¨å±€æ•™å¸ˆç¼–ç å™¨
    global_teacher_encoder = None
    if enable_soft_routing_repa:
        if global_teacher_type == "dinov2":
            logger.info("ğŸ“¦ åŠ è½½DINOv2å…¨å±€æ•™å¸ˆç¼–ç å™¨...")
            from models.multimodal_encoder.dinov2_encoder import create_dinov2_encoder
            global_teacher_encoder = create_dinov2_encoder(
                model_size="large", 
                select_feature="cls_only"
            )
        elif global_teacher_type == "siglip":
            logger.info("ğŸ“¦ åŠ è½½SigLIPå…¨å±€æ•™å¸ˆç¼–ç å™¨...")
            from models.multimodal_encoder.siglip_global_encoder import create_siglip_global_encoder
            global_teacher_encoder = create_siglip_global_encoder(
                model_name=args.pretrained_vision_encoder_name_or_path,
                pooling_strategy="mean",
                feature_dim=global_feature_dim
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å…¨å±€æ•™å¸ˆç±»å‹: {global_teacher_type}")
        
        global_teacher_encoder.to(accelerator.device, dtype=weight_dtype)
        global_teacher_encoder.print_model_info()

    # â­ åˆ›å»ºæ·±åº¦æ•™å¸ˆç¼–ç å™¨
    depth_teacher_encoder = None
    if enable_soft_routing_repa:
        if depth_teacher_type == "depth_anything_v2":
            logger.info("ğŸ“¦ åŠ è½½DepthAnythingV2æ·±åº¦æ•™å¸ˆç¼–ç å™¨...")
            from models.multimodal_encoder.depth_encoder import create_depth_encoder
            depth_teacher_encoder = create_depth_encoder(
                model_size="metric_large",
                feature_dim=1024,
                device=accelerator.device,
                use_metric_model=True
            )
        elif depth_teacher_type == "siglip":
            logger.info("ğŸ“¦ åŠ è½½SigLIPæ·±åº¦æ•™å¸ˆç¼–ç å™¨...")
            from models.multimodal_encoder.siglip_depth_encoder import create_siglip_depth_encoder
            depth_teacher_encoder = create_siglip_depth_encoder(
                model_name=args.pretrained_vision_encoder_name_or_path,
                feature_dim=depth_feature_dim,
                output_format="patch_tokens",  # å¯é…ç½®
                device=accelerator.device
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ·±åº¦æ•™å¸ˆç±»å‹: {depth_teacher_type}")
        
        depth_teacher_encoder.to(accelerator.device, dtype=weight_dtype)
        depth_teacher_encoder.print_model_info()

    # â­ æ„å»ºRDTæ¨¡å‹
    logger.info("ğŸ”¨ æ„å»ºè½¯è·¯ç”±åŒæ•™å¸ˆRDTæ¨¡å‹...")
    img_cond_len = (config["common"]["img_history_size"] * 
                    config["common"]["num_cameras"] *
                    vision_encoder.num_patches)
    
    repa_activation_layer = model_config.get("repa_activation_layer", 21)
    
    rdt = RDTRunner(
        action_dim=config["common"]["state_dim"],
        pred_horizon=config["common"]["action_chunk_size"],
        config=config["model"],
        lang_token_dim=config["model"]["lang_token_dim"],
        img_token_dim=config["model"]["img_token_dim"],
        state_token_dim=config["model"]["state_token_dim"],
        max_lang_cond_len=config["dataset"]["tokenizer_max_length"],
        img_cond_len=img_cond_len,
        img_pos_embed_config=[
            ("image", (
                config["common"]["img_history_size"],
                config["common"]["num_cameras"],
                -vision_encoder.num_patches,
            )),
        ],
        lang_pos_embed_config=[
            ("lang", -config["dataset"]["tokenizer_max_length"]),
        ],
        dtype=weight_dtype,
        enable_soft_routing_repa=enable_soft_routing_repa,
        soft_routing_repa_weight=soft_routing_repa_weight,
        global_feature_dim=global_feature_dim,
        depth_feature_dim=depth_feature_dim,  # â­ ä¼ é€’æ·±åº¦ç»´åº¦
        soft_routing_config=soft_routing_config,
        repa_activation_layer=repa_activation_layer
    )
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
    if args.pretrained_model_name_or_path and os.path.isfile(args.pretrained_model_name_or_path):
        logger.info(f"Loading pretrained weights: {args.pretrained_model_name_or_path}")
        ckpt = torch.load(args.pretrained_model_name_or_path, map_location="cpu")

        if isinstance(ckpt, dict) and "module" in ckpt:
            pretrained_sd = ckpt["module"]
        elif isinstance(ckpt, dict) and "state_dict" in ckpt:
            pretrained_sd = ckpt["state_dict"]
        else:
            pretrained_sd = ckpt

        own_sd = rdt.state_dict()
        filtered = {}
        for k, v in pretrained_sd.items():
            if k in own_sd and v.shape == own_sd[k].shape:
                filtered[k] = v
            else:
                logger.debug(f"Skipping parameter {k}: checkpoint {tuple(v.shape)} vs model {tuple(own_sd.get(k, v).shape)}")

        rdt.load_state_dict(filtered, strict=False)
        logger.info("Loaded matching pretrained weights; others remain randomly initialized")
    else:
        logger.info("Only using config; skipping pretrained weight loading")

    # EMAæ¨¡å‹
    ema_rdt = copy.deepcopy(rdt)
    ema_model = EMAModel(
        ema_rdt,
        update_after_step=config["model"]["ema"]["update_after_step"],
        inv_gamma=config["model"]["ema"]["inv_gamma"],
        power=config["model"]["ema"]["power"],
        min_value=config["model"]["ema"]["min_value"],
        max_value=config["model"]["ema"]["max_value"],
    )

    # ä¿å­˜é’©å­
    def save_model_hook(models, weights, output_dir):
        if accelerator.is_main_process:
            for model in models:
                model_to_save = model.module if hasattr(model, "module") else model
                if isinstance(model_to_save, type(accelerator.unwrap_model(rdt))):
                    model_to_save.save_pretrained(output_dir)

    accelerator.register_save_state_pre_hook(save_model_hook)

    if args.gradient_checkpointing:
        raise NotImplementedError("Gradient checkpointing is not yet implemented.")

    # Enable TF32 for faster training on Ampere GPUs
    if args.allow_tf32:
        torch.backends.cuda.matmul.allow_tf32 = True

    if args.scale_lr:
        args.learning_rate = (args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size *
                              accelerator.num_processes)

    # ä¼˜åŒ–å™¨
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            raise ImportError("To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.")
        optimizer_class = bnb.optim.AdamW8bit
    else:
        optimizer_class = torch.optim.AdamW

    params_to_optimize = rdt.parameters()
    optimizer = optimizer_class(
        params_to_optimize,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )
    critical_annotation_config = model_config.get("critical_annotation_config", {})
    use_dinov2_features = (global_teacher_type == "dinov2")
    use_siglip_global_features = (global_teacher_type == "siglip")
    # â­ åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆè‡ªåŠ¨ä¼ é€’å‚æ•°ï¼‰
    train_dataset = VLAConsumerDataset(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=args.image_aug,
        cond_mask_prob=args.cond_mask_prob,
        cam_ext_mask_prob=args.cam_ext_mask_prob,
        state_noise_snr=args.state_noise_snr,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        # â­ è‡ªåŠ¨è®¾ç½®çš„ç‰¹å¾æ ‡å¿—
        use_dinov2_features=use_dinov2_features,
        use_siglip_global_features=use_siglip_global_features,
        use_depth_anything_v2=use_depth_anything_v2,  # â­ æ–°å¢
        use_siglip_depth_features=use_siglip_depth_features,  # â­ æ–°å¢
        # å…³é”®æ—¶é—´æ®µæ ‡æ³¨å‚æ•°
        task_type=task_type,
        enable_critical_annotation=enable_critical_annotation,
        critical_annotation_config=critical_annotation_config,
    )
    
    # â­ åˆ›å»ºé‡‡æ ·æ•°æ®é›†ï¼ˆè‡ªåŠ¨ä¼ é€’å‚æ•°ï¼‰
    sample_dataset = VLAConsumerDataset(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=args.image_aug,
        cond_mask_prob=args.cond_mask_prob,
        cam_ext_mask_prob=args.cam_ext_mask_prob,
        state_noise_snr=args.state_noise_snr,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        # â­ è‡ªåŠ¨è®¾ç½®çš„ç‰¹å¾æ ‡å¿—
        use_dinov2_features=use_dinov2_features,
        use_siglip_global_features=use_siglip_global_features,
        use_depth_anything_v2=use_depth_anything_v2,  # â­ æ–°å¢
        use_siglip_depth_features=use_siglip_depth_features,  # â­ æ–°å¢
        # å…³é”®æ—¶é—´æ®µæ ‡æ³¨å‚æ•°
        task_type=task_type,
        enable_critical_annotation=enable_critical_annotation,
        critical_annotation_config=critical_annotation_config,
    )

    data_collator = DataCollatorForVLAConsumerDataset(tokenizer)

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.train_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=args.dataloader_num_workers,
        pin_memory=True,
        persistent_workers=True,
    )
    
    sample_dataloader = torch.utils.data.DataLoader(
        sample_dataset,
        batch_size=args.sample_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=args.dataloader_num_workers,
        pin_memory=True,
        persistent_workers=True,
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    overrode_max_train_steps = False
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
    )

    # å‡†å¤‡è®­ç»ƒ
    rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler = (
        accelerator.prepare(rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler)
    )

    ema_rdt.to(accelerator.device, dtype=weight_dtype)

    if text_encoder is not None:
        text_encoder.to(accelerator.device, dtype=weight_dtype)

    if vision_encoder is not None:
        vision_encoder.vision_tower.to(accelerator.device, dtype=weight_dtype)

    # é‡æ–°è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # åˆå§‹åŒ–è¿½è¸ªå™¨
    if accelerator.is_main_process:
        tracker_config = vars(args).copy()
        tracker_config.update({
            'global_teacher_type': global_teacher_type,  # â­ è®°å½•å…¨å±€æ•™å¸ˆç±»å‹
            'global_feature_dim': global_feature_dim,  # â­ è®°å½•å…¨å±€ç‰¹å¾ç»´åº¦
            'task_type': task_type,
            'enable_critical_annotation': enable_critical_annotation,
            'critical_annotation_config': critical_annotation_config,
            'enable_soft_routing_repa': enable_soft_routing_repa,
            'soft_routing_config': soft_routing_config,
        })
        # â­ æ ¹æ®å…¨å±€æ•™å¸ˆç±»å‹è®¾ç½®è¿è¡Œå
        task_name = "grasp" if task_type == 1 else "click"
        run_name = f"RDT_SoftRouting_{global_teacher_type.upper()}_{task_name}_{args.CONFIG_NAME}"
        accelerator.init_trackers(
            "VLA_Soft_Routing_Dual_Teacher_REPA",
            config=tracker_config,
            init_kwargs={"wandb": {
                "name": run_name,
            }},
        )
    # è®­ç»ƒä¿¡æ¯
    total_batch_size = (args.train_batch_size * accelerator.num_processes * 
                       args.gradient_accumulation_steps)

    logger.info("***** å¼€å§‹è½¯è·¯ç”±åŒæ•™å¸ˆREPAè®­ç»ƒ *****")
    logger.info(f"  ç¤ºä¾‹æ•°é‡ = {len(train_dataset)}")
    logger.info(f"  Epochæ•°é‡ = {args.num_train_epochs}")
    logger.info(f"  å…¨å±€æ•™å¸ˆç±»å‹ = {global_teacher_type.upper()}")  # â­ æ–°å¢
    logger.info(f"  æ·±åº¦æ•™å¸ˆç±»å‹ = DepthAnythingV2")
    logger.info(f"  ä»»åŠ¡ç±»å‹ = {TaskType(task_type).name}")
    logger.info(f"  è·¯ç”±ç­–ç•¥ = è§„åˆ™é©±åŠ¨ + å¯é€‰ç¥ç»è°ƒæ•´")
    
    global_step = 0
    first_epoch = 0

    # å¯èƒ½ä»æ£€æŸ¥ç‚¹æ¢å¤
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint != "latest":
            path = os.path.basename(args.resume_from_checkpoint)
        else:
            dirs = os.listdir(args.output_dir)
            dirs = [d for d in dirs if d.startswith("checkpoint")]
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
            path = dirs[-1] if len(dirs) > 0 else None

        if path is None:
            accelerator.print(f"Checkpoint '{args.resume_from_checkpoint}' not found. Starting new training.")
            args.resume_from_checkpoint = None
        else:
            accelerator.print(f"Resuming from checkpoint: {path}")
            try:
                accelerator.load_state(os.path.join(args.output_dir, path))
            except:
                logger.info("Failed to restore training state. Trying to load model checkpoint only.")
                checkpoint = torch.load(
                    os.path.join(args.output_dir, path, "pytorch_model", "mp_rank_00_model_states.pt"))
                rdt.module.load_state_dict(checkpoint["module"])

            load_model(ema_rdt, os.path.join(args.output_dir, path, "ema", "model.safetensors"))
            global_step = int(path.split("-")[1])

            resume_global_step = global_step * args.gradient_accumulation_steps
            first_epoch = global_step // num_update_steps_per_epoch
            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)

    # è¿›åº¦æ¡
    progress_bar = tqdm(
        range(global_step, args.max_train_steps),
        disable=not accelerator.is_local_main_process,
    )
    progress_bar.set_description("Steps")

    # ç²¾ç®€ç‰ˆç»Ÿè®¡å˜é‡
    soft_routing_stats = {
        'total_samples': 0,
        'critical_timesteps': 0,
    }
    
    # è®­ç»ƒå¾ªç¯ä¸­çš„ä¿®æ”¹
    for epoch in range(first_epoch, args.num_train_epochs):
        rdt.train()
        accelerator.unwrap_model(rdt).reset_batch_count()

        for batch in train_dataloader:
            with accelerator.accumulate(rdt):
                # å‡†å¤‡è¾“å…¥æ•°æ®
                images = batch["images"].to(dtype=weight_dtype)
                states = batch["states"].to(dtype=weight_dtype)[:, -1:, :]
                actions = batch["actions"].to(dtype=weight_dtype)
                state_elem_mask = batch["state_elem_mask"].to(dtype=weight_dtype)
                ctrl_freqs = batch["ctrl_freqs"]
                critical_labels = batch.get("critical_labels", None)

                # ç¼–ç è§†è§‰ç‰¹å¾
                with torch.no_grad():
                    # 1ï¸âƒ£ SigLIPç¼–ç ï¼ˆä¸»å¹²ï¼‰
                    batch_size, _, C, H, W = images.shape
                    image_embeds = vision_encoder(images.reshape(-1, C, H, W)).detach()
                    image_embeds = image_embeds.reshape(
                        (batch_size, -1, vision_encoder.hidden_size)
                    )

                    # 2ï¸âƒ£ æ–‡æœ¬ç¼–ç 
                    lang_attn_mask = batch["lang_attn_mask"]
                    text_embeds = (
                        batch["lang_embeds"].to(dtype=weight_dtype) 
                        if args.precomp_lang_embed 
                        else text_encoder(
                            input_ids=batch["input_ids"], 
                            attention_mask=lang_attn_mask
                        )["last_hidden_state"].detach()
                    )

                    # 3ï¸âƒ£ â­ å…¨å±€æ•™å¸ˆç¼–ç 
                    global_cls_token = None
                    if global_teacher_encoder is not None:
                        if global_teacher_type == "dinov2" and "dinov2_images" in batch:
                            dinov2_images = batch["dinov2_images"].to(dtype=weight_dtype)
                            dinov2_input = dinov2_images[:, 0]
                            global_cls_token = global_teacher_encoder(dinov2_input)
                        
                        elif global_teacher_type == "siglip" and "siglip_global_images" in batch:
                            siglip_images = batch["siglip_global_images"].to(dtype=weight_dtype)
                            siglip_input = siglip_images[:, 0]
                            global_cls_token = global_teacher_encoder(siglip_input)

                    # 4ï¸âƒ£ â­ æ·±åº¦æ•™å¸ˆç¼–ç 
                    depth_features = None
                    if depth_teacher_encoder is not None:
                        if depth_teacher_type == "depth_anything_v2" and "depth_images" in batch:
                            depth_images = batch["depth_images"].to(dtype=weight_dtype)
                            depth_input = depth_images[:, 0]
                            depth_features, _ = depth_teacher_encoder(depth_input)
                        
                        elif depth_teacher_type == "siglip" and "siglip_depth_images" in batch:
                            siglip_depth_images = batch["siglip_depth_images"].to(dtype=weight_dtype)
                            siglip_depth_input = siglip_depth_images[:, 0]
                            depth_features, _ = depth_teacher_encoder(siglip_depth_input)

                # è®¡ç®—æŸå¤±ï¼ˆè‡ªåŠ¨å¤„ç†ä¸åŒç»´åº¦ï¼‰
                state_elem_mask = state_elem_mask.unsqueeze(1)
                if enable_soft_routing_repa:
                    total_loss, diffusion_loss, repa_loss, detailed_metrics = (
                        accelerator.unwrap_model(rdt).compute_loss(
                            lang_tokens=text_embeds,
                            lang_attn_mask=lang_attn_mask,
                            img_tokens=image_embeds,
                            state_tokens=states,
                            action_gt=actions,
                            action_mask=state_elem_mask,
                            ctrl_freqs=ctrl_freqs,
                            cls_token=global_cls_token,
                            depth_features=depth_features,
                            critical_labels=critical_labels,
                        )
                    )
                    
                    # â­â­â­ å…³é”®ä¿®å¤ï¼šå°† total_loss èµ‹å€¼ç»™ loss
                    loss = total_loss
                    
                    # ç²¾ç®€ç‰ˆæŒ‡æ ‡æ”¶é›†
                    loss_for_log = {
                        "diffusion_loss": diffusion_loss.detach().item(),
                        "repa_loss": repa_loss.detach().item(),
                        "alignment_loss": detailed_metrics.get('soft_routing_alignment_loss', 0.0),
                        'global_similarity': detailed_metrics.get('global_similarity_avg', 0.0),
                        'depth_similarity': detailed_metrics.get('depth_similarity_avg', 0.0),
                        'critical_ratio': detailed_metrics.get('critical_ratio', 0.0),
                        'avg_global_weight': detailed_metrics.get('avg_global_weight', 0.5),
                        'avg_depth_weight': detailed_metrics.get('avg_depth_weight', 0.5),
                    }
                    
                    if 'critical_avg_global_weight' in detailed_metrics:
                        loss_for_log.update({
                            'critical_global_weight': detailed_metrics['critical_avg_global_weight'],
                            'critical_depth_weight': detailed_metrics['critical_avg_depth_weight'],
                        })
                    
                    if 'non_critical_avg_global_weight' in detailed_metrics:
                        loss_for_log.update({
                            'non_critical_global_weight': detailed_metrics['non_critical_avg_global_weight'],
                            'non_critical_depth_weight': detailed_metrics['non_critical_avg_depth_weight'],
                        })
                    
                    if 'weight_drift' in detailed_metrics:
                        loss_for_log['weight_drift'] = detailed_metrics['weight_drift']
                    
                else:
                    loss = rdt(
                        lang_tokens=text_embeds,
                        lang_attn_mask=lang_attn_mask,
                        img_tokens=image_embeds,
                        state_tokens=states,
                        action_gt=actions,
                        action_mask=state_elem_mask,
                        ctrl_freqs=ctrl_freqs,
                    )
                    loss_for_log = {"diffusion_loss": loss.detach().item()}

                # åå‘ä¼ æ’­
                accelerator.backward(loss)  
                if accelerator.sync_gradients:
                    params_to_clip = rdt.parameters()
                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=args.set_grads_to_none)

            # EMAæ›´æ–°
            ema_model.step(accelerator.unwrap_model(rdt))

            # æ£€æŸ¥ç‚¹å’Œé‡‡æ ·
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

                if global_step % args.checkpointing_period == 0:
                    save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                    accelerator.save_state(save_path)
                    ema_save_path = os.path.join(save_path, f"ema")
                    accelerator.save_model(ema_rdt, ema_save_path)
                    logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {save_path}")

                # ç²¾ç®€ç‰ˆç›‘æ§
                if global_step % 500 == 0 and enable_soft_routing_repa:
                    logger.info(f"Step {global_step} - æ ¸å¿ƒæŒ‡æ ‡:")
                    logger.info(f"  æŸå¤±: diffusion={loss_for_log.get('diffusion_loss', 0):.4f}, "
                               f"repa={loss_for_log.get('repa_loss', 0):.4f}")
                    logger.info(f"  å…¨å±€æ•™å¸ˆ: {global_teacher_type.upper()}")  # â­ æ˜¾ç¤ºå½“å‰æ•™å¸ˆ
                    
                    if 'global_similarity' in loss_for_log:
                        logger.info(f"  å¯¹é½è´¨é‡: global_sim={loss_for_log['global_similarity']:.3f}, "
                                   f"depth_sim={loss_for_log['depth_similarity']:.3f}")
                        logger.info(f"  è·¯ç”±å¥åº·: critical_ratio={loss_for_log['critical_ratio']:.3f}, "
                                   f"avg_weights=[{loss_for_log['avg_global_weight']:.3f}, "
                                   f"{loss_for_log['avg_depth_weight']:.3f}]")

                if args.sample_period > 0 and global_step % args.sample_period == 0:
                    sample_loss_for_log = log_sample_res(
                        text_encoder,
                        vision_encoder,
                        rdt,
                        args,
                        accelerator,
                        weight_dtype,
                        sample_dataset.get_dataset_id2name(),
                        sample_dataloader,
                        logger,
                    )
                    logger.info(sample_loss_for_log)
                    accelerator.log(sample_loss_for_log, step=global_step)

            # è®°å½•æ—¥å¿—
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            logs.update(loss_for_log)
            accelerator.log(logs, step=global_step)

            if global_step >= args.max_train_steps:
                break

    # è®­ç»ƒç»“æŸæ—¶çš„æ€»ç»“
    if accelerator.is_main_process:
        logger.info("è®­ç»ƒå®Œæˆ - æœ€ç»ˆæ€»ç»“:")
        logger.info(f"  å…¨å±€æ•™å¸ˆç±»å‹: {global_teacher_type.upper()}")  # â­ è®°å½•
        logger.info(f"  å…¨å±€ç‰¹å¾ç»´åº¦: {global_feature_dim}")
        
        if soft_routing_stats['total_samples'] > 0:
            final_critical_ratio = (soft_routing_stats['critical_timesteps'] / 
                                  soft_routing_stats['total_samples'])
            logger.info(f"  æ€»æ—¶é—´æ­¥: {soft_routing_stats['total_samples']}")
            logger.info(f"  å…³é”®æ—¶é—´æ­¥: {soft_routing_stats['critical_timesteps']}")
            logger.info(f"  æœ€ç»ˆå…³é”®æ¯”ä¾‹: {final_critical_ratio:.3f}")
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        task_name = "grasp" if task_type == 1 else "click"
        final_config = {
            'global_teacher_type': global_teacher_type,  # â­ ä¿å­˜æ•™å¸ˆç±»å‹
            'global_feature_dim': global_feature_dim,
            'task_type': task_type,
            'task_name': task_name,
            'enable_critical_annotation': enable_critical_annotation,
            'enable_soft_routing_repa': enable_soft_routing_repa,
            'final_statistics': {
                'total_timesteps': soft_routing_stats['total_samples'],
                'critical_timesteps': soft_routing_stats['critical_timesteps'],
                'critical_ratio': (soft_routing_stats['critical_timesteps'] / 
                                 soft_routing_stats['total_samples'] 
                                 if soft_routing_stats['total_samples'] > 0 else 0.0),
            },
            'training_hyperparameters': {
                'soft_routing_repa_weight': soft_routing_repa_weight,
                'learning_rate': args.learning_rate,
                'train_batch_size': args.train_batch_size,
                'max_train_steps': args.max_train_steps,
            },
        }
        
        import json
        config_filename = f"soft_routing_{global_teacher_type}_training_config.json"
        with open(os.path.join(args.output_dir, config_filename), "w") as f:
            json.dump(final_config, f, indent=2)
        
        logger.info(f"è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {os.path.join(args.output_dir, config_filename)}")
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        accelerator.unwrap_model(rdt).save_pretrained(args.output_dir)
        ema_save_path = os.path.join(args.output_dir, f"ema")
        accelerator.save_model(ema_rdt, ema_save_path)

        logger.info(f"Model saved to {args.output_dir}")

        if args.push_to_hub:
            save_model_card(
                repo_id,
                base_model=args.pretrained_model_name_or_path,
                repo_folder=args.output_dir,
            )
            upload_folder(
                repo_id=repo_id,
                folder_path=args.output_dir,
                commit_message="End of soft routing dual-teacher REPA + critical timestep training",
                token=args.hub_token,
                allow_patterns=["pytorch_model.bin", "*.json", "*.md"],
            )

    accelerator.end_training()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Train RDT with Soft Routing Dual-Teacher REPA")
    
    # åŸºç¡€è®­ç»ƒå‚æ•°
    parser.add_argument("--config_path", type=str, required=True, help="Path to config file")
    parser.add_argument("--model_config_path", type=str, required=True, help="Path to model config file")
    parser.add_argument("--pretrained_model_name_or_path", type=str, help="Path to pretrained model")
    parser.add_argument("--pretrained_text_encoder_name_or_path", type=str, help="Path to pretrained text encoder")
    parser.add_argument("--pretrained_vision_encoder_name_or_path", type=str, help="Path to pretrained vision encoder")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--mixed_precision", type=str, default="bf16", choices=["no", "fp16", "bf16"])
    parser.add_argument("--report_to", type=str, default="wandb", help="Logging platform")
    parser.add_argument("--logging_dir", type=str, default="logs")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--dataset_type", type=str, default="pretrain", choices=["pretrain", "finetune"])
    
    # æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
    parser.add_argument("--train_batch_size", type=int, default=32)
    parser.add_argument("--sample_batch_size", type=int, default=64)
    parser.add_argument("--num_train_epochs", type=int, default=3)
    parser.add_argument("--max_train_steps", type=int, default=20000)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--dataloader_num_workers", type=int, default=8)
    
    # ä¼˜åŒ–å™¨å‚æ•°
    parser.add_argument("--learning_rate", type=float, default=1e-4)
    parser.add_argument("--adam_beta1", type=float, default=0.9)
    parser.add_argument("--adam_beta2", type=float, default=0.999)
    parser.add_argument("--adam_weight_decay", type=float, default=0.01)
    parser.add_argument("--adam_epsilon", type=float, default=1e-8)
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    parser.add_argument("--use_8bit_adam", action="store_true")
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    parser.add_argument("--lr_scheduler", type=str, default="constant_with_warmup")
    parser.add_argument("--lr_warmup_steps", type=int, default=500)
    parser.add_argument("--lr_num_cycles", type=int, default=1)
    parser.add_argument("--lr_power", type=float, default=1.0)
    parser.add_argument("--scale_lr", action="store_true")
    
    # æ£€æŸ¥ç‚¹å’Œé‡‡æ ·
    parser.add_argument("--checkpointing_period", type=int, default=2500)
    parser.add_argument("--sample_period", type=int, default=100)
    parser.add_argument("--num_sample_batches", type=int, default=2)
    parser.add_argument("--checkpoints_total_limit", type=int, default=40)
    parser.add_argument("--resume_from_checkpoint", type=str, help="Path to checkpoint to resume from")
    
    # æ•°æ®å’Œé¢„å¤„ç†
    parser.add_argument("--load_from_hdf5", action="store_true", help="Load data from HDF5 files")
    parser.add_argument("--precomp_lang_embed", action="store_true", help="Use precomputed language embeddings")
    parser.add_argument("--image_aug", action="store_true", help="Enable image augmentation")
    parser.add_argument("--cond_mask_prob", type=float, default=0.1, help="Condition masking probability")
    parser.add_argument("--cam_ext_mask_prob", type=float, default=-1.0, help="External camera masking probability")
    parser.add_argument("--state_noise_snr", type=float, help="State noise SNR")
    
    # ç³»ç»Ÿé…ç½®
    parser.add_argument("--gradient_checkpointing", action="store_true")
    parser.add_argument("--allow_tf32", action="store_true")
    parser.add_argument("--set_grads_to_none", action="store_true")
    parser.add_argument("--deepspeed", type=str, help="Path to DeepSpeed config")
    
    # Hubç›¸å…³
    parser.add_argument("--push_to_hub", action="store_true")
    parser.add_argument("--hub_model_id", type=str, help="Hub model ID")
    parser.add_argument("--hub_token", type=str, help="Hub token")
    
    # è½¯è·¯ç”±å‚æ•°
    parser.add_argument("--CONFIG_NAME", type=str, default="soft_routing", help="Configuration name for logging")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger = logging.getLogger(__name__)
    
    # å¼€å§‹è®­ç»ƒ
    train(args, logger)
    
    
    
    
    
    
    
    
    
    
    
    
# import copy
# import logging
# import math
# import os
# from pathlib import Path

# import diffusers
# import torch
# import torch.utils.checkpoint
# import transformers
# import yaml
# from accelerate import Accelerator
# from accelerate.utils import DeepSpeedPlugin, ProjectConfiguration, set_seed
# from diffusers.optimization import get_scheduler
# from diffusers.utils import is_wandb_available
# from huggingface_hub import create_repo, upload_folder
# from tqdm.auto import tqdm
# from safetensors.torch import load_model

# from models.ema_model import EMAModel
# from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
# from models.multimodal_encoder.t5_encoder import T5Embedder
# from models.rdt_runner import RDTRunner
# from train.dataset import DataCollatorForVLAConsumerDataset, VLAConsumerDataset
# from train.sample import log_sample_res

# # å¯¼å…¥DINOv2å’ŒDepthAnythingV2ç¼–ç å™¨
# from models.multimodal_encoder.dinov2_encoder import create_dinov2_encoder
# from models.multimodal_encoder.depth_encoder import create_depth_encoder



# if is_wandb_available():
#     import wandb


# def save_model_card(repo_id: str, base_model=str, repo_folder=None):
#     yaml_header = f"""
# ---
# license: mit
# base_model: {base_model}
# language:
# - en
# pipeline_tag: robotics
# library_name: transformers
# tags:
# - robotics
# - pytorch
# - multimodal
# - pretraining
# - vla
# - diffusion
# - rdt
# - soft-routing
# - dual-teachers
# - critical-timestep
# - binary-labels
# ---
#     """
#     model_card = f"""
# # RDT with Soft Routing Dual-Teacher REPA - {repo_id}

# This is a RDT model with soft routing dual-teacher REPA alignment loss, task-driven critical timestep annotation 
# derived from {base_model}. The weights were trained using [RDT](https://rdt-robotics.github.io/rdt-robotics/) 
# with advanced soft routing multi-modal alignment strategies.

# ## Key Features
# - **Soft Routing Strategy**: Rule-driven weight allocation based on binary critical timestep labels
# - **Critical Timestep Annotation**: Task-driven annotation for precise temporal alignment
# - **Dual Visual Teachers**: DINOv2 (global semantic) + DepthAnythingV2 (depth geometric)
# - **Neural Weight Adjustment**: Optional fine-tuning with temporal smoothing
# - **Contrastive Learning**: Enhanced feature alignment with contrastive loss

# ## Weight Allocation Strategy
# - **Critical Timesteps (1)**: Global 25%, Depth 75% - Focus on precise manipulation
# - **Non-Critical Timesteps (0)**: Global 75%, Depth 25% - Focus on scene understanding

# ## Architecture Components
# 1. **Binary Label Soft Router**: Rule-driven weight allocation with optional neural adjustment
# 2. **Dual Visual Teachers**: DINOv2 (global) + DepthAnythingV2 (geometric)
# 3. **Temporal Smoothing**: Prevents sudden weight transitions
# 4. **Contrastive Learning**: Enhances feature alignment quality

# ## Task Types Supported
# - **Grasp Tasks (task_type=1)**: Deceleration â†’ Gripper closing alignment
# - **Click Tasks (task_type=2)**: Gripper closing â†’ Deceleration alignment
# """
#     with open(os.path.join(repo_folder, "README.md"), "w") as f:
#         f.write(yaml_header + model_card)


# def check_critical_alerts(metrics, global_step, logger):
#     """
#     æ£€æŸ¥å…³é”®å¼‚å¸¸æƒ…å†µå¹¶å‘å‡ºé¢„è­¦
#     """
#     alerts = []
    
#     # 1. ç‰¹å¾å¯¹é½å¤±æ•ˆ
#     if metrics.get('global_similarity', 1.0) < 0.3 and global_step > 1000:
#         alerts.append("WARNING: Low global similarity - feature alignment may be failing")
    
#     # 2. æƒé‡åˆ†é…ä¸¥é‡åç¦»
#     if 'critical_global_weight' in metrics:
#         expected_critical_global = 0.25
#         actual = metrics['critical_global_weight']
#         if abs(actual - expected_critical_global) > 0.2:  # åç¦»è¶…è¿‡20%
#             alerts.append(f"WARNING: Critical weight deviation - expected {expected_critical_global}, got {actual:.3f}")
    
#     # 3. æƒé‡è°ƒæ•´è¿‡åº¦
#     if metrics.get('weight_drift', 0) > 0.15:
#         alerts.append("WARNING: Excessive weight drift - neural adjustment may be too aggressive")
    
#     # 4. æ•°æ®ä¸å¹³è¡¡
#     critical_ratio = metrics.get('critical_ratio', 0.3)
#     if critical_ratio < 0.1 or critical_ratio > 0.6:
#         alerts.append(f"WARNING: Critical ratio imbalance - {critical_ratio:.3f}")
    
#     # è®°å½•è­¦å‘Š
#     for alert in alerts:
#         logger.warning(alert)
    
#     return alerts


# def train(args, logger):
#     # Read the config
#     with open(args.config_path, "r") as fp:
#         config = yaml.safe_load(fp)

#     with open(args.model_config_path, "r") as f:
#         model_config = yaml.safe_load(f)
    
#     args.output_dir = model_config["checkpoint_path"]
#     logging_dir = Path(args.output_dir, args.logging_dir)

#     accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)
#     accelerator = Accelerator(
#         deepspeed_plugin=(DeepSpeedPlugin(hf_ds_config=args.deepspeed) if args.deepspeed is not None else None),
#         gradient_accumulation_steps=args.gradient_accumulation_steps,
#         mixed_precision=args.mixed_precision,
#         log_with=args.report_to,
#         project_dir=logging_dir,
#         project_config=accelerator_project_config,
#     )

#     if args.report_to == "wandb":
#         if not is_wandb_available():
#             raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

#     # Make one log on every process with the configuration for debugging.
#     logging.basicConfig(
#         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
#         datefmt="%m/%d/%Y %H:%M:%S",
#         level=logging.INFO,
#     )
#     logger.info(accelerator.state, main_process_only=False)
#     if accelerator.is_local_main_process:
#         transformers.utils.logging.set_verbosity_warning()
#         diffusers.utils.logging.set_verbosity_info()
#     else:
#         transformers.utils.logging.set_verbosity_error()
#         diffusers.utils.logging.set_verbosity_error()

#     # If passed along, set the training seed now.
#     if args.seed is not None:
#         set_seed(args.seed)

#     # Handle the repository creation
#     if accelerator.is_main_process:
#         if args.output_dir is not None:
#             os.makedirs(args.output_dir, exist_ok=True)

#         if args.push_to_hub:
#             repo_id = create_repo(
#                 repo_id=args.hub_model_id or Path(args.output_dir).name,
#                 exist_ok=True,
#                 token=args.hub_token,
#             ).repo_id

#     # For mixed precision training
#     weight_dtype = torch.float32
#     if accelerator.mixed_precision == "fp16":
#         weight_dtype = torch.float16
#     elif accelerator.mixed_precision == "bf16":
#         weight_dtype = torch.bfloat16

#     # ä»æ¨¡å‹é…ç½®è¯»å–è§†è§‰èåˆå‚æ•°
#     enable_vision_fusion = model_config.get("enable_vision_fusion", True)
#     vision_fusion_type = model_config.get("vision_fusion_type", "cross_attention")
#     use_dinov2_features = model_config.get("use_dinov2_features", True)
#     use_depth_features = model_config.get("use_depth_features", True)
    
#     # ğŸ”´ å¼ºåˆ¶å…³é—­REPAå’Œå…³é”®æ—¶é—´æ®µæ ‡æ³¨
#     enable_soft_routing_repa = False
#     enable_critical_annotation = False
    
#     # ğŸ”§ ä¿®å¤ï¼šå®šä¹‰æ‰€æœ‰å¯èƒ½ç”¨åˆ°çš„å˜é‡ï¼Œæä¾›é»˜è®¤å€¼
#     task_type = model_config.get("task_type", 1)
#     critical_annotation_config = model_config.get("critical_annotation_config", {})
#     soft_routing_config = model_config.get("soft_routing_config", {})
#     soft_routing_repa_weight = model_config.get("soft_routing_repa_weight", 0.2)
    
#     logger.info(f"ğŸ”§ è§†è§‰èåˆé…ç½®:")
#     logger.info(f"   - è§†è§‰èåˆå¯ç”¨: {enable_vision_fusion}")
#     logger.info(f"   - èåˆç±»å‹: {vision_fusion_type}")
#     logger.info(f"   - DINOv2ç‰¹å¾: {use_dinov2_features}")
#     logger.info(f"   - Depthç‰¹å¾: {use_depth_features}")
#     logger.info(f"   - REPAå¯¹é½: {enable_soft_routing_repa} (å·²å…³é—­)")
#     logger.info(f"   - å…³é”®æ—¶é—´æ®µæ ‡æ³¨: {enable_critical_annotation} (å·²å…³é—­)")

#     # æ–‡æœ¬ç¼–ç å™¨
#     if args.precomp_lang_embed:
#         tokenizer, text_encoder = None, None
#     else:
#         text_embedder = T5Embedder(
#             from_pretrained=args.pretrained_text_encoder_name_or_path,
#             model_max_length=config["dataset"]["tokenizer_max_length"],
#             device=accelerator.device,
#         )
#         tokenizer, text_encoder = text_embedder.tokenizer, text_embedder.model

#     # SigLIPç¼–ç å™¨ï¼ˆä¸»å¹²ï¼‰
#     vision_encoder = SiglipVisionTower(
#         vision_tower=args.pretrained_vision_encoder_name_or_path, 
#         args=None
#     )
#     image_processor = vision_encoder.image_processor

#     # ğŸ†• DINOv2ç¼–ç å™¨ï¼ˆå…¨å±€è¯­ä¹‰patch tokensï¼‰
#     dinov2_encoder = None
#     if use_dinov2_features and enable_vision_fusion:
#         logger.info("ğŸ“¦ åŠ è½½DINOv2ç¼–ç å™¨ (Patch Tokens)...")
#         dinov2_encoder = create_dinov2_encoder(
#             model_size="large", 
#             select_feature="patch"  # ğŸ”´ åªè¦patch tokens
#         )
#         dinov2_encoder.to(accelerator.device, dtype=weight_dtype)
#         dinov2_encoder.print_model_info()

#     # ğŸ†• DepthAnythingV2ç¼–ç å™¨ï¼ˆæ·±åº¦patch tokensï¼‰
#     depth_encoder = None
#     if use_depth_features and enable_vision_fusion:
#         logger.info("ğŸ“¦ åŠ è½½DepthAnythingV2ç¼–ç å™¨ (Patch Tokens)...")
#         depth_encoder = create_depth_encoder(
#             model_size="metric_large",
#             feature_dim=1024,
#             device=accelerator.device,
#             use_metric_model=True
#         )
#         depth_encoder.to(accelerator.device, dtype=weight_dtype)
#         depth_encoder.print_model_info()
#     logger.info("ğŸ”¨ æ„å»ºå¸¦è§†è§‰èåˆçš„RDTæ¨¡å‹...")
#     img_cond_len = (config["common"]["img_history_size"] * 
#                     config["common"]["num_cameras"] *
#                     vision_encoder.num_patches)
    
#     rdt = RDTRunner(
#         action_dim=config["common"]["state_dim"],
#         pred_horizon=config["common"]["action_chunk_size"],
#         config=config["model"],
#         lang_token_dim=config["model"]["lang_token_dim"],
#         img_token_dim=config["model"]["img_token_dim"],
#         state_token_dim=config["model"]["state_token_dim"],
#         max_lang_cond_len=config["dataset"]["tokenizer_max_length"],
#         img_cond_len=img_cond_len,
#         img_pos_embed_config=[
#             ("image", (
#                 config["common"]["img_history_size"],
#                 config["common"]["num_cameras"],
#                 -vision_encoder.num_patches,
#             )),
#         ],
#         lang_pos_embed_config=[
#             ("lang", -config["dataset"]["tokenizer_max_length"]),
#         ],
#         dtype=weight_dtype,
#         # ğŸ†• è§†è§‰èåˆé…ç½®
#         enable_vision_fusion=enable_vision_fusion,
#         vision_fusion_type=vision_fusion_type,
#         dinov2_feature_dim=1024,
#         depth_feature_dim=1024,
#         fusion_num_heads=8,
#         fusion_dropout=0.1,
#         img_history_size=config["common"]["img_history_size"],  # ğŸ†• ä¼ å…¥
#         num_cameras=config["common"]["num_cameras"],            # ğŸ†• ä¼ å…¥
#     )
#     # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
#     if args.pretrained_model_name_or_path and os.path.isfile(args.pretrained_model_name_or_path):
#         logger.info(f"Loading pretrained weights: {args.pretrained_model_name_or_path}")
#         ckpt = torch.load(args.pretrained_model_name_or_path, map_location="cpu")

#         if isinstance(ckpt, dict) and "module" in ckpt:
#             pretrained_sd = ckpt["module"]
#         elif isinstance(ckpt, dict) and "state_dict" in ckpt:
#             pretrained_sd = ckpt["state_dict"]
#         else:
#             pretrained_sd = ckpt

#         own_sd = rdt.state_dict()
#         filtered = {}
#         for k, v in pretrained_sd.items():
#             if k in own_sd and v.shape == own_sd[k].shape:
#                 filtered[k] = v
#             else:
#                 logger.debug(f"Skipping parameter {k}: checkpoint {tuple(v.shape)} vs model {tuple(own_sd.get(k, v).shape)}")

#         rdt.load_state_dict(filtered, strict=False)
#         logger.info("Loaded matching pretrained weights; others remain randomly initialized")
#     else:
#         logger.info("Only using config; skipping pretrained weight loading")

#     # EMAæ¨¡å‹
#     ema_rdt = copy.deepcopy(rdt)
#     ema_model = EMAModel(
#         ema_rdt,
#         update_after_step=config["model"]["ema"]["update_after_step"],
#         inv_gamma=config["model"]["ema"]["inv_gamma"],
#         power=config["model"]["ema"]["power"],
#         min_value=config["model"]["ema"]["min_value"],
#         max_value=config["model"]["ema"]["max_value"],
#     )

#     # ä¿å­˜é’©å­
#     def save_model_hook(models, weights, output_dir):
#         if accelerator.is_main_process:
#             for model in models:
#                 model_to_save = model.module if hasattr(model, "module") else model
#                 if isinstance(model_to_save, type(accelerator.unwrap_model(rdt))):
#                     model_to_save.save_pretrained(output_dir)

#     accelerator.register_save_state_pre_hook(save_model_hook)

#     if args.gradient_checkpointing:
#         raise NotImplementedError("Gradient checkpointing is not yet implemented.")

#     # Enable TF32 for faster training on Ampere GPUs
#     if args.allow_tf32:
#         torch.backends.cuda.matmul.allow_tf32 = True

#     if args.scale_lr:
#         args.learning_rate = (args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size *
#                               accelerator.num_processes)

#     # ä¼˜åŒ–å™¨
#     if args.use_8bit_adam:
#         try:
#             import bitsandbytes as bnb
#         except ImportError:
#             raise ImportError("To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.")
#         optimizer_class = bnb.optim.AdamW8bit
#     else:
#         optimizer_class = torch.optim.AdamW

#     params_to_optimize = rdt.parameters()
#     optimizer = optimizer_class(
#         params_to_optimize,
#         lr=args.learning_rate,
#         betas=(args.adam_beta1, args.adam_beta2),
#         weight_decay=args.adam_weight_decay,
#         eps=args.adam_epsilon,
#     )

#     train_dataset = VLAConsumerDataset(
#         model_config_path=args.model_config_path,
#         config=config["dataset"],
#         tokenizer=tokenizer,
#         image_processor=image_processor,
#         num_cameras=config["common"]["num_cameras"],
#         img_history_size=config["common"]["img_history_size"],
#         dataset_type=args.dataset_type,
#         image_aug=args.image_aug,
#         cond_mask_prob=args.cond_mask_prob,
#         cam_ext_mask_prob=args.cam_ext_mask_prob,
#         state_noise_snr=args.state_noise_snr,
#         use_hdf5=args.load_from_hdf5,
#         use_precomp_lang_embed=args.precomp_lang_embed,
#         use_dinov2_features=use_dinov2_features,
#         use_depth_features=use_depth_features,
#         # ğŸ”´ å…³é—­å…³é”®æ—¶é—´æ®µæ ‡æ³¨
#         enable_critical_annotation=False,
#     )
    
#     sample_dataset = VLAConsumerDataset(
#         model_config_path=args.model_config_path,
#         config=config["dataset"],
#         tokenizer=tokenizer,
#         image_processor=image_processor,
#         num_cameras=config["common"]["num_cameras"],
#         img_history_size=config["common"]["img_history_size"],
#         dataset_type=args.dataset_type,
#         image_aug=False,
#         cond_mask_prob=0,
#         cam_ext_mask_prob=-1,
#         state_noise_snr=None,
#         use_hdf5=args.load_from_hdf5,
#         use_precomp_lang_embed=args.precomp_lang_embed,
#         use_dinov2_features=use_dinov2_features,
#         use_depth_features=use_depth_features,
#         enable_critical_annotation=False,
#     )

#     data_collator = DataCollatorForVLAConsumerDataset(tokenizer)

#     train_dataloader = torch.utils.data.DataLoader(
#         train_dataset,
#         batch_size=args.train_batch_size,
#         shuffle=True,
#         collate_fn=data_collator,
#         num_workers=args.dataloader_num_workers,
#         pin_memory=True,
#         persistent_workers=True,
#     )
    
#     sample_dataloader = torch.utils.data.DataLoader(
#         sample_dataset,
#         batch_size=args.sample_batch_size,
#         shuffle=True,
#         collate_fn=data_collator,
#         num_workers=args.dataloader_num_workers,
#         pin_memory=True,
#         persistent_workers=True,
#     )

#     # å­¦ä¹ ç‡è°ƒåº¦å™¨
#     overrode_max_train_steps = False
#     num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
#     if args.max_train_steps is None:
#         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
#         overrode_max_train_steps = True

#     lr_scheduler = get_scheduler(
#         args.lr_scheduler,
#         optimizer=optimizer,
#         num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
#         num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
#         num_cycles=args.lr_num_cycles,
#         power=args.lr_power,
#     )

#     # å‡†å¤‡è®­ç»ƒ
#     rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler = (
#         accelerator.prepare(rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler)
#     )

#     ema_rdt.to(accelerator.device, dtype=weight_dtype)

#     if text_encoder is not None:
#         text_encoder.to(accelerator.device, dtype=weight_dtype)

#     if vision_encoder is not None:
#         vision_encoder.vision_tower.to(accelerator.device, dtype=weight_dtype)

#     # é‡æ–°è®¡ç®—è®­ç»ƒæ­¥æ•°
#     num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
#     if overrode_max_train_steps:
#         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
#     args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

#     # åˆå§‹åŒ–è¿½è¸ªå™¨
#     if accelerator.is_main_process:
#         tracker_config = vars(args).copy()
        
#         # åŸºç¡€é…ç½®
#         tracker_config.update({
#             'enable_vision_fusion': enable_vision_fusion,
#             'vision_fusion_type': vision_fusion_type,
#             'use_dinov2_features': use_dinov2_features,
#             'use_depth_features': use_depth_features,
#             'enable_soft_routing_repa': enable_soft_routing_repa,
#             'enable_critical_annotation': enable_critical_annotation,
#         })
        
#         # åªåœ¨å¯ç”¨æ—¶æ·»åŠ ç›¸å…³é…ç½®
#         if enable_critical_annotation:
#             tracker_config['task_type'] = task_type
#             tracker_config['critical_annotation_config'] = critical_annotation_config
        
#         if enable_soft_routing_repa:
#             tracker_config['soft_routing_config'] = soft_routing_config
        
#         # ğŸ”§ æ ¹æ®æ¨¡å¼è®¾ç½®é¡¹ç›®åå’Œè¿è¡Œå
#         if enable_vision_fusion:
#             project_name = "VLA_Vision_Fusion"
#             run_name = f"RDT_VisionFusion_{vision_fusion_type}_{args.CONFIG_NAME}"
#         elif enable_soft_routing_repa:
#             project_name = "VLA_Soft_Routing_Dual_Teacher_REPA"
#             task_name = "grasp" if task_type == 1 else "click"
#             run_name = f"RDT_SoftRouting_{task_name}_{args.CONFIG_NAME}"
#         else:
#             project_name = "VLA_Training"
#             run_name = f"RDT_{args.CONFIG_NAME}"
        
#         accelerator.init_trackers(
#             project_name,
#             config=tracker_config,
#             init_kwargs={"wandb": {
#                 "name": run_name,
#             }},
#         )

#     # è®­ç»ƒä¿¡æ¯
#     total_batch_size = (args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps)

#     logger.info("***** Starting Soft Routing Dual-Teacher REPA Training *****")
#     logger.info(f"  Num examples = {len(train_dataset)}")
#     logger.info(f"  Num Epochs = {args.num_train_epochs}")
#     logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")
#     logger.info(f"  Total train batch size = {total_batch_size}")
#     logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")
#     logger.info(f"  Total optimization steps = {args.max_train_steps}")
#     task_name = "grasp" if task_type == 1 else ("click" if task_type == 2 else "unknown")
#     logger.info(f"  Task Type = {task_name}")
#     logger.info(f"  Routing Strategy = Rule-driven + Optional Neural Adjustment")
    
#     global_step = 0
#     first_epoch = 0

#     # å¯èƒ½ä»æ£€æŸ¥ç‚¹æ¢å¤
#     if args.resume_from_checkpoint:
#         if args.resume_from_checkpoint != "latest":
#             path = os.path.basename(args.resume_from_checkpoint)
#         else:
#             dirs = os.listdir(args.output_dir)
#             dirs = [d for d in dirs if d.startswith("checkpoint")]
#             dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
#             path = dirs[-1] if len(dirs) > 0 else None

#         if path is None:
#             accelerator.print(f"Checkpoint '{args.resume_from_checkpoint}' not found. Starting new training.")
#             args.resume_from_checkpoint = None
#         else:
#             accelerator.print(f"Resuming from checkpoint: {path}")
#             try:
#                 accelerator.load_state(os.path.join(args.output_dir, path))
#             except:
#                 logger.info("Failed to restore training state. Trying to load model checkpoint only.")
#                 checkpoint = torch.load(
#                     os.path.join(args.output_dir, path, "pytorch_model", "mp_rank_00_model_states.pt"))
#                 rdt.module.load_state_dict(checkpoint["module"])

#             load_model(ema_rdt, os.path.join(args.output_dir, path, "ema", "model.safetensors"))
#             global_step = int(path.split("-")[1])

#             resume_global_step = global_step * args.gradient_accumulation_steps
#             first_epoch = global_step // num_update_steps_per_epoch
#             resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)

#     # è¿›åº¦æ¡
#     progress_bar = tqdm(
#         range(global_step, args.max_train_steps),
#         disable=not accelerator.is_local_main_process,
#     )
#     progress_bar.set_description("Steps")

#     # ç²¾ç®€ç‰ˆç»Ÿè®¡å˜é‡
#     soft_routing_stats = {
#         'total_samples': 0,
#         'critical_timesteps': 0,
#     }
    
#     # è®­ç»ƒå¾ªç¯
#     for epoch in range(first_epoch, args.num_train_epochs):
#         rdt.train()
        
#         # æ¯ä¸ªepochå¼€å§‹æ—¶é‡ç½®batchè®¡æ•°
#         accelerator.unwrap_model(rdt).reset_batch_count()

#         if args.resume_from_checkpoint and epoch == first_epoch:
#             progress_bar.update(resume_step // args.gradient_accumulation_steps)

#         for batch in train_dataloader:
#             with accelerator.accumulate(rdt):
#                 # å‡†å¤‡è¾“å…¥æ•°æ®
#                 images = batch["images"].to(dtype=weight_dtype)
#                 states = batch["states"].to(dtype=weight_dtype)[:, -1:, :]
#                 actions = batch["actions"].to(dtype=weight_dtype)
#                 state_elem_mask = batch["state_elem_mask"].to(dtype=weight_dtype)
#                 ctrl_freqs = batch["ctrl_freqs"]

#                 # ========================================
#                 # ğŸ†• ä¸‰è·¯è§†è§‰ç‰¹å¾æå–
#                 # ========================================
                
#                 with torch.no_grad():
#                     # 1ï¸âƒ£ SigLIPç¼–ç ï¼ˆä¸»å¹²ï¼‰
#                     batch_size, _, C, H, W = images.shape
#                     image_embeds = vision_encoder(
#                         images.reshape(-1, C, H, W)
#                     ).detach()
#                     image_embeds = image_embeds.reshape(
#                         (batch_size, -1, vision_encoder.hidden_size)
#                     )  # (B, img_history_size*num_cameras*729, 1152)

#                     # 2ï¸âƒ£ æ–‡æœ¬ç¼–ç 
#                     lang_attn_mask = batch["lang_attn_mask"]
#                     text_embeds = (
#                         batch["lang_embeds"].to(dtype=weight_dtype) 
#                         if args.precomp_lang_embed 
#                         else text_encoder(
#                             input_ids=batch["input_ids"], 
#                             attention_mask=lang_attn_mask
#                         )["last_hidden_state"].detach()
#                     )

#                     # 3ï¸âƒ£ DINOv2ç¼–ç ï¼ˆpatch tokensï¼‰
#                     dinov2_features = None
#                     if dinov2_encoder is not None and "dinov2_images" in batch:
#                         dinov2_images = batch["dinov2_images"].to(dtype=weight_dtype)
#                         dinov2_input = dinov2_images[:, 0]  # (B, 3, 518, 518)
#                         dinov2_features = dinov2_encoder(dinov2_input)  # (B, 1369, 1024)

#                     # 4ï¸âƒ£ DepthAnythingV2ç¼–ç ï¼ˆpatch tokensï¼‰
#                     depth_features = None
#                     if depth_encoder is not None and "depth_images" in batch:
#                         depth_images = batch["depth_images"].to(dtype=weight_dtype)
#                         depth_input = depth_images[:, 0]  # (B, 3, 518, 518)
#                         depth_features, _ = depth_encoder(depth_input)  # (B, 1370, 1024)

#                 # ========================================
#                 # ğŸ†• è®¡ç®—æŸå¤±ï¼ˆè§†è§‰èåˆåœ¨å†…éƒ¨å®Œæˆï¼‰
#                 # ========================================
                
#                 state_elem_mask = state_elem_mask.unsqueeze(1)
#                 loss = accelerator.unwrap_model(rdt).compute_loss(
#                     lang_tokens=text_embeds,
#                     lang_attn_mask=lang_attn_mask,
#                     img_tokens=image_embeds,  # SigLIP tokens
#                     state_tokens=states,
#                     action_gt=actions,
#                     action_mask=state_elem_mask,
#                     ctrl_freqs=ctrl_freqs,
#                     # ğŸ†• ä¼ å…¥DINOv2å’ŒDepthç‰¹å¾
#                     dinov2_features=dinov2_features,
#                     depth_features=depth_features,
#                 )

#                 # åå‘ä¼ æ’­ï¼ˆä¿æŒä¸å˜ï¼‰
#                 accelerator.backward(loss)
#                 if accelerator.sync_gradients:
#                     params_to_clip = rdt.parameters()
#                     accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
#                 optimizer.step()
#                 lr_scheduler.step()
#                 optimizer.zero_grad(set_to_none=args.set_grads_to_none)

#             # EMAæ›´æ–°ï¼ˆä¿æŒä¸å˜ï¼‰
#             ema_model.step(accelerator.unwrap_model(rdt))

#             # æ£€æŸ¥ç‚¹å’Œæ—¥å¿—ï¼ˆä¿æŒä¸å˜ï¼‰
#             if accelerator.sync_gradients:
#                 progress_bar.update(1)
#                 global_step += 1

#                 if global_step % args.checkpointing_period == 0:
#                     save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
#                     accelerator.save_state(save_path)
#                     ema_save_path = os.path.join(save_path, f"ema")
#                     accelerator.save_model(ema_rdt, ema_save_path)
#                     logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {save_path}")

#                 # ğŸ†• é‡‡æ ·è¯„ä¼°æ—¶ä¹Ÿä¼ å…¥ç¼–ç å™¨
#                 if args.sample_period > 0 and global_step % args.sample_period == 0:
#                     sample_loss_for_log = log_sample_res(
#                     text_encoder,
#                     vision_encoder,
#                     rdt,
#                     args,
#                     accelerator,
#                     weight_dtype,
#                     sample_dataset.get_dataset_id2name(),
#                     sample_dataloader,
#                     logger,
#                     # ğŸ†• ä¼ å…¥é¢å¤–çš„ç¼–ç å™¨
#                     dinov2_encoder=dinov2_encoder,
#                     depth_encoder=depth_encoder,
#                     )
#                     logger.info(sample_loss_for_log)
#                     accelerator.log(sample_loss_for_log, step=global_step)

#             # è®°å½•æ—¥å¿—
#             logs = {
#                 "loss": loss.detach().item(), 
#                 "lr": lr_scheduler.get_last_lr()[0]
#             }
#             accelerator.log(logs, step=global_step)
#             # åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ è°ƒè¯•
#             print(f"dinov2_images shape: {batch['dinov2_images'].shape if 'dinov2_images' in batch else 'None'}")
#             print(f"depth_images shape: {batch['depth_images'].shape if 'depth_images' in batch else 'None'}")
#             print(f"images shape: {batch['images'].shape}")
#             if global_step >= args.max_train_steps:
#                 break

#     # è®­ç»ƒç»“æŸæ—¶çš„ç²¾ç®€ç»Ÿè®¡æ€»ç»“
#     if accelerator.is_main_process:
#         logger.info("Training Complete - Final Summary:")
        
#         if soft_routing_stats['total_samples'] > 0:
#             final_critical_ratio = soft_routing_stats['critical_timesteps'] / soft_routing_stats['total_samples']
#             logger.info(f"  Total Timesteps Processed: {soft_routing_stats['total_samples']}")
#             logger.info(f"  Critical Timesteps: {soft_routing_stats['critical_timesteps']}")
#             logger.info(f"  Final Critical Ratio: {final_critical_ratio:.3f}")
        
#         # è·å–æœ€ç»ˆçš„è½¯è·¯ç”±é…ç½®
#         final_soft_routing_stats = accelerator.unwrap_model(rdt).get_soft_routing_statistics()
#         if final_soft_routing_stats:
#             logger.info(f"  Final Routing Temperature: {final_soft_routing_stats.get('routing_temperature', 1.0):.4f}")
#             logger.info(f"  Neural Adjustment Enabled: {final_soft_routing_stats.get('enable_neural_adjustment', False)}")
#             logger.info(f"  Temporal Smoothing: {final_soft_routing_stats.get('temporal_smoothing', 0.0):.2f}")

#         # ä¿å­˜ç²¾ç®€ç‰ˆè®­ç»ƒé…ç½®
#         task_name = "grasp" if task_type == 1 else ("click" if task_type == 2 else "unknown")
#         final_config = {
#             'task_type': task_type,
#             'task_name': task_name,  # ğŸ”§ ä½¿ç”¨å˜é‡è€Œä¸æ˜¯ TaskType
#             'enable_critical_annotation': enable_critical_annotation,
#             'enable_soft_routing_repa': enable_soft_routing_repa,
#             'enable_vision_fusion': enable_vision_fusion,
#             'vision_fusion_type': vision_fusion_type,
#             'final_statistics': {
#                 'total_timesteps': soft_routing_stats['total_samples'],
#                 'critical_timesteps': soft_routing_stats['critical_timesteps'],
#                 'critical_ratio': soft_routing_stats['critical_timesteps'] / soft_routing_stats['total_samples'] if soft_routing_stats['total_samples'] > 0 else 0.0,
#             },
#             'training_hyperparameters': {
#                 'soft_routing_repa_weight': soft_routing_repa_weight,
#                 'learning_rate': args.learning_rate,
#                 'train_batch_size': args.train_batch_size,
#                 'max_train_steps': args.max_train_steps,
#             },
#             'soft_routing_final_state': final_soft_routing_stats,
#         }
        
#         import json
#         config_filename = "vision_fusion_training_config.json" if enable_vision_fusion else "soft_routing_training_config.json"
#         with open(os.path.join(args.output_dir, config_filename), "w") as f:
#             json.dump(final_config, f, indent=2)
        
#         logger.info(f"Training config saved to: {os.path.join(args.output_dir, config_filename)}")

#     # ä¿å­˜æœ€ç»ˆæ¨¡å‹
#     accelerator.wait_for_everyone()
#     if accelerator.is_main_process:
#         accelerator.unwrap_model(rdt).save_pretrained(args.output_dir)
#         ema_save_path = os.path.join(args.output_dir, f"ema")
#         accelerator.save_model(ema_rdt, ema_save_path)

#         logger.info(f"Model saved to {args.output_dir}")

#         if args.push_to_hub:
#             save_model_card(
#                 repo_id,
#                 base_model=args.pretrained_model_name_or_path,
#                 repo_folder=args.output_dir,
#             )
#             upload_folder(
#                 repo_id=repo_id,
#                 folder_path=args.output_dir,
#                 commit_message="End of soft routing dual-teacher REPA + critical timestep training",
#                 token=args.hub_token,
#                 allow_patterns=["pytorch_model.bin", "*.json", "*.md"],
#             )

#     accelerator.end_training()


# if __name__ == "__main__":
#     import argparse
    
#     parser = argparse.ArgumentParser(description="Train RDT with Soft Routing Dual-Teacher REPA")
    
#     # åŸºç¡€è®­ç»ƒå‚æ•°
#     parser.add_argument("--config_path", type=str, required=True, help="Path to config file")
#     parser.add_argument("--model_config_path", type=str, required=True, help="Path to model config file")
#     parser.add_argument("--pretrained_model_name_or_path", type=str, help="Path to pretrained model")
#     parser.add_argument("--pretrained_text_encoder_name_or_path", type=str, help="Path to pretrained text encoder")
#     parser.add_argument("--pretrained_vision_encoder_name_or_path", type=str, help="Path to pretrained vision encoder")
    
#     # è®­ç»ƒé…ç½®
#     parser.add_argument("--mixed_precision", type=str, default="bf16", choices=["no", "fp16", "bf16"])
#     parser.add_argument("--report_to", type=str, default="wandb", help="Logging platform")
#     parser.add_argument("--logging_dir", type=str, default="logs")
#     parser.add_argument("--seed", type=int, default=42, help="Random seed")
#     parser.add_argument("--dataset_type", type=str, default="pretrain", choices=["pretrain", "finetune"])
    
#     # æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
#     parser.add_argument("--train_batch_size", type=int, default=32)
#     parser.add_argument("--sample_batch_size", type=int, default=64)
#     parser.add_argument("--num_train_epochs", type=int, default=3)
#     parser.add_argument("--max_train_steps", type=int, default=20000)
#     parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
#     parser.add_argument("--dataloader_num_workers", type=int, default=8)
    
#     # ä¼˜åŒ–å™¨å‚æ•°
#     parser.add_argument("--learning_rate", type=float, default=1e-4)
#     parser.add_argument("--adam_beta1", type=float, default=0.9)
#     parser.add_argument("--adam_beta2", type=float, default=0.999)
#     parser.add_argument("--adam_weight_decay", type=float, default=0.01)
#     parser.add_argument("--adam_epsilon", type=float, default=1e-8)
#     parser.add_argument("--max_grad_norm", type=float, default=1.0)
#     parser.add_argument("--use_8bit_adam", action="store_true")
    
#     # å­¦ä¹ ç‡è°ƒåº¦å™¨
#     parser.add_argument("--lr_scheduler", type=str, default="constant_with_warmup")
#     parser.add_argument("--lr_warmup_steps", type=int, default=500)
#     parser.add_argument("--lr_num_cycles", type=int, default=1)
#     parser.add_argument("--lr_power", type=float, default=1.0)
#     parser.add_argument("--scale_lr", action="store_true")
    
#     # æ£€æŸ¥ç‚¹å’Œé‡‡æ ·
#     parser.add_argument("--checkpointing_period", type=int, default=2500)
#     parser.add_argument("--sample_period", type=int, default=100)
#     parser.add_argument("--num_sample_batches", type=int, default=2)
#     parser.add_argument("--checkpoints_total_limit", type=int, default=40)
#     parser.add_argument("--resume_from_checkpoint", type=str, help="Path to checkpoint to resume from")
    
#     # æ•°æ®å’Œé¢„å¤„ç†
#     parser.add_argument("--load_from_hdf5", action="store_true", help="Load data from HDF5 files")
#     parser.add_argument("--precomp_lang_embed", action="store_true", help="Use precomputed language embeddings")
#     parser.add_argument("--image_aug", action="store_true", help="Enable image augmentation")
#     parser.add_argument("--cond_mask_prob", type=float, default=0.1, help="Condition masking probability")
#     parser.add_argument("--cam_ext_mask_prob", type=float, default=-1.0, help="External camera masking probability")
#     parser.add_argument("--state_noise_snr", type=float, help="State noise SNR")
    
#     # ç³»ç»Ÿé…ç½®
#     parser.add_argument("--gradient_checkpointing", action="store_true")
#     parser.add_argument("--allow_tf32", action="store_true")
#     parser.add_argument("--set_grads_to_none", action="store_true")
#     parser.add_argument("--deepspeed", type=str, help="Path to DeepSpeed config")
    
#     # Hubç›¸å…³
#     parser.add_argument("--push_to_hub", action="store_true")
#     parser.add_argument("--hub_model_id", type=str, help="Hub model ID")
#     parser.add_argument("--hub_token", type=str, help="Hub token")
    
#     # è½¯è·¯ç”±å‚æ•°
#     parser.add_argument("--CONFIG_NAME", type=str, default="soft_routing", help="Configuration name for logging")
    
#     args = parser.parse_args()
    
#     # è®¾ç½®æ—¥å¿—
#     logging.basicConfig(
#         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
#         datefmt="%m/%d/%Y %H:%M:%S",
#         level=logging.INFO,
#     )
#     logger = logging.getLogger(__name__)
    
#     # å¼€å§‹è®­ç»ƒ
#     train(args, logger)