#!/usr/bin/env python
# coding=utf-8

import copy
import logging
import math
import os
from pathlib import Path

import diffusers
import torch
import torch.utils.checkpoint
import transformers
import yaml
from accelerate import Accelerator
from accelerate.utils import DeepSpeedPlugin, ProjectConfiguration, set_seed
from diffusers.optimization import get_scheduler
from diffusers.utils import is_wandb_available
from huggingface_hub import create_repo, upload_folder
from tqdm.auto import tqdm
from safetensors.torch import load_model

from models.ema_model import EMAModel
from models.multimodal_encoder.siglip_encoder import SiglipVisionTower
from models.multimodal_encoder.t5_encoder import T5Embedder
from models.rdt_runner import RDTRunner
from train.dataset import DataCollatorForVLAConsumerDataset, VLAConsumerDataset
from train.sample import log_sample_res

# ðŸ†• å¯¼å…¥DINOv2å’ŒDepthAnythingV2ç¼–ç å™¨
from models.multimodal_encoder.dinov2_encoder import create_dinov2_encoder
from models.multimodal_encoder.depth_encoder import create_depth_encoder

# ðŸ†• å¯¼å…¥å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨
from data.critical_timestep_annotator import TaskType

if is_wandb_available():
    import wandb


def save_model_card(repo_id: str, base_model=str, repo_folder=None):
    yaml_header = f"""
---
license: mit
base_model: {base_model}
language:
- en
pipeline_tag: robotics
library_name: transformers
tags:
- robotics
- pytorch
- multimodal
- pretraining
- vla
- diffusion
- rdt
- repa
- dual-teachers
- critical-timestep
---
    """
    model_card = f"""
# RDT with Dual-Teacher REPA and Critical Timestep Annotation - {repo_id}

This is a RDT model with dual-teacher REPA alignment loss and task-driven critical timestep annotation derived from {base_model}. 
The weights were trained using [RDT](https://rdt-robotics.github.io/rdt-robotics/) 
with dual visual alignment using DINOv2 (global semantic) and DepthAnythingV2 (depth geometric) features.

## Key Features
- **Dual-Teacher Alignment Strategy**: Dynamic routing between global semantic and depth geometric experts
- **Critical Timestep Annotation**: Task-driven annotation for precise temporal alignment
- **Non-critical timesteps**: Action tokens align with DINOv2 CLS token (global semantics)
- **Critical timesteps**: Action tokens align with DepthAnythingV2 CLS token (depth geometry)

## Task Types Supported
- **Grasp Tasks (task_type=1)**: Deceleration â†’ Gripper closing alignment
- **Click Tasks (task_type=2)**: Gripper closing â†’ Deceleration alignment
"""
    with open(os.path.join(repo_folder, "README.md"), "w") as f:
        f.write(yaml_header + model_card)


def train(args, logger):
    # Read the config
    with open(args.config_path, "r") as fp:
        config = yaml.safe_load(fp)

    with open(args.model_config_path, "r") as f:
        model_config = yaml.safe_load(f)
    
    args.output_dir = model_config["checkpoint_path"]
    logging_dir = Path(args.output_dir, args.logging_dir)

    accelerator_project_config = ProjectConfiguration(total_limit=args.checkpoints_total_limit)
    accelerator = Accelerator(
        deepspeed_plugin=(DeepSpeedPlugin(hf_ds_config=args.deepspeed) if args.deepspeed is not None else None),
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
        log_with=args.report_to,
        project_dir=logging_dir,
        project_config=accelerator_project_config,
    )

    if args.report_to == "wandb":
        if not is_wandb_available():
            raise ImportError("Make sure to install wandb if you want to use it for logging during training.")

    # Make one log on every process with the configuration for debugging.
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        level=logging.INFO,
    )
    logger.info(accelerator.state, main_process_only=False)
    if accelerator.is_local_main_process:
        transformers.utils.logging.set_verbosity_warning()
        diffusers.utils.logging.set_verbosity_info()
    else:
        transformers.utils.logging.set_verbosity_error()
        diffusers.utils.logging.set_verbosity_error()

    # If passed along, set the training seed now.
    if args.seed is not None:
        set_seed(args.seed)

    # Handle the repository creation
    if accelerator.is_main_process:
        if args.output_dir is not None:
            os.makedirs(args.output_dir, exist_ok=True)

        if args.push_to_hub:
            repo_id = create_repo(
                repo_id=args.hub_model_id or Path(args.output_dir).name,
                exist_ok=True,
                token=args.hub_token,
            ).repo_id

    # For mixed precision training
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16

    # ðŸ†• èŽ·å–åŒæ•™å¸ˆREPAå’Œå…³é”®æ—¶é—´æ®µæ ‡æ³¨é…ç½®
    enable_repa_loss = model_config.get("enable_repa_loss", True)
    repa_loss_weight = model_config.get("repa_loss_weight", 0.2)
    use_dinov2_features = model_config.get("use_dinov2_features", True)
    use_depth_features = model_config.get("use_depth_features", True)
    routing_loss_weight = model_config.get("routing_loss_weight", 0.1)
    
    # ðŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨é…ç½®
    enable_critical_annotation = model_config.get("enable_critical_annotation", True)
    task_type = model_config.get("task_type", 1)  # 1=æŠ“å–ç±», 2=ç‚¹å‡»ç±»
    critical_annotation_config = model_config.get("critical_annotation_config", {})
    
    logger.info(f"ðŸ”§ åŒæ•™å¸ˆREPA + å…³é”®æ—¶é—´æ®µæ ‡æ³¨é…ç½®:")
    logger.info(f"   - REPAæŸå¤±å¯ç”¨: {enable_repa_loss}")
    logger.info(f"   - REPAæŸå¤±æƒé‡: {repa_loss_weight}")
    logger.info(f"   - ä½¿ç”¨DINOv2ç‰¹å¾: {use_dinov2_features}")
    logger.info(f"   - ä½¿ç”¨æ·±åº¦ç‰¹å¾: {use_depth_features}")
    logger.info(f"   - è·¯ç”±æŸå¤±æƒé‡: {routing_loss_weight}")
    logger.info(f"   - å…³é”®æ—¶é—´æ®µæ ‡æ³¨: {enable_critical_annotation}")
    logger.info(f"   - ä»»åŠ¡ç±»åž‹: {TaskType(task_type).name} ({task_type})")
    if critical_annotation_config:
        logger.info(f"   - æ ‡æ³¨é…ç½®: {critical_annotation_config}")

    # æ–‡æœ¬ç¼–ç å™¨
    if args.precomp_lang_embed:
        tokenizer, text_encoder = None, None
    else:
        text_embedder = T5Embedder(
            from_pretrained=args.pretrained_text_encoder_name_or_path,
            model_max_length=config["dataset"]["tokenizer_max_length"],
            device=accelerator.device,
        )
        tokenizer, text_encoder = text_embedder.tokenizer, text_embedder.model

    # è§†è§‰ç¼–ç å™¨
    vision_encoder = SiglipVisionTower(vision_tower=args.pretrained_vision_encoder_name_or_path, args=None)
    image_processor = vision_encoder.image_processor

    # ðŸ†• åˆ›å»ºDINOv2ç¼–ç å™¨ï¼ˆå…¨å±€è¯­ä¹‰ç‰¹å¾ï¼‰
    dinov2_encoder = None
    if use_dinov2_features and enable_repa_loss:
        logger.info("ðŸ”§ åŠ è½½DINOv2ç¼–ç å™¨ï¼ˆå…¨å±€è¯­ä¹‰æ•™å¸ˆï¼‰...")
        dinov2_encoder = create_dinov2_encoder(model_size="large", select_feature="cls_only")
        dinov2_encoder.to(accelerator.device, dtype=weight_dtype)
        dinov2_encoder.print_model_info()

    # ðŸ†• åˆ›å»ºDepthAnythingV2ç¼–ç å™¨ï¼ˆæ·±åº¦å‡ ä½•ç‰¹å¾ï¼‰
    depth_encoder = None
    if use_depth_features and enable_repa_loss:
        logger.info("ðŸ”§ åŠ è½½DepthAnythingV2ç¼–ç å™¨ï¼ˆæ·±åº¦å‡ ä½•æ•™å¸ˆï¼‰...")
        depth_encoder = create_depth_encoder(
            model_size="metric_large",  # ä½¿ç”¨Metric Largeç‰ˆæœ¬
            feature_dim=1024,
            device=accelerator.device,
            use_metric_model=True  # å¯ç”¨Metricç‰ˆæœ¬
        )
        depth_encoder.to(accelerator.device, dtype=weight_dtype)
        depth_encoder.print_model_info()

    # æž„å»ºRDTæ¨¡åž‹
    logger.info("ðŸ”§ æž„å»ºåŒæ•™å¸ˆRDTæ¨¡åž‹...")
    img_cond_len = (config["common"]["img_history_size"] * config["common"]["num_cameras"] *
                    vision_encoder.num_patches)
    
    rdt = RDTRunner(
        action_dim=config["common"]["state_dim"],
        pred_horizon=config["common"]["action_chunk_size"],
        config=config["model"],
        lang_token_dim=config["model"]["lang_token_dim"],
        img_token_dim=config["model"]["img_token_dim"],
        state_token_dim=config["model"]["state_token_dim"],
        max_lang_cond_len=config["dataset"]["tokenizer_max_length"],
        img_cond_len=img_cond_len,
        img_pos_embed_config=[
            ("image", (
                config["common"]["img_history_size"],
                config["common"]["num_cameras"],
                -vision_encoder.num_patches,
            )),
        ],
        lang_pos_embed_config=[
            ("lang", -config["dataset"]["tokenizer_max_length"]),
        ],
        dtype=weight_dtype,
        enable_repa_loss=enable_repa_loss,
        repa_loss_weight=repa_loss_weight,
        use_dual_teachers=use_depth_features,  # ðŸ†• å¯ç”¨åŒæ•™å¸ˆæ¨¡å¼
        routing_loss_weight=routing_loss_weight,  # ðŸ†• è·¯ç”±æŸå¤±æƒé‡
    )
    
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æžœæä¾›ï¼‰
    if args.pretrained_model_name_or_path and os.path.isfile(args.pretrained_model_name_or_path):
        logger.info(f"ðŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {args.pretrained_model_name_or_path}")
        ckpt = torch.load(args.pretrained_model_name_or_path, map_location="cpu")

        if isinstance(ckpt, dict) and "module" in ckpt:
            pretrained_sd = ckpt["module"]
        elif isinstance(ckpt, dict) and "state_dict" in ckpt:
            pretrained_sd = ckpt["state_dict"]
        else:
            pretrained_sd = ckpt

        own_sd = rdt.state_dict()
        filtered = {}
        for k, v in pretrained_sd.items():
            if k in own_sd and v.shape == own_sd[k].shape:
                filtered[k] = v
            else:
                logger.debug(f"è·³è¿‡å‚æ•° {k}: checkpoint {tuple(v.shape)} vs model {tuple(own_sd.get(k, v).shape)}")

        rdt.load_state_dict(filtered, strict=False)
        logger.info("âœ… åŠ è½½åŒ¹é…çš„é¢„è®­ç»ƒæƒé‡ï¼›å…¶ä½™ä¿æŒéšæœºåˆå§‹åŒ–")
    else:
        logger.info("ðŸŽ² ä»…ä½¿ç”¨é…ç½®ï¼›è·³è¿‡é¢„è®­ç»ƒæƒé‡åŠ è½½")

    # EMAæ¨¡åž‹
    ema_rdt = copy.deepcopy(rdt)
    ema_model = EMAModel(
        ema_rdt,
        update_after_step=config["model"]["ema"]["update_after_step"],
        inv_gamma=config["model"]["ema"]["inv_gamma"],
        power=config["model"]["ema"]["power"],
        min_value=config["model"]["ema"]["min_value"],
        max_value=config["model"]["ema"]["max_value"],
    )

    # ä¿å­˜é’©å­
    def save_model_hook(models, weights, output_dir):
        if accelerator.is_main_process:
            for model in models:
                model_to_save = model.module if hasattr(model, "module") else model
                if isinstance(model_to_save, type(accelerator.unwrap_model(rdt))):
                    model_to_save.save_pretrained(output_dir)

    accelerator.register_save_state_pre_hook(save_model_hook)

    if args.gradient_checkpointing:
        raise NotImplementedError("Gradient checkpointing is not yet implemented.")

    # Enable TF32 for faster training on Ampere GPUs
    if args.allow_tf32:
        torch.backends.cuda.matmul.allow_tf32 = True

    if args.scale_lr:
        args.learning_rate = (args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size *
                              accelerator.num_processes)

    # ä¼˜åŒ–å™¨
    if args.use_8bit_adam:
        try:
            import bitsandbytes as bnb
        except ImportError:
            raise ImportError("To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.")
        optimizer_class = bnb.optim.AdamW8bit
    else:
        optimizer_class = torch.optim.AdamW

    params_to_optimize = rdt.parameters()
    optimizer = optimizer_class(
        params_to_optimize,
        lr=args.learning_rate,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.adam_weight_decay,
        eps=args.adam_epsilon,
    )

    # ðŸ†• æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ï¼ˆé›†æˆå…³é”®æ—¶é—´æ®µæ ‡æ³¨ï¼‰
    train_dataset = VLAConsumerDataset(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=args.image_aug,
        cond_mask_prob=args.cond_mask_prob,
        cam_ext_mask_prob=args.cam_ext_mask_prob,
        state_noise_snr=args.state_noise_snr,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        use_dinov2_features=use_dinov2_features,
        use_depth_features=use_depth_features,
        # ðŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨å‚æ•°
        task_type=task_type,
        enable_critical_annotation=enable_critical_annotation,
        critical_annotation_config=critical_annotation_config,
    )
    
    sample_dataset = VLAConsumerDataset(
        model_config_path=args.model_config_path,
        config=config["dataset"],
        tokenizer=tokenizer,
        image_processor=image_processor,
        num_cameras=config["common"]["num_cameras"],
        img_history_size=config["common"]["img_history_size"],
        dataset_type=args.dataset_type,
        image_aug=False,
        cond_mask_prob=0,
        cam_ext_mask_prob=-1,
        state_noise_snr=None,
        use_hdf5=args.load_from_hdf5,
        use_precomp_lang_embed=args.precomp_lang_embed,
        use_dinov2_features=use_dinov2_features,
        use_depth_features=use_depth_features,
        # ðŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨å‚æ•°ï¼ˆé‡‡æ ·æ—¶ä¹Ÿå¯ç”¨ï¼‰
        task_type=task_type,
        enable_critical_annotation=enable_critical_annotation,
        critical_annotation_config=critical_annotation_config,
    )

    data_collator = DataCollatorForVLAConsumerDataset(tokenizer)

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.train_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=args.dataloader_num_workers,
        pin_memory=True,
        persistent_workers=True,
    )
    
    sample_dataloader = torch.utils.data.DataLoader(
        sample_dataset,
        batch_size=args.sample_batch_size,
        shuffle=True,
        collate_fn=data_collator,
        num_workers=args.dataloader_num_workers,
        pin_memory=True,
        persistent_workers=True,
    )

    # å­¦ä¹ çŽ‡è°ƒåº¦å™¨
    overrode_max_train_steps = False
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if args.max_train_steps is None:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
        overrode_max_train_steps = True

    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
        num_cycles=args.lr_num_cycles,
        power=args.lr_power,
    )

    # å‡†å¤‡è®­ç»ƒ
    rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler = (
        accelerator.prepare(rdt, optimizer, train_dataloader, sample_dataloader, lr_scheduler)
    )

    ema_rdt.to(accelerator.device, dtype=weight_dtype)

    if text_encoder is not None:
        text_encoder.to(accelerator.device, dtype=weight_dtype)

    if vision_encoder is not None:
        vision_encoder.vision_tower.to(accelerator.device, dtype=weight_dtype)

    # é‡æ–°è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
    if overrode_max_train_steps:
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)

    # åˆå§‹åŒ–è¿½è¸ªå™¨
    if accelerator.is_main_process:
        tracker_config = vars(args).copy()
        tracker_config.update({
            'task_type': task_type,
            'enable_critical_annotation': enable_critical_annotation,
            'critical_annotation_config': critical_annotation_config,
        })
        
        accelerator.init_trackers(
            "VLA_Dual_Teacher_REPA_Critical",
            config=tracker_config,
            init_kwargs={"wandb": {
                "name": f"RDT_DualTeacher_Critical_{TaskType(task_type).name}_{args.CONFIG_NAME}",
            }},
        )

    # è®­ç»ƒä¿¡æ¯
    total_batch_size = (args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps)

    logger.info("***** å¼€å§‹åŒæ•™å¸ˆREPA + å…³é”®æ—¶é—´æ®µæ ‡æ³¨è®­ç»ƒ *****")
    logger.info(f"  æ ·æœ¬æ•°é‡ = {len(train_dataset)}")
    logger.info(f"  æ¯epochæ‰¹æ¬¡æ•° = {len(train_dataloader)}")
    logger.info(f"  Epochæ•° = {args.num_train_epochs}")
    logger.info(f"  æ¯è®¾å¤‡çž¬æ—¶æ‰¹æ¬¡å¤§å° = {args.train_batch_size}")
    logger.info(f"  æ€»è®­ç»ƒæ‰¹æ¬¡å¤§å° = {total_batch_size}")
    logger.info(f"  æ¢¯åº¦ç´¯ç§¯æ­¥æ•° = {args.gradient_accumulation_steps}")
    logger.info(f"  æ€»ä¼˜åŒ–æ­¥æ•° = {args.max_train_steps}")
    logger.info(f"  ä»»åŠ¡ç±»åž‹ = {TaskType(task_type).name}")
    
    global_step = 0
    first_epoch = 0

    # å¯èƒ½ä»Žæ£€æŸ¥ç‚¹æ¢å¤
    if args.resume_from_checkpoint:
        if args.resume_from_checkpoint != "latest":
            path = os.path.basename(args.resume_from_checkpoint)
        else:
            dirs = os.listdir(args.output_dir)
            dirs = [d for d in dirs if d.startswith("checkpoint")]
            dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
            path = dirs[-1] if len(dirs) > 0 else None

        if path is None:
            accelerator.print(f"æ£€æŸ¥ç‚¹ '{args.resume_from_checkpoint}' ä¸å­˜åœ¨ã€‚å¼€å§‹æ–°çš„è®­ç»ƒã€‚")
            args.resume_from_checkpoint = None
        else:
            accelerator.print(f"ä»Žæ£€æŸ¥ç‚¹æ¢å¤: {path}")
            try:
                accelerator.load_state(os.path.join(args.output_dir, path))
            except:
                logger.info("æ¢å¤è®­ç»ƒçŠ¶æ€å¤±è´¥ã€‚å°è¯•ä»…åŠ è½½æ¨¡åž‹æ£€æŸ¥ç‚¹ã€‚")
                checkpoint = torch.load(
                    os.path.join(args.output_dir, path, "pytorch_model", "mp_rank_00_model_states.pt"))
                rdt.module.load_state_dict(checkpoint["module"])

            load_model(ema_rdt, os.path.join(args.output_dir, path, "ema", "model.safetensors"))
            global_step = int(path.split("-")[1])

            resume_global_step = global_step * args.gradient_accumulation_steps
            first_epoch = global_step // num_update_steps_per_epoch
            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)

    # è¿›åº¦æ¡
    progress_bar = tqdm(
        range(global_step, args.max_train_steps),
        disable=not accelerator.is_local_main_process,
    )
    progress_bar.set_description("Steps")

    # ðŸ†• ç”¨äºŽè®°å½•å…³é”®æ—¶é—´æ®µæ ‡æ³¨ç»Ÿè®¡çš„å˜é‡
    critical_stats = {
        'total_samples': 0,
        'critical_timesteps': 0,
        'global_expert_usage': 0.0,
        'depth_expert_usage': 0.0,
    }
    
    loss_for_log = {}
    
    # è®­ç»ƒå¾ªçŽ¯
    for epoch in range(first_epoch, args.num_train_epochs):
        rdt.train()

        if args.resume_from_checkpoint and epoch == first_epoch:
            progress_bar.update(resume_step // args.gradient_accumulation_steps)

        for batch in train_dataloader:
            with accelerator.accumulate(rdt):
                # å‡†å¤‡è¾“å…¥æ•°æ®
                images = batch["images"].to(dtype=weight_dtype)
                states = batch["states"].to(dtype=weight_dtype)[:, -1:, :]  # åªä½¿ç”¨æœ€åŽä¸€ä¸ªçŠ¶æ€
                actions = batch["actions"].to(dtype=weight_dtype)
                state_elem_mask = batch["state_elem_mask"].to(dtype=weight_dtype)
                ctrl_freqs = batch["ctrl_freqs"]

                # ðŸ†• èŽ·å–å…³é”®æ—¶é—´æ®µæ ‡ç­¾
                critical_labels = batch.get("critical_labels", None)
                if critical_labels is not None:
                    critical_labels = critical_labels.to(accelerator.device)
                    
                    # ç»Ÿè®¡å…³é”®æ—¶é—´æ®µä¿¡æ¯
                    batch_size, seq_len = critical_labels.shape
                    critical_stats['total_samples'] += batch_size * seq_len
                    critical_stats['critical_timesteps'] += critical_labels.sum().item()

                # ç¼–ç åŽŸå§‹å›¾åƒï¼ˆSigLIPï¼‰
                with torch.no_grad():
                    batch_size, _, C, H, W = images.shape
                    image_embeds = vision_encoder(images.reshape(-1, C, H, W)).detach()
                    image_embeds = image_embeds.reshape((batch_size, -1, vision_encoder.hidden_size))

                    lang_attn_mask = batch["lang_attn_mask"]
                    text_embeds = (batch["lang_embeds"].to(dtype=weight_dtype) 
                                 if args.precomp_lang_embed 
                                 else text_encoder(input_ids=batch["input_ids"], 
                                                 attention_mask=lang_attn_mask)["last_hidden_state"].detach())

                # ðŸ†• æå–DINOv2å…¨å±€è¯­ä¹‰ç‰¹å¾
                cls_token = None
                if dinov2_encoder is not None and "dinov2_images" in batch:
                    with torch.no_grad():
                        dinov2_images = batch["dinov2_images"].to(dtype=weight_dtype)
                        dinov2_input = dinov2_images[:, 0]  # (B, 3, 518, 518)
                        cls_token = dinov2_encoder(dinov2_input)  # (B, 1, 1024)

                # ðŸ†• æå–DepthAnythingV2æ·±åº¦å‡ ä½•ç‰¹å¾
                depth_features = None
                if depth_encoder is not None and "depth_images" in batch:
                    with torch.no_grad():
                        depth_images = batch["depth_images"].to(dtype=weight_dtype)
                        depth_input = depth_images[:, 0]  # (B, 3, 518, 518)
                        depth_features, _ = depth_encoder(depth_input)  # (B, 1370, 1024)

                # ðŸ†• è®¡ç®—åŒæ•™å¸ˆREPAæŸå¤±ï¼ˆä½¿ç”¨å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼‰
                state_elem_mask = state_elem_mask.unsqueeze(1)
                if enable_repa_loss:
                    total_loss, diffusion_loss, repa_loss, routing_loss, intermediate_activations = accelerator.unwrap_model(rdt).compute_loss(
                        lang_tokens=text_embeds,
                        lang_attn_mask=lang_attn_mask,
                        img_tokens=image_embeds,
                        state_tokens=states,
                        action_gt=actions,
                        action_mask=state_elem_mask,
                        ctrl_freqs=ctrl_freqs,
                        cls_token=cls_token,              
                        depth_features=depth_features,   
                        critical_labels=critical_labels,  # ðŸ†• ä¼ å…¥å…³é”®æ—¶é—´æ®µæ ‡ç­¾
                    )
                    loss = total_loss
                    
                    # è®°å½•è¯¦ç»†æŸå¤±
                    loss_for_log["diffusion_loss"] = diffusion_loss.detach().item()
                    loss_for_log["repa_loss"] = repa_loss.detach().item()
                    loss_for_log["routing_loss"] = routing_loss.detach().item()
                    
                    # ðŸ†• è®°å½•å…³é”®æ—¶é—´æ®µå’Œè·¯ç”±ç»Ÿè®¡
                    if critical_labels is not None:
                        critical_ratio = critical_labels.float().mean().item()
                        loss_for_log["critical_ratio"] = critical_ratio
                        
                        # è®°å½•æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨çŽ‡
                        if 'routing_weights' in intermediate_activations:
                            routing_weights = intermediate_activations['routing_weights']
                            global_usage = routing_weights[:, :, 0].mean().item()
                            depth_usage = routing_weights[:, :, 1].mean().item()
                            loss_for_log["global_expert_usage"] = global_usage
                            loss_for_log["depth_expert_usage"] = depth_usage
                            
                            # æ›´æ–°ç´¯ç§¯ç»Ÿè®¡
                            critical_stats['global_expert_usage'] = (
                                critical_stats['global_expert_usage'] * 0.99 + global_usage * 0.01
                            )
                            critical_stats['depth_expert_usage'] = (
                                critical_stats['depth_expert_usage'] * 0.99 + depth_usage * 0.01
                            )
                            
                            # è®°å½•å…³é”®æ—¶é—´æ®µçš„ä¸“å®¶é€‰æ‹©å‡†ç¡®çŽ‡
                            if critical_labels is not None:
                                # è®¡ç®—åœ¨å…³é”®æ—¶é—´æ®µé€‰æ‹©æ·±åº¦ä¸“å®¶çš„æ¯”ä¾‹
                                critical_mask = critical_labels.bool()
                                if critical_mask.any():
                                    critical_depth_usage = routing_weights[critical_mask][:, 1].mean().item()
                                    loss_for_log["critical_depth_expert_accuracy"] = critical_depth_usage
                                
                                # è®¡ç®—åœ¨éžå…³é”®æ—¶é—´æ®µé€‰æ‹©å…¨å±€ä¸“å®¶çš„æ¯”ä¾‹
                                non_critical_mask = ~critical_mask
                                if non_critical_mask.any():
                                    non_critical_global_usage = routing_weights[non_critical_mask][:, 0].mean().item()
                                    loss_for_log["non_critical_global_expert_accuracy"] = non_critical_global_usage
                else:
                    # åŽŸå§‹æ–¹å¼ï¼ˆå…¼å®¹æ€§ï¼‰
                    loss = rdt(
                        lang_tokens=text_embeds,
                        lang_attn_mask=lang_attn_mask,
                        img_tokens=image_embeds,
                        state_tokens=states,
                        action_gt=actions,
                        action_mask=state_elem_mask,
                        ctrl_freqs=ctrl_freqs,
                    )
                    loss_for_log["diffusion_loss"] = loss.detach().item()

                # åå‘ä¼ æ’­
                accelerator.backward(loss)
                if accelerator.sync_gradients:
                    params_to_clip = rdt.parameters()
                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad(set_to_none=args.set_grads_to_none)

            # EMAæ›´æ–°
            ema_model.step(accelerator.unwrap_model(rdt))

            # æ£€æŸ¥ç‚¹å’Œé‡‡æ ·
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1

                if global_step % args.checkpointing_period == 0:
                    save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")
                    accelerator.save_state(save_path)
                    ema_save_path = os.path.join(save_path, f"ema")
                    accelerator.save_model(ema_rdt, ema_save_path)
                    logger.info(f"ðŸ’¾ ä¿å­˜çŠ¶æ€åˆ° {save_path}")

                # ðŸ†• æ¯éš”ä¸€æ®µæ—¶é—´è®°å½•å…³é”®æ—¶é—´æ®µç»Ÿè®¡ä¿¡æ¯
                if global_step % 100 == 0 and critical_stats['total_samples'] > 0:
                    overall_critical_ratio = critical_stats['critical_timesteps'] / critical_stats['total_samples']
                    logger.info(f"ðŸ“Š å…³é”®æ—¶é—´æ®µç»Ÿè®¡ (æ­¥éª¤ {global_step}):")
                    logger.info(f"   - æ€»ä½“å…³é”®æ—¶é—´æ®µæ¯”ä¾‹: {overall_critical_ratio:.3f}")
                    logger.info(f"   - å…¨å±€ä¸“å®¶å¹³å‡ä½¿ç”¨çŽ‡: {critical_stats['global_expert_usage']:.3f}")
                    logger.info(f"   - æ·±åº¦ä¸“å®¶å¹³å‡ä½¿ç”¨çŽ‡: {critical_stats['depth_expert_usage']:.3f}")
                    
                    # è®°å½•åˆ°wandb
                    loss_for_log["overall_critical_ratio"] = overall_critical_ratio
                    loss_for_log["cumulative_global_expert_usage"] = critical_stats['global_expert_usage']
                    loss_for_log["cumulative_depth_expert_usage"] = critical_stats['depth_expert_usage']

                if args.sample_period > 0 and global_step % args.sample_period == 0:
                    sample_loss_for_log = log_sample_res(
                        text_encoder,
                        vision_encoder,
                        rdt,
                        args,
                        accelerator,
                        weight_dtype,
                        sample_dataset.get_dataset_id2name(),
                        sample_dataloader,
                        logger,
                    )
                    logger.info(sample_loss_for_log)
                    accelerator.log(sample_loss_for_log, step=global_step)

            # è®°å½•æ—¥å¿—
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
            progress_bar.set_postfix(**logs)
            logs.update(loss_for_log)
            accelerator.log(logs, step=global_step)

            if global_step >= args.max_train_steps:
                break

    # ðŸ†• è®­ç»ƒç»“æŸæ—¶çš„ç»Ÿè®¡æ€»ç»“
    if accelerator.is_main_process and critical_stats['total_samples'] > 0:
        final_critical_ratio = critical_stats['critical_timesteps'] / critical_stats['total_samples']
        logger.info("ðŸŽ¯ è®­ç»ƒå®Œæˆ - å…³é”®æ—¶é—´æ®µæ ‡æ³¨ç»Ÿè®¡æ€»ç»“:")
        logger.info(f"   - å¤„ç†çš„æ€»æ—¶é—´æ­¥æ•°: {critical_stats['total_samples']}")
        logger.info(f"   - å…³é”®æ—¶é—´æ­¥æ•°: {critical_stats['critical_timesteps']}")
        logger.info(f"   - æœ€ç»ˆå…³é”®æ—¶é—´æ®µæ¯”ä¾‹: {final_critical_ratio:.3f}")
        logger.info(f"   - å…¨å±€ä¸“å®¶æœ€ç»ˆä½¿ç”¨çŽ‡: {critical_stats['global_expert_usage']:.3f}")
        logger.info(f"   - æ·±åº¦ä¸“å®¶æœ€ç»ˆä½¿ç”¨çŽ‡: {critical_stats['depth_expert_usage']:.3f}")

    # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        accelerator.unwrap_model(rdt).save_pretrained(args.output_dir)
        ema_save_path = os.path.join(args.output_dir, f"ema")
        accelerator.save_model(ema_rdt, ema_save_path)

        logger.info(f"ðŸ’¾ ä¿å­˜æ¨¡åž‹åˆ° {args.output_dir}")

        # ðŸ†• ä¿å­˜è®­ç»ƒé…ç½®å’Œç»Ÿè®¡ä¿¡æ¯
        final_config = {
            'task_type': task_type,
            'task_name': TaskType(task_type).name,
            'enable_critical_annotation': enable_critical_annotation,
            'critical_annotation_config': critical_annotation_config,
            'final_statistics': {
                'total_timesteps': critical_stats['total_samples'],
                'critical_timesteps': critical_stats['critical_timesteps'],
                'critical_ratio': final_critical_ratio if critical_stats['total_samples'] > 0 else 0.0,
                'global_expert_usage': critical_stats['global_expert_usage'],
                'depth_expert_usage': critical_stats['depth_expert_usage'],
            }
        }
        
        import json
        with open(os.path.join(args.output_dir, "training_config.json"), "w") as f:
            json.dump(final_config, f, indent=2)

        if args.push_to_hub:
            save_model_card(
                repo_id,
                base_model=args.pretrained_model_name_or_path,
                repo_folder=args.output_dir,
            )
            upload_folder(
                repo_id=repo_id,
                folder_path=args.output_dir,
                commit_message="End of dual-teacher REPA + critical timestep training",
                token=args.hub_token,
                allow_patterns=["pytorch_model.bin", "*.json", "*.md"],
            )

    accelerator.end_training()