# # train/dataset.py - æ›´æ–°ç‰ˆæœ¬ï¼Œé›†æˆä»»åŠ¡é©±åŠ¨çš„å…³é”®æ—¶é—´æ®µæ ‡æ³¨

# import traceback
# import time
# import os
# import json
# import math
# import random
# from typing import Dict, Sequence

# import numpy as np
# import torch
# from torch.utils.data import Dataset
# from torchvision import transforms
# from PIL import Image
# import transformers

# from data.filelock import FileLock
# from data.hdf5_vla_dataset import HDF5VLADataset
# from train.image_corrupt import image_corrupt

# # ğŸ†• å¯¼å…¥ä»»åŠ¡é©±åŠ¨çš„å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨
# from data.critical_timestep_annotator import (
#     TaskType, 
#     create_silent_task_annotator,
#     TaskDrivenCriticalTimestepAnnotator
# )


# def get_clean_item(chunk_dir):
#     """Get indexes of clean items in a chunk."""
#     dirty_bit = read_dirty_bit(chunk_dir)
#     return np.where(1 - dirty_bit)[0].tolist()


# def save_dirty_bit(chunk_dir, dirty_bit):
#     """Save the dirty bit to the chunk directory."""
#     time_stmp = time.time()
#     while time.time() - time_stmp < 10.0:
#         try:
#             file_path = os.path.join(chunk_dir, "dirty_bit")
#             lock = FileLock(file_path)
#             lock.acquire_write_lock()
#             with open(file_path, "wb") as file:
#                 file.write(dirty_bit.tobytes())
#             lock.release_lock()
#             return
#         except KeyboardInterrupt:
#             lock.release_lock()
#             raise KeyboardInterrupt
#         except BaseException:
#             lock.release_lock()
#             continue
#     raise RuntimeError("Failed to save dirty bit.")


# def read_dirty_bit(chunk_dir):
#     """Read the dirty bit from the chunk directory."""
#     time_stmp = time.time()
#     while time.time() - time_stmp < 10.0:
#         try:
#             file_path = os.path.join(chunk_dir, "dirty_bit")
#             lock = FileLock(file_path)
#             lock.acquire_read_lock()
#             with open(file_path, "rb") as file:
#                 dirty_bit = np.frombuffer(file.read(), dtype=np.uint8).copy()
#             lock.release_lock()
#             assert len(dirty_bit) > 0
#             return dirty_bit
#         except KeyboardInterrupt:
#             lock.release_lock()
#             raise KeyboardInterrupt
#         except BaseException:
#             lock.release_lock()
#             continue
#     raise RuntimeError("Failed to read dirty bit.")


# class VLAConsumerDataset(Dataset):
#     """A vision-language-action Dataset for supervised training.
#     ğŸ†• é›†æˆä»»åŠ¡é©±åŠ¨çš„å…³é”®æ—¶é—´æ®µæ ‡æ³¨æœºåˆ¶
#     """

#     def __init__(
#         self,
#         model_config_path,
#         config,
#         tokenizer,
#         image_processor,
#         num_cameras,
#         img_history_size,
#         image_size=None,
#         auto_adjust_image_brightness=False,
#         image_aug=False,
#         dataset_type="pretrain",
#         cond_mask_prob=0.1,
#         cam_ext_mask_prob=-1.0,
#         state_noise_snr=None,
#         use_hdf5=False,
#         use_precomp_lang_embed=False,
#         use_dinov2_features=False,
#         use_depth_features=False,
#         # ğŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨ç›¸å…³å‚æ•°
#         task_type: int = 1,  # 1=æŠ“å–ç±», 2=ç‚¹å‡»ç±»
#         enable_critical_annotation: bool = True,
#         critical_annotation_config: Dict = None,
#     ):
#         super(VLAConsumerDataset, self).__init__()

#         # Load the control frequency for each dataset
#         with open("configs/dataset_control_freq.json", "r") as fp:
#             self.control_freq = json.load(fp)
#         # Load the dataset names
#         dataset_names_cfg = ("configs/pretrain_datasets.json"
#                              if dataset_type == "pretrain" else "configs/finetune_datasets.json")
#         with open(dataset_names_cfg, "r") as file:
#             DATASET_NAMES = json.load(file)
#         # Create the mapping between dataset name and id
#         self.dataset_name2id = {name: i for i, name in enumerate(DATASET_NAMES)}
#         self.dataset_id2name = {i: name for i, name in enumerate(DATASET_NAMES)}

#         self.image_processor = image_processor
#         self.model_config_path = model_config_path
#         self.buffer_dir = config["buf_path"]
#         self.num_chunks = config["buf_num_chunks"]
#         self.chunk_size = config["buf_chunk_size"]
#         self.tokenizer_max_length = config["tokenizer_max_length"]
#         self.image_aspect_ratio = config["image_aspect_ratio"]
#         self.state_noise_snr = state_noise_snr
#         self.num_cameras = num_cameras
#         self.img_history_size = img_history_size
#         self.cond_mask_prob = cond_mask_prob
#         self.cam_ext_mask_prob = cam_ext_mask_prob
#         self.use_hdf5 = use_hdf5
#         self.hdf5_dataset = None
#         if use_hdf5:
#             self.hdf5_dataset = HDF5VLADataset(self.model_config_path)
#         self.use_precomp_lang_embed = use_precomp_lang_embed
#         if use_precomp_lang_embed:
#             self.empty_lang_embed = torch.load("data/empty_lang_embed.pt")
        
#         # DINOv2ç›¸å…³é…ç½®
#         self.use_dinov2_features = use_dinov2_features
#         self.dinov2_image_size = 518
        
#         # DepthAnythingV2ç›¸å…³é…ç½®
#         self.use_depth_features = use_depth_features
#         self.depth_image_size = 518

#         # Load dataset stat
#         with open("configs/dataset_stat.json", "r") as f:
#             dataset_stat = json.load(f)
#         self.dataset_stat = dataset_stat

#         self.tokenizer = tokenizer
#         self.image_size = image_size
#         self.auto_adjust_image_brightness = auto_adjust_image_brightness
#         self.image_aug = image_aug

#         self.last_content = None
#         self.last_meta = None
        
#         # ğŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨é…ç½®
#         self.task_type = TaskType(task_type)
#         self.enable_critical_annotation = enable_critical_annotation
        
#         if enable_critical_annotation:
#             # è®¾ç½®é»˜è®¤é…ç½®
#             default_config = {
#                 'relative_low_speed_ratio': 0.15,
#                 'min_deceleration_threshold': -0.0008,
#                 'gripper_close_delta_threshold': -0.01,
#                 'smooth': True,
#                 'verbose': False
#             }
            
#             # åˆå¹¶ç”¨æˆ·é…ç½®
#             if critical_annotation_config:
#                 default_config.update(critical_annotation_config)
            
#             # åˆ›å»ºæ ‡æ³¨å™¨
#             self.critical_annotator = TaskDrivenCriticalTimestepAnnotator(
#                 task_type=self.task_type,
#                 **default_config
#             )
            
#             print(f"ğŸ¯ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨åˆå§‹åŒ–:")
#             print(f"   - ä»»åŠ¡ç±»å‹: {self.task_type.name} ({self.task_type.value})")
#             print(f"   - é…ç½®: {default_config}")
#         else:
#             self.critical_annotator = None
#             print("âš ï¸  å…³é”®æ—¶é—´æ®µæ ‡æ³¨å·²ç¦ç”¨")

#     def get_dataset_name2id(self):
#         return self.dataset_name2id

#     def get_dataset_id2name(self):
#         return self.dataset_id2name

#     @staticmethod
#     def pairwise(iterable):
#         a = iter(iterable)
#         return zip(a, a)

#     @staticmethod
#     def _load_data_from_chunk(chunk_dir, chunk_item_idx):
#         time_stmp = time.time()
#         while time.time() - time_stmp < 10.0:
#             try:
#                 locks = []
#                 file_path = os.path.join(chunk_dir, f"json_content_{chunk_item_idx}.json")
#                 lock = FileLock(file_path)
#                 locks.append(lock)
#                 lock.acquire_read_lock()
#                 with open(file_path, "r") as file:
#                     json_content = json.load(file)
#                 lock.release_lock()
#                 file_path = os.path.join(chunk_dir, f"sample_{chunk_item_idx}.npz")
#                 lock = FileLock(file_path)
#                 locks.append(lock)
#                 lock.acquire_read_lock()
#                 with open(file_path, "rb") as file:
#                     sample_dict = np.load(file)
#                     meta = tuple(sample_dict.values())
#                 lock.release_lock()
#                 return json_content, meta
#             except KeyboardInterrupt:
#                 for lock in locks:
#                     lock.release_lock()
#                 raise KeyboardInterrupt
#             except BaseException:
#                 for lock in locks:
#                     lock.release_lock()
#                 continue
#         raise RuntimeError("Failed to load sample.")

#     def __len__(self) -> int:
#         if self.use_hdf5:
#             return len(self.hdf5_dataset)
#         else:
#             return self.num_chunks * self.chunk_size

#     def _safe_load(self, index):
#         read_chunk_item_indices = []
#         # Start searching from a random chunk
#         read_chunk_idx = index // self.chunk_size
#         while len(read_chunk_item_indices) == 0:
#             read_chunk_dir = os.path.join(self.buffer_dir, f"chunk_{read_chunk_idx}")
#             try:
#                 read_chunk_item_indices = get_clean_item(read_chunk_dir)
#             except BaseException as e:
#                 print("Error catched when searching a clean chunk:", e)
#                 traceback.print_exc()
#                 read_chunk_item_indices = []
#             read_chunk_idx = (read_chunk_idx + 1) % self.num_chunks

#         random_item_index = index % len(read_chunk_item_indices)
#         read_chunk_item_index = read_chunk_item_indices[random_item_index]

#         # Modify the dirty bit
#         try:
#             dirty_bit = read_dirty_bit(read_chunk_dir)
#             dirty_bit[read_chunk_item_index] = 1
#             save_dirty_bit(read_chunk_dir, dirty_bit)
#         except BaseException as e:
#             print("Error catched when modifying the dirty bit:", e)
#             traceback.print_exc()

#         # load the sample
#         try:
#             content, meta = self._load_data_from_chunk(read_chunk_dir, read_chunk_item_index)
#             self.last_content, self.last_meta = content, meta
#         except BaseException as e:
#             print("Error catched when loading sample:", e)
#             traceback.print_exc()
#             content, meta = self.last_content, self.last_meta

#         return (content, *meta)

#     def _generate_critical_labels(self, qpos_trajectory, action_horizon=64):
#         """
#         ğŸ†• ç”Ÿæˆå…³é”®æ—¶é—´æ®µæ ‡ç­¾
#         ğŸ”§ ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        
#         Args:
#             qpos_trajectory: (T, 14) numpyæ•°ç»„ï¼Œå…³èŠ‚è§’åº¦è½¨è¿¹
#             action_horizon: åŠ¨ä½œé¢„æµ‹çš„æ—¶é—´èŒƒå›´
            
#         Returns:
#             critical_labels: (action_horizon,) torch tensorï¼Œå…³é”®æ—¶é—´æ®µæ ‡ç­¾
#         """
#         try:
#             if self.critical_annotator is None:
#                 # å¦‚æœæ²¡æœ‰æ ‡æ³¨å™¨ï¼Œè¿”å›åˆç†çš„é»˜è®¤æ ‡ç­¾
#                 # ä½¿ç”¨ç®€å•çš„å¯å‘å¼ï¼šå‰25%å’Œå25%ä¸ºå…³é”®æ—¶é—´æ®µ
#                 critical_labels = np.zeros(action_horizon, dtype=np.int64)
                
#                 # å¼€å§‹é˜¶æ®µï¼ˆå‰25%ï¼‰å’Œç»“æŸé˜¶æ®µï¼ˆå25%ï¼‰æ ‡è®°ä¸ºå…³é”®
#                 start_critical = int(action_horizon * 0.25)
#                 end_critical = int(action_horizon * 0.75)
                
#                 critical_labels[:start_critical] = 1  # å¼€å§‹é˜¶æ®µ
#                 critical_labels[end_critical:] = 1    # ç»“æŸé˜¶æ®µ
                
#                 return torch.from_numpy(critical_labels).long()
            
#             # æœ‰æ ‡æ³¨å™¨æ—¶ä½¿ç”¨æ ‡æ³¨å™¨
#             if qpos_trajectory is None or len(qpos_trajectory) == 0:
#                 print("âš ï¸ qpos_trajectoryä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾")
#                 # è¿”å›äº¤æ›¿æ¨¡å¼çš„æ ‡ç­¾
#                 critical_labels = np.zeros(action_horizon, dtype=np.int64)
#                 critical_labels[action_horizon//4:action_horizon//2] = 1  # ä¸­é—´ä¸€æ®µä¸ºå…³é”®
#                 return torch.from_numpy(critical_labels).long()
            
#             # ä½¿ç”¨æ ‡æ³¨å™¨ç”Ÿæˆæ ‡ç­¾
#             critical_labels, analysis_info = self.critical_annotator.annotate(qpos_trajectory)
            
#             # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ ‡ç­¾æ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹
#             if not isinstance(critical_labels, np.ndarray):
#                 critical_labels = np.array(critical_labels, dtype=np.int64)
#             else:
#                 critical_labels = critical_labels.astype(np.int64)
            
#             # ğŸ”§ ä¿®å¤ï¼šå¤„ç†é•¿åº¦ä¸åŒ¹é…çš„é—®é¢˜
#             if len(critical_labels) > action_horizon:
#                 # å¦‚æœæ ‡æ³¨ç»“æœå¤ªé•¿ï¼Œæˆªå–å‰action_horizonä¸ª
#                 critical_labels = critical_labels[:action_horizon]
#             elif len(critical_labels) < action_horizon:
#                 # å¦‚æœæ ‡æ³¨ç»“æœå¤ªçŸ­ï¼Œä½¿ç”¨ç­–ç•¥å¡«å……
#                 padding_len = action_horizon - len(critical_labels)
                
#                 if len(critical_labels) > 0:
#                     # ä½¿ç”¨æœ€åä¸€ä¸ªå€¼å¡«å……
#                     last_value = critical_labels[-1]
#                     padding = np.full(padding_len, last_value, dtype=np.int64)
#                 else:
#                     # å¦‚æœå®Œå…¨æ²¡æœ‰æ ‡æ³¨ï¼Œä½¿ç”¨0å¡«å……
#                     padding = np.zeros(padding_len, dtype=np.int64)
                
#                 critical_labels = np.concatenate([critical_labels, padding])
            
#             # ğŸ”§ ä¿®å¤ï¼šéªŒè¯è¾“å‡ºå½¢çŠ¶
#             assert len(critical_labels) == action_horizon, f"æ ‡ç­¾é•¿åº¦ä¸åŒ¹é…: {len(critical_labels)} vs {action_horizon}"
#             assert critical_labels.dtype == np.int64, f"æ ‡ç­¾ç±»å‹é”™è¯¯: {critical_labels.dtype}"
            
#             # è½¬æ¢ä¸ºtorch tensor
#             critical_labels_tensor = torch.from_numpy(critical_labels).long()
            
#             # ğŸ”§ éªŒè¯å€¼çš„èŒƒå›´
#             if torch.any(critical_labels_tensor < 0) or torch.any(critical_labels_tensor > 1):
#                 print(f"âš ï¸ æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´ [0,1]: {critical_labels_tensor.unique()}")
#                 # å°†æ‰€æœ‰é0å€¼è½¬æ¢ä¸º1
#                 critical_labels_tensor = torch.clamp(critical_labels_tensor, 0, 1)
            
#             return critical_labels_tensor
            
#         except Exception as e:
#             print(f"âš ï¸ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å¤±è´¥: {e}")
#             import traceback
#             traceback.print_exc()
            
#             # ğŸ”§ é™çº§æ–¹æ¡ˆï¼šè¿”å›å®‰å…¨çš„é»˜è®¤æ ‡ç­¾
#             print("ğŸ”§ ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æ ‡ç­¾")
#             critical_labels = np.zeros(action_horizon, dtype=np.int64)
            
#             # ç®€å•ç­–ç•¥ï¼šä¸­é—´30%ä¸ºå…³é”®æ—¶é—´æ®µ
#             start_idx = int(action_horizon * 0.35)
#             end_idx = int(action_horizon * 0.65)
#             critical_labels[start_idx:end_idx] = 1
            
#             return torch.from_numpy(critical_labels).long()

#     def __getitem__(self, index):
#         # For robustness, we will try to load the data until we succeed
#         while True:
#             data_dict = None
#             try:
#                 if self.use_hdf5:
#                     res = self.hdf5_dataset.get_item()
#                     content = res["meta"]
#                     states = res["state"]
#                     actions = res["actions"]
#                     state_elem_mask = res["state_indicator"]
#                     image_metas = [
#                         res["cam_high"],
#                         res["cam_high_mask"],
#                         res["cam_right_wrist"],
#                         res["cam_right_wrist_mask"],
#                         res["cam_left_wrist"],
#                         res["cam_left_wrist_mask"],
#                     ]
#                     state_std = res["state_std"]
#                     state_mean = res["state_mean"]
#                     state_norm = res["state_norm"]
                    
#                     # ğŸ†• è·å–qposè½¨è¿¹ç”¨äºå…³é”®æ—¶é—´æ®µåˆ†æ
#                     qpos_trajectory = res.get("qpos_trajectory", None)
#                 else:
#                     (
#                         content,
#                         _,
#                         states,
#                         _,
#                         actions,
#                         _,
#                         state_elem_mask,
#                         *image_metas,
#                         state_std,
#                         state_mean,
#                         state_norm,
#                     ) = self._safe_load(index)
#                     qpos_trajectory = None  # éHDF5æ¨¡å¼æš‚ä¸æ”¯æŒ

#                 data_dict = {}
#                 data_dict["dataset_name"] = content["dataset_name"]
#                 data_dict["data_idx"] = self.dataset_name2id[data_dict["dataset_name"]]
#                 data_dict["ctrl_freq"] = (self.control_freq[data_dict["dataset_name"]]
#                                           if random.random() > self.cond_mask_prob else 0)

#                 if self.state_noise_snr is not None:
#                     states += np.random.normal(
#                         0.0,
#                         state_std / np.sqrt(10**(self.state_noise_snr / 10)),
#                         states.shape,
#                     )
#                 ds_state_mean = np.array(self.dataset_stat[data_dict["dataset_name"]]["state_mean"])
#                 ds_state_mean = np.tile(ds_state_mean[None], (states.shape[0], 1))
                
#                 data_dict["states"] = (states if random.random() > self.cond_mask_prob else ds_state_mean)
#                 data_dict["actions"] = actions
#                 data_dict["state_elem_mask"] = (state_elem_mask if random.random() > self.cond_mask_prob else
#                                                 np.zeros_like(state_elem_mask))
#                 data_dict["state_norm"] = state_norm

#                 # ğŸ†• ç”Ÿæˆå…³é”®æ—¶é—´æ®µæ ‡ç­¾ - ä¿®å¤ç‰ˆæœ¬
#                 action_horizon = actions.shape[0]  # åŠ¨ä½œåºåˆ—é•¿åº¦
                
#                 if self.enable_critical_annotation:
#                     try:
#                         critical_labels = self._generate_critical_labels(qpos_trajectory, action_horizon)
                        
#                         # ğŸ”§ é¢å¤–éªŒè¯
#                         if critical_labels.shape[0] != action_horizon:
#                             print(f"âš ï¸ æ ·æœ¬ {index}: æ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é… {critical_labels.shape[0]} vs {action_horizon}")
#                             # é‡æ–°ç”Ÿæˆæ­£ç¡®å½¢çŠ¶çš„æ ‡ç­¾
#                             critical_labels = torch.zeros(action_horizon, dtype=torch.long)
#                             critical_labels[action_horizon//3:2*action_horizon//3] = 1
                        
#                         data_dict["critical_labels"] = critical_labels
                        
#                         # å¯é€‰ï¼šè®°å½•æ ‡æ³¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
#                         if hasattr(self.critical_annotator, 'verbose') and self.critical_annotator and self.critical_annotator.verbose:
#                             critical_ratio = critical_labels.float().mean().item()
#                             print(f"ğŸ“Š æ ·æœ¬ {index}: å…³é”®æ—¶é—´æ®µæ¯”ä¾‹ = {critical_ratio:.3f}, å½¢çŠ¶ = {critical_labels.shape}")
                            
#                     except Exception as e:
#                         print(f"âš ï¸ æ ·æœ¬ {index} å…³é”®æ—¶é—´æ®µæ ‡æ³¨å¤±è´¥: {e}")
#                         # ğŸ”§ å¤±è´¥æ—¶ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æ ‡ç­¾
#                         critical_labels = torch.zeros(action_horizon, dtype=torch.long)
#                         # ç®€å•æ¨¡å¼ï¼šæ¯4ä¸ªæ—¶é—´æ­¥æœ‰1ä¸ªå…³é”®æ—¶é—´æ®µ
#                         critical_labels[::4] = 1
#                         data_dict["critical_labels"] = critical_labels
#                 else:
#                     # å¦‚æœæ²¡æœ‰å¯ç”¨æ ‡æ³¨ï¼Œä½¿ç”¨ç®€å•çš„é»˜è®¤æ¨¡å¼
#                     critical_labels = torch.zeros(action_horizon, dtype=torch.long)
#                     # ä½¿ç”¨å¯å‘å¼ï¼šä¸­é—´50%ä¸ºå…³é”®æ—¶é—´æ®µ
#                     start_critical = action_horizon // 4
#                     end_critical = 3 * action_horizon // 4
#                     critical_labels[start_critical:end_critical] = 1
#                     data_dict["critical_labels"] = critical_labels

#                 # ğŸ”§ æœ€ç»ˆéªŒè¯critical_labels
#                 final_critical_labels = data_dict["critical_labels"]
#                 if not isinstance(final_critical_labels, torch.Tensor):
#                     final_critical_labels = torch.tensor(final_critical_labels, dtype=torch.long)
#                     data_dict["critical_labels"] = final_critical_labels
                
#                 # éªŒè¯å½¢çŠ¶å’Œç±»å‹
#                 assert final_critical_labels.shape == (action_horizon,), f"æ ‡ç­¾å½¢çŠ¶é”™è¯¯: {final_critical_labels.shape} vs ({action_horizon},)"
#                 assert final_critical_labels.dtype == torch.long, f"æ ‡ç­¾ç±»å‹é”™è¯¯: {final_critical_labels.dtype}"
#                 assert torch.all(final_critical_labels >= 0) and torch.all(final_critical_labels <= 1), f"æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´: {final_critical_labels.unique()}"

#                 # ... ç»§ç»­å¤„ç†å…¶ä»–æ•°æ®ï¼ˆå›¾åƒç­‰ï¼‰
#                 # Background image for padding/masking
#                 background_color = np.array(
#                     [int(x * 255) for x in self.image_processor.image_mean],
#                     dtype=np.uint8,
#                 ).reshape(1, 1, 3)
#                 background_image = (np.ones(
#                     (
#                         self.image_processor.size["height"],
#                         self.image_processor.size["width"],
#                         3,
#                     ),
#                     dtype=np.uint8,
#                 ) * background_color)

#                 image_metas = list(self.pairwise(image_metas))
#                 mask_probs = [self.cond_mask_prob] * self.num_cameras
#                 if self.cam_ext_mask_prob >= 0.0:
#                     mask_probs[0] = self.cam_ext_mask_prob
                
#                 rearranged_images = []
#                 for i in range(self.img_history_size):
#                     for j in range(self.num_cameras):
#                         images, image_mask = image_metas[j]
#                         image, valid = images[i], image_mask[i]
#                         if (valid and (math.prod(image.shape) > 0) and (random.random() > mask_probs[j])):
#                             rearranged_images.append((image, True))
#                         else:
#                             rearranged_images.append((background_image.copy(), False))

#                 # ä¸ºDINOv2å‡†å¤‡å•ç‹¬çš„å›¾åƒ
#                 if self.use_dinov2_features:
#                     camera_idx = 0
#                     frame_idx = self.img_history_size - 1
                    
#                     if camera_idx >= len(image_metas):
#                         raise ValueError(f"ç›¸æœºç´¢å¼• {camera_idx} è¶…å‡ºèŒƒå›´")
                    
#                     images, image_mask = image_metas[camera_idx]
#                     image, valid = images[frame_idx], image_mask[frame_idx]
                    
#                     if not valid or math.prod(image.shape) <= 0:
#                         raise ValueError(f"DINOv2å›¾åƒæ— æ•ˆ")
                    
#                     # é¢„å¤„ç†DINOv2å›¾åƒ
#                     pil_image = Image.fromarray(image)
#                     pil_image = pil_image.resize((self.dinov2_image_size, self.dinov2_image_size), Image.BILINEAR)
#                     image_array = np.array(pil_image).astype(np.float32) / 255.0
#                     image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)
                    
#                     # ImageNetæ ‡å‡†åŒ–
#                     mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
#                     std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
#                     image_tensor = (image_tensor - mean) / std
                    
#                     data_dict["dinov2_images"] = image_tensor.unsqueeze(0)

#                 # ä¸ºDepthAnythingV2å‡†å¤‡æ·±åº¦å›¾åƒ
#                 if self.use_depth_features:
#                     camera_idx = 0
#                     frame_idx = self.img_history_size - 1
                    
#                     if camera_idx >= len(image_metas):
#                         raise ValueError(f"æ·±åº¦ç¼–ç å™¨ç›¸æœºç´¢å¼• {camera_idx} è¶…å‡ºèŒƒå›´")
                    
#                     images, image_mask = image_metas[camera_idx]
#                     image, valid = images[frame_idx], image_mask[frame_idx]
                    
#                     if not valid or math.prod(image.shape) <= 0:
#                         raise ValueError(f"æ·±åº¦ç¼–ç å™¨å›¾åƒæ— æ•ˆ")
                    
#                     # DepthAnythingV2é¢„å¤„ç†
#                     pil_image = Image.fromarray(image)
#                     pil_image = pil_image.resize((self.depth_image_size, self.depth_image_size), Image.BILINEAR)
#                     image_array = np.array(pil_image).astype(np.float32) / 255.0
#                     image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)
                    
#                     # ImageNetæ ‡å‡†åŒ–
#                     mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
#                     std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
#                     image_tensor = (image_tensor - mean) / std
                    
#                     data_dict["depth_images"] = image_tensor.unsqueeze(0)

#                 # å¤„ç†åŸå§‹å›¾åƒï¼ˆSigLIPç¼–ç ç”¨ï¼‰
#                 preprocessed_images = []
#                 processor = self.image_processor
#                 for image, valid in rearranged_images:
#                     image = Image.fromarray(image)
#                     if self.image_size is not None:
#                         image = transforms.Resize(self.image_size)(image)

#                     if valid and self.auto_adjust_image_brightness:
#                         pixel_values = list(image.getdata())
#                         average_brightness = sum(sum(pixel) for pixel in pixel_values) / (len(pixel_values) * 255.0 * 3)
#                         if average_brightness <= 0.15:
#                             image = transforms.ColorJitter(brightness=(1.75, 1.75))(image)

#                     # å›¾åƒå¢å¼º
#                     if valid and self.image_aug and (random.random() > 0.5):
#                         aug_type = random.choice(["corrput_only", "color_only", "both"])
#                         if aug_type != "corrput_only":
#                             image = transforms.ColorJitter(brightness=0.3, contrast=0.4, saturation=0.5,
#                                                            hue=0.03)(image)
#                         if aug_type != "color_only":
#                             image = image_corrupt(image)

#                     if self.image_aspect_ratio == "pad":
#                         def expand2square(pil_img, background_color):
#                             width, height = pil_img.size
#                             if width == height:
#                                 return pil_img
#                             elif width > height:
#                                 result = Image.new(pil_img.mode, (width, width), background_color)
#                                 result.paste(pil_img, (0, (width - height) // 2))
#                                 return result
#                             else:
#                                 result = Image.new(pil_img.mode, (height, height), background_color)
#                                 result.paste(pil_img, ((height - width) // 2, 0))
#                                 return result

#                         image = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))
#                     image = processor.preprocess(image, return_tensors="pt")["pixel_values"][0]
#                     preprocessed_images.append(image)
#                 data_dict["images"] = preprocessed_images

#                 # å¤„ç†è¯­è¨€è¾“å…¥
#                 if self.use_precomp_lang_embed:
#                     if content["instruction"][-1] == ".":
#                         content["instruction"] = content["instruction"][:-1]
#                     data_dict["lang_embed"] = (torch.load(content["instruction"])
#                                                if random.random() > self.cond_mask_prob else self.empty_lang_embed)
#                 else:
#                     instruction = (content["instruction"] if random.random() > self.cond_mask_prob else "")
#                     data_dict["input_ids"] = self.tokenizer(
#                         instruction,
#                         return_tensors="pt",
#                         padding="longest",
#                         truncation=False,
#                     ).input_ids[0]

#                     assert (
#                         len(data_dict["input_ids"]) <= self.tokenizer_max_length
#                     ), f"Instruction length {len(data_dict['input_ids'])} exceeds the maximum length {self.tokenizer_max_length}."

#                 # è½¬æ¢numpyæ•°ç»„ä¸ºtensor
#                 for k, v in data_dict.items():
#                     if isinstance(v, np.ndarray):
#                         data_dict[k] = torch.from_numpy(v)

#                 # ğŸ”§ æœ€ç»ˆæ£€æŸ¥æ‰€æœ‰tensor
#                 for k, v in data_dict.items():
#                     assert not isinstance(v, np.ndarray), f"key: {k}, value: {v}"

#                 # ğŸ”§ æœ€ç»ˆéªŒè¯critical_labelsï¼ˆå†æ¬¡æ£€æŸ¥ï¼‰
#                 if "critical_labels" in data_dict:
#                     labels = data_dict["critical_labels"]
#                     if not isinstance(labels, torch.Tensor):
#                         print(f"âš ï¸ critical_labelsä¸æ˜¯tensor: {type(labels)}")
#                         data_dict["critical_labels"] = torch.tensor(labels, dtype=torch.long)
#                     elif labels.dtype != torch.long:
#                         print(f"âš ï¸ critical_labelsç±»å‹é”™è¯¯: {labels.dtype}")
#                         data_dict["critical_labels"] = labels.long()

#                 return data_dict
                
#             except BaseException as e:
#                 if data_dict is not None:
#                     print(f"Error catched when processing sample from {data_dict.get('dataset_name')}: {e}")
#                 else:
#                     print(f"Error catched when processing sample: {e}")
#                 traceback.print_exc()
#                 index = (index + 1) % len(self)

# # ä¿®å¤ train/dataset.py ä¸­çš„ DataCollatorForVLAConsumerDataset éƒ¨åˆ†

# class DataCollatorForVLAConsumerDataset(object):
#     """Collate examples for supervised training.
#     ğŸ†• æ”¯æŒå…³é”®æ—¶é—´æ®µæ ‡ç­¾çš„æ•°æ®æ”¶é›†
#     ğŸ”§ ä¿®å¤ç‰ˆæœ¬ï¼šå¢å¼ºé”™è¯¯å¤„ç†å’Œå½¢çŠ¶éªŒè¯
#     """

#     def __init__(self, tokenizer: transformers.PreTrainedTokenizer) -> None:
#         self.tokenizer = tokenizer

#     def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
#         batch = {
#             "states": [],
#             "actions": [],
#             "state_elem_mask": [],
#             "state_norm": [],
#             "images": [],
#             "data_indices": [],
#             "ctrl_freqs": [],
#         }
        
#         # è§†è§‰ç‰¹å¾æ”¶é›†
#         dinov2_images = []
#         depth_images = []
        
#         # ğŸ†• å…³é”®æ—¶é—´æ®µæ ‡ç­¾æ”¶é›† - ä¿®å¤ç‰ˆæœ¬
#         critical_labels = []
        
#         # è¯­è¨€ç‰¹å¾æ”¶é›†
#         input_ids = []
#         lang_embeds = []
#         lang_embed_lens = []

#         for idx, instance in enumerate(instances):
#             try:
#                 # æ”¶é›†åŸºç¡€æ•°æ®
#                 keys_to_check = ["states", "actions", "state_elem_mask", "state_norm"]
#                 for key in keys_to_check:
#                     if isinstance(instance[key], torch.Tensor):
#                         item = instance[key]
#                     else:
#                         item = torch.from_numpy(instance[key])
#                     batch[key].append(item)

#                 # æ”¶é›†è¯­è¨€æ•°æ®
#                 if "input_ids" in instance:
#                     input_ids.append(instance["input_ids"])
#                 else:
#                     lang_embeds.append(instance["lang_embed"])
#                     lang_embed_lens.append(instance["lang_embed"].shape[0])

#                 # æ”¶é›†å›¾åƒå’Œå…¶ä»–æ•°æ®
#                 batch["images"].append(torch.stack(instance["images"], dim=0))
#                 batch["data_indices"].append(instance["data_idx"])
#                 batch["ctrl_freqs"].append(instance["ctrl_freq"])
                
#                 # æ”¶é›†DINOv2å›¾åƒ
#                 if "dinov2_images" in instance:
#                     dinov2_images.append(instance["dinov2_images"])
                
#                 # æ”¶é›†DepthAnythingV2å›¾åƒ
#                 if "depth_images" in instance:
#                     depth_images.append(instance["depth_images"])
                
#                 # ğŸ†• æ”¶é›†å…³é”®æ—¶é—´æ®µæ ‡ç­¾ - ä¿®å¤ç‰ˆæœ¬
#                 if "critical_labels" in instance:
#                     labels = instance["critical_labels"]
                    
#                     # ğŸ”§ éªŒè¯å’Œè½¬æ¢æ ‡ç­¾
#                     if not isinstance(labels, torch.Tensor):
#                         labels = torch.tensor(labels, dtype=torch.long)
#                     elif labels.dtype != torch.long:
#                         labels = labels.long()
                    
#                     # ğŸ”§ éªŒè¯æ ‡ç­¾å€¼èŒƒå›´
#                     if torch.any(labels < 0) or torch.any(labels > 1):
#                         print(f"âš ï¸ å®ä¾‹ {idx}: æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´ [0,1]: {labels.unique()}")
#                         labels = torch.clamp(labels, 0, 1)
                    
#                     # ğŸ”§ éªŒè¯å½¢çŠ¶
#                     if labels.dim() != 1:
#                         print(f"âš ï¸ å®ä¾‹ {idx}: æ ‡ç­¾ç»´åº¦é”™è¯¯: {labels.shape}ï¼Œåº”è¯¥æ˜¯1D")
#                         if labels.numel() > 0:
#                             labels = labels.flatten()
#                         else:
#                             # å¦‚æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æ ‡ç­¾
#                             action_len = instance["actions"].shape[0]
#                             labels = torch.zeros(action_len, dtype=torch.long)
                    
#                     critical_labels.append(labels)
#                 else:
#                     # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œåˆ›å»ºé»˜è®¤æ ‡ç­¾
#                     action_len = instance["actions"].shape[0]
#                     default_labels = torch.zeros(action_len, dtype=torch.long)
#                     # ç®€å•å¯å‘å¼ï¼šä¸­é—´éƒ¨åˆ†ä¸ºå…³é”®æ—¶é—´æ®µ
#                     start_critical = action_len // 4
#                     end_critical = 3 * action_len // 4
#                     default_labels[start_critical:end_critical] = 1
#                     critical_labels.append(default_labels)
                    
#             except Exception as e:
#                 print(f"âŒ å¤„ç†å®ä¾‹ {idx} æ—¶å‡ºé”™: {e}")
#                 import traceback
#                 traceback.print_exc()
                
#                 # ğŸ”§ åˆ›å»ºé™çº§æ•°æ®
#                 action_len = 64  # é»˜è®¤é•¿åº¦
#                 if "actions" in instance:
#                     try:
#                         action_len = instance["actions"].shape[0]
#                     except:
#                         pass
                
#                 # æ·»åŠ é»˜è®¤çš„critical_labels
#                 default_labels = torch.zeros(action_len, dtype=torch.long)
#                 default_labels[action_len//3:2*action_len//3] = 1  # ä¸­é—´1/3ä¸ºå…³é”®
#                 critical_labels.append(default_labels)
                
#                 # ç»§ç»­å¤„ç†å…¶ä»–å¿…è¦å­—æ®µ...
#                 continue

#         # ğŸ”§ éªŒè¯æ‰€æœ‰å®ä¾‹éƒ½æœ‰æ ‡ç­¾
#         if len(critical_labels) != len(instances):
#             print(f"âš ï¸ æ ‡ç­¾æ•°é‡ä¸åŒ¹é…: {len(critical_labels)} vs {len(instances)}")
#             # è¡¥å……ç¼ºå¤±çš„æ ‡ç­¾
#             while len(critical_labels) < len(instances):
#                 default_labels = torch.zeros(64, dtype=torch.long)
#                 critical_labels.append(default_labels)

#         # å †å åŸºç¡€æ•°æ®
#         keys_to_stack = ["states", "actions", "state_elem_mask", "state_norm", "images"]
#         for key in keys_to_stack:
#             batch[key] = torch.stack(batch[key], dim=0)

#         batch["ctrl_freqs"] = torch.tensor(batch["ctrl_freqs"])

#         # å¤„ç†è¯­è¨€æ•°æ®
#         if len(input_ids) > 0:
#             input_ids = torch.nn.utils.rnn.pad_sequence(input_ids,
#                                                         batch_first=True,
#                                                         padding_value=self.tokenizer.pad_token_id)
#             batch["input_ids"] = input_ids
#             batch["lang_attn_mask"] = input_ids.ne(self.tokenizer.pad_token_id)
#         else:
#             lang_embeds = torch.nn.utils.rnn.pad_sequence(lang_embeds, batch_first=True, padding_value=0)
#             input_lang_attn_mask = torch.zeros(lang_embeds.shape[0], lang_embeds.shape[1], dtype=torch.bool)
#             for i, l in enumerate(lang_embed_lens):
#                 input_lang_attn_mask[i, :l] = True
#             batch["lang_embeds"] = lang_embeds
#             batch["lang_attn_mask"] = input_lang_attn_mask
        
#         # å †å DINOv2å›¾åƒ
#         if len(dinov2_images) > 0:
#             batch["dinov2_images"] = torch.stack(dinov2_images, dim=0)
        
#         # å †å DepthAnythingV2å›¾åƒ
#         if len(depth_images) > 0:
#             batch["depth_images"] = torch.stack(depth_images, dim=0)
        
#         # ğŸ†• å †å å…³é”®æ—¶é—´æ®µæ ‡ç­¾ - ä¿®å¤ç‰ˆæœ¬
#         if len(critical_labels) > 0:
#             try:
#                 # ğŸ”§ æ‰¾åˆ°æœ€å¤§é•¿åº¦è¿›è¡Œå¡«å……
#                 max_len = max(labels.shape[0] for labels in critical_labels)
                
#                 # ğŸ”§ éªŒè¯æ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯1D
#                 validated_labels = []
#                 for i, labels in enumerate(critical_labels):
#                     if labels.dim() != 1:
#                         print(f"âš ï¸ æ ‡ç­¾ {i} ç»´åº¦é”™è¯¯: {labels.shape}")
#                         labels = labels.flatten()
                    
#                     # ğŸ”§ å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦
#                     if labels.shape[0] < max_len:
#                         padding_len = max_len - labels.shape[0]
#                         # ä½¿ç”¨0å¡«å……ï¼ˆéå…³é”®æ—¶é—´æ®µï¼‰
#                         padding = torch.zeros(padding_len, dtype=labels.dtype)
#                         labels = torch.cat([labels, padding], dim=0)
#                     elif labels.shape[0] > max_len:
#                         # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
#                         labels = labels[:max_len]
                    
#                     validated_labels.append(labels)
                
#                 # ğŸ”§ å †å æ ‡ç­¾
#                 batch["critical_labels"] = torch.stack(validated_labels, dim=0)
                
#                 # ğŸ”§ æœ€ç»ˆéªŒè¯
#                 final_shape = batch["critical_labels"].shape
#                 expected_shape = (len(instances), max_len)
                
#                 if final_shape != expected_shape:
#                     print(f"âš ï¸ æœ€ç»ˆæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…: {final_shape} vs æœŸæœ›çš„ {expected_shape}")
#                     # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„é»˜è®¤æ ‡ç­¾
#                     batch["critical_labels"] = torch.zeros(len(instances), max_len, dtype=torch.long)
#                     # ä½¿ç”¨ç®€å•æ¨¡å¼å¡«å……
#                     for i in range(len(instances)):
#                         # ä¸­é—´40%ä¸ºå…³é”®æ—¶é—´æ®µ
#                         start_idx = int(max_len * 0.3)
#                         end_idx = int(max_len * 0.7)
#                         batch["critical_labels"][i, start_idx:end_idx] = 1
                
#                 # ğŸ”§ éªŒè¯æ•°æ®ç±»å‹å’Œå€¼èŒƒå›´
#                 if batch["critical_labels"].dtype != torch.long:
#                     batch["critical_labels"] = batch["critical_labels"].long()
                
#                 # éªŒè¯å€¼åœ¨[0,1]èŒƒå›´å†…
#                 if torch.any(batch["critical_labels"] < 0) or torch.any(batch["critical_labels"] > 1):
#                     print(f"âš ï¸ æ‰¹æ¬¡ä¸­æœ‰æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´: {batch['critical_labels'].unique()}")
#                     batch["critical_labels"] = torch.clamp(batch["critical_labels"], 0, 1)
#             except Exception as e:
#                 print(f"âŒ å¤„ç†å…³é”®æ—¶é—´æ®µæ ‡ç­¾æ—¶å‡ºé”™: {e}")
#                 import traceback
#                 traceback.print_exc()
                
#                 # ğŸ”§ åˆ›å»ºå®‰å…¨çš„é»˜è®¤æ ‡ç­¾
#                 print("ğŸ”§ ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æ ‡ç­¾")
#                 batch_size = len(instances)
#                 default_seq_len = 64  # é»˜è®¤åºåˆ—é•¿åº¦
                
#                 # å°è¯•ä»actionsè·å–çœŸå®é•¿åº¦
#                 if "actions" in batch and len(batch["actions"]) > 0:
#                     try:
#                         default_seq_len = batch["actions"].shape[1]
#                     except:
#                         pass
                
#                 # åˆ›å»ºé»˜è®¤æ ‡ç­¾å¼ é‡
#                 batch["critical_labels"] = torch.zeros(batch_size, default_seq_len, dtype=torch.long)
                
#                 # ä¸ºæ¯ä¸ªåºåˆ—è®¾ç½®ç®€å•çš„å…³é”®æ—¶é—´æ®µæ¨¡å¼
#                 for i in range(batch_size):
#                     # ç­–ç•¥ï¼šå¼€å§‹25%å’Œç»“æŸ25%ä¸ºå…³é”®æ—¶é—´æ®µ
#                     quarter_len = default_seq_len // 4
#                     batch["critical_labels"][i, :quarter_len] = 1          # å¼€å§‹é˜¶æ®µ
#                     batch["critical_labels"][i, -quarter_len:] = 1         # ç»“æŸé˜¶æ®µ
        
#         else:
#             # ğŸ”§ å¦‚æœå®Œå…¨æ²¡æœ‰æ ‡ç­¾ï¼Œåˆ›å»ºé»˜è®¤æ‰¹æ¬¡
#             print("âš ï¸ æ²¡æœ‰ä»»ä½•å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼Œåˆ›å»ºé»˜è®¤æ‰¹æ¬¡")
#             batch_size = len(instances)
#             default_seq_len = 64
            
#             if "actions" in batch and len(batch["actions"]) > 0:
#                 try:
#                     default_seq_len = batch["actions"].shape[1]
#                 except:
#                     pass
            
#             batch["critical_labels"] = torch.zeros(batch_size, default_seq_len, dtype=torch.long)
#             # ç®€å•ç­–ç•¥ï¼šä¸­é—´50%ä¸ºå…³é”®æ—¶é—´æ®µ
#             start_critical = default_seq_len // 4
#             end_critical = 3 * default_seq_len // 4
#             batch["critical_labels"][:, start_critical:end_critical] = 1

#         return batch
    
    
    
    
    
    
    
    
    
# train/dataset.py - æ›´æ–°ç‰ˆæœ¬ï¼Œé›†æˆä»»åŠ¡é©±åŠ¨çš„å…³é”®æ—¶é—´æ®µæ ‡æ³¨

import traceback
import time
import os
import json
import math
import random
from typing import Dict, Sequence

import numpy as np
import torch
from torch.utils.data import Dataset
from torchvision import transforms
from PIL import Image
import transformers

from data.filelock import FileLock
from data.hdf5_vla_dataset import HDF5VLADataset
from train.image_corrupt import image_corrupt

# ğŸ†• å¯¼å…¥ä»»åŠ¡é©±åŠ¨çš„å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨
from data.critical_timestep_annotator import (
    TaskType, 
    create_silent_task_annotator,
    TaskDrivenCriticalTimestepAnnotator
)


def get_clean_item(chunk_dir):
    """Get indexes of clean items in a chunk."""
    dirty_bit = read_dirty_bit(chunk_dir)
    return np.where(1 - dirty_bit)[0].tolist()


def save_dirty_bit(chunk_dir, dirty_bit):
    """Save the dirty bit to the chunk directory."""
    time_stmp = time.time()
    while time.time() - time_stmp < 10.0:
        try:
            file_path = os.path.join(chunk_dir, "dirty_bit")
            lock = FileLock(file_path)
            lock.acquire_write_lock()
            with open(file_path, "wb") as file:
                file.write(dirty_bit.tobytes())
            lock.release_lock()
            return
        except KeyboardInterrupt:
            lock.release_lock()
            raise KeyboardInterrupt
        except BaseException:
            lock.release_lock()
            continue
    raise RuntimeError("Failed to save dirty bit.")


def read_dirty_bit(chunk_dir):
    """Read the dirty bit from the chunk directory."""
    time_stmp = time.time()
    while time.time() - time_stmp < 10.0:
        try:
            file_path = os.path.join(chunk_dir, "dirty_bit")
            lock = FileLock(file_path)
            lock.acquire_read_lock()
            with open(file_path, "rb") as file:
                dirty_bit = np.frombuffer(file.read(), dtype=np.uint8).copy()
            lock.release_lock()
            assert len(dirty_bit) > 0
            return dirty_bit
        except KeyboardInterrupt:
            lock.release_lock()
            raise KeyboardInterrupt
        except BaseException:
            lock.release_lock()
            continue
    raise RuntimeError("Failed to read dirty bit.")


class VLAConsumerDataset(Dataset):
    """A vision-language-action Dataset for supervised training.
    ğŸ†• é›†æˆä»»åŠ¡é©±åŠ¨çš„å…³é”®æ—¶é—´æ®µæ ‡æ³¨æœºåˆ¶
    """

    def __init__(
        self,
        model_config_path,
        config,
        tokenizer,
        image_processor,
        num_cameras,
        img_history_size,
        image_size=None,
        auto_adjust_image_brightness=False,
        image_aug=False,
        dataset_type="pretrain",
        cond_mask_prob=0.1,
        cam_ext_mask_prob=-1.0,
        state_noise_snr=None,
        use_hdf5=False,
        use_precomp_lang_embed=False,
        use_dinov2_features=False,
        use_depth_features=False,
        # ğŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨ç›¸å…³å‚æ•°
        task_type: int = 1,  # 1=æŠ“å–ç±», 2=ç‚¹å‡»ç±»
        enable_critical_annotation: bool = True,
        critical_annotation_config: Dict = None,
    ):
        super(VLAConsumerDataset, self).__init__()

        # Load the control frequency for each dataset
        with open("configs/dataset_control_freq.json", "r") as fp:
            self.control_freq = json.load(fp)
        # Load the dataset names
        dataset_names_cfg = ("configs/pretrain_datasets.json"
                             if dataset_type == "pretrain" else "configs/finetune_datasets.json")
        with open(dataset_names_cfg, "r") as file:
            DATASET_NAMES = json.load(file)
        # Create the mapping between dataset name and id
        self.dataset_name2id = {name: i for i, name in enumerate(DATASET_NAMES)}
        self.dataset_id2name = {i: name for i, name in enumerate(DATASET_NAMES)}

        self.image_processor = image_processor
        self.model_config_path = model_config_path
        self.buffer_dir = config["buf_path"]
        self.num_chunks = config["buf_num_chunks"]
        self.chunk_size = config["buf_chunk_size"]
        self.tokenizer_max_length = config["tokenizer_max_length"]
        self.image_aspect_ratio = config["image_aspect_ratio"]
        self.state_noise_snr = state_noise_snr
        self.num_cameras = num_cameras
        self.img_history_size = img_history_size
        self.cond_mask_prob = cond_mask_prob
        self.cam_ext_mask_prob = cam_ext_mask_prob
        self.use_hdf5 = use_hdf5
        self.hdf5_dataset = None
        if use_hdf5:
            self.hdf5_dataset = HDF5VLADataset(self.model_config_path)
        self.use_precomp_lang_embed = use_precomp_lang_embed
        if use_precomp_lang_embed:
            self.empty_lang_embed = torch.load("data/empty_lang_embed.pt")
        
        # DINOv2ç›¸å…³é…ç½®
        self.use_dinov2_features = use_dinov2_features
        self.dinov2_image_size = 518
        
        # DepthAnythingV2ç›¸å…³é…ç½®
        self.use_depth_features = use_depth_features
        self.depth_image_size = 518

        # Load dataset stat
        with open("configs/dataset_stat.json", "r") as f:
            dataset_stat = json.load(f)
        self.dataset_stat = dataset_stat

        self.tokenizer = tokenizer
        self.image_size = image_size
        self.auto_adjust_image_brightness = auto_adjust_image_brightness
        self.image_aug = image_aug

        self.last_content = None
        self.last_meta = None
        
        # ğŸ†• å…³é”®æ—¶é—´æ®µæ ‡æ³¨é…ç½®
        self.task_type = TaskType(task_type)
        self.enable_critical_annotation = enable_critical_annotation
        
        if enable_critical_annotation:
            # è®¾ç½®é»˜è®¤é…ç½®
            default_config = {
                'relative_low_speed_ratio': 0.15,
                'min_deceleration_threshold': -0.0008,
                'gripper_close_delta_threshold': -0.01,
                'smooth': True,
                'verbose': False
            }
            
            # åˆå¹¶ç”¨æˆ·é…ç½®
            if critical_annotation_config:
                default_config.update(critical_annotation_config)
            
            # åˆ›å»ºæ ‡æ³¨å™¨
            self.critical_annotator = TaskDrivenCriticalTimestepAnnotator(
                task_type=self.task_type,
                **default_config
            )
            
            print(f"ğŸ¯ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å™¨åˆå§‹åŒ–:")
            print(f"   - ä»»åŠ¡ç±»å‹: {self.task_type.name} ({self.task_type.value})")
            print(f"   - é…ç½®: {default_config}")
        else:
            self.critical_annotator = None
            print("âš ï¸  å…³é”®æ—¶é—´æ®µæ ‡æ³¨å·²ç¦ç”¨")

    def get_dataset_name2id(self):
        return self.dataset_name2id

    def get_dataset_id2name(self):
        return self.dataset_id2name

    @staticmethod
    def pairwise(iterable):
        a = iter(iterable)
        return zip(a, a)

    @staticmethod
    def _load_data_from_chunk(chunk_dir, chunk_item_idx):
        time_stmp = time.time()
        while time.time() - time_stmp < 10.0:
            try:
                locks = []
                file_path = os.path.join(chunk_dir, f"json_content_{chunk_item_idx}.json")
                lock = FileLock(file_path)
                locks.append(lock)
                lock.acquire_read_lock()
                with open(file_path, "r") as file:
                    json_content = json.load(file)
                lock.release_lock()
                file_path = os.path.join(chunk_dir, f"sample_{chunk_item_idx}.npz")
                lock = FileLock(file_path)
                locks.append(lock)
                lock.acquire_read_lock()
                with open(file_path, "rb") as file:
                    sample_dict = np.load(file)
                    meta = tuple(sample_dict.values())
                lock.release_lock()
                return json_content, meta
            except KeyboardInterrupt:
                for lock in locks:
                    lock.release_lock()
                raise KeyboardInterrupt
            except BaseException:
                for lock in locks:
                    lock.release_lock()
                continue
        raise RuntimeError("Failed to load sample.")

    def __len__(self) -> int:
        if self.use_hdf5:
            return len(self.hdf5_dataset)
        else:
            return self.num_chunks * self.chunk_size

    def _safe_load(self, index):
        read_chunk_item_indices = []
        # Start searching from a random chunk
        read_chunk_idx = index // self.chunk_size
        while len(read_chunk_item_indices) == 0:
            read_chunk_dir = os.path.join(self.buffer_dir, f"chunk_{read_chunk_idx}")
            try:
                read_chunk_item_indices = get_clean_item(read_chunk_dir)
            except BaseException as e:
                print("Error catched when searching a clean chunk:", e)
                traceback.print_exc()
                read_chunk_item_indices = []
            read_chunk_idx = (read_chunk_idx + 1) % self.num_chunks

        random_item_index = index % len(read_chunk_item_indices)
        read_chunk_item_index = read_chunk_item_indices[random_item_index]

        # Modify the dirty bit
        try:
            dirty_bit = read_dirty_bit(read_chunk_dir)
            dirty_bit[read_chunk_item_index] = 1
            save_dirty_bit(read_chunk_dir, dirty_bit)
        except BaseException as e:
            print("Error catched when modifying the dirty bit:", e)
            traceback.print_exc()

        # load the sample
        try:
            content, meta = self._load_data_from_chunk(read_chunk_dir, read_chunk_item_index)
            self.last_content, self.last_meta = content, meta
        except BaseException as e:
            print("Error catched when loading sample:", e)
            traceback.print_exc()
            content, meta = self.last_content, self.last_meta

        return (content, *meta)

    def _generate_critical_labels(self, qpos_trajectory, action_horizon=64):
        """
        ğŸ†• ç”Ÿæˆå…³é”®æ—¶é—´æ®µæ ‡ç­¾
        ğŸ”§ ä¿®å¤ç‰ˆæœ¬ï¼šç¡®ä¿è¾“å‡ºå½¢çŠ¶æ­£ç¡®
        
        Args:
            qpos_trajectory: (T, 14) numpyæ•°ç»„ï¼Œå…³èŠ‚è§’åº¦è½¨è¿¹
            action_horizon: åŠ¨ä½œé¢„æµ‹çš„æ—¶é—´èŒƒå›´
            
        Returns:
            critical_labels: (action_horizon,) torch tensorï¼Œå…³é”®æ—¶é—´æ®µæ ‡ç­¾
        """
        try:
            if self.critical_annotator is None:
                # å¦‚æœæ²¡æœ‰æ ‡æ³¨å™¨ï¼Œè¿”å›åˆç†çš„é»˜è®¤æ ‡ç­¾
                # ä½¿ç”¨ç®€å•çš„å¯å‘å¼ï¼šå‰25%å’Œå25%ä¸ºå…³é”®æ—¶é—´æ®µ
                critical_labels = np.zeros(action_horizon, dtype=np.int64)
                
                # å¼€å§‹é˜¶æ®µï¼ˆå‰25%ï¼‰å’Œç»“æŸé˜¶æ®µï¼ˆå25%ï¼‰æ ‡è®°ä¸ºå…³é”®
                start_critical = int(action_horizon * 0.25)
                end_critical = int(action_horizon * 0.75)
                
                critical_labels[:start_critical] = 1  # å¼€å§‹é˜¶æ®µ
                critical_labels[end_critical:] = 1    # ç»“æŸé˜¶æ®µ
                
                return torch.from_numpy(critical_labels).long()
            
            # æœ‰æ ‡æ³¨å™¨æ—¶ä½¿ç”¨æ ‡æ³¨å™¨
            if qpos_trajectory is None or len(qpos_trajectory) == 0:
                print("âš ï¸ qpos_trajectoryä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾")
                # è¿”å›äº¤æ›¿æ¨¡å¼çš„æ ‡ç­¾
                critical_labels = np.zeros(action_horizon, dtype=np.int64)
                critical_labels[action_horizon//4:action_horizon//2] = 1  # ä¸­é—´ä¸€æ®µä¸ºå…³é”®
                return torch.from_numpy(critical_labels).long()
            
            # ä½¿ç”¨æ ‡æ³¨å™¨ç”Ÿæˆæ ‡ç­¾
            critical_labels, analysis_info = self.critical_annotator.annotate(qpos_trajectory)
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ ‡ç­¾æ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹
            if not isinstance(critical_labels, np.ndarray):
                critical_labels = np.array(critical_labels, dtype=np.int64)
            else:
                critical_labels = critical_labels.astype(np.int64)
            
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†é•¿åº¦ä¸åŒ¹é…çš„é—®é¢˜
            # if len(critical_labels)  # ğŸ”´ è§†è§‰èåˆæ¨¡å¼ä¸éœ€è¦ > action_horizon:
                # å¦‚æœæ ‡æ³¨ç»“æœå¤ªé•¿ï¼Œæˆªå–å‰action_horizonä¸ª
#                critical_labels = critical_labels[:action_horizon]
            el# if len(critical_labels)  # ğŸ”´ è§†è§‰èåˆæ¨¡å¼ä¸éœ€è¦ < action_horizon:
                # å¦‚æœæ ‡æ³¨ç»“æœå¤ªçŸ­ï¼Œä½¿ç”¨ç­–ç•¥å¡«å……
#                padding_len = action_horizon - len(critical_labels)
                
#                if len(critical_labels) > 0:
                    # ä½¿ç”¨æœ€åä¸€ä¸ªå€¼å¡«å……
#                    last_value = critical_labels[-1]
#                    padding = np.full(padding_len, last_value, dtype=np.int64)
#                else:
                    # å¦‚æœå®Œå…¨æ²¡æœ‰æ ‡æ³¨ï¼Œä½¿ç”¨0å¡«å……
#                    padding = np.zeros(padding_len, dtype=np.int64)
                
#                critical_labels = np.concatenate([critical_labels, padding])
            
            # ğŸ”§ ä¿®å¤ï¼šéªŒè¯è¾“å‡ºå½¢çŠ¶
            assert len(critical_labels) == action_horizon, f"æ ‡ç­¾é•¿åº¦ä¸åŒ¹é…: {len(critical_labels)} vs {action_horizon}"
            assert critical_labels.dtype == np.int64, f"æ ‡ç­¾ç±»å‹é”™è¯¯: {critical_labels.dtype}"
            
            # è½¬æ¢ä¸ºtorch tensor
            critical_labels_tensor = torch.from_numpy(critical_labels).long()
            
            # ğŸ”§ éªŒè¯å€¼çš„èŒƒå›´
            if torch.any(critical_labels_tensor < 0) or torch.any(critical_labels_tensor > 1):
                print(f"âš ï¸ æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´ [0,1]: {critical_labels_tensor.unique()}")
                # å°†æ‰€æœ‰é0å€¼è½¬æ¢ä¸º1
                critical_labels_tensor = torch.clamp(critical_labels_tensor, 0, 1)
            
            return critical_labels_tensor
            
        except Exception as e:
            print(f"âš ï¸ å…³é”®æ—¶é—´æ®µæ ‡æ³¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # ğŸ”§ é™çº§æ–¹æ¡ˆï¼šè¿”å›å®‰å…¨çš„é»˜è®¤æ ‡ç­¾
            print("ğŸ”§ ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æ ‡ç­¾")
            critical_labels = np.zeros(action_horizon, dtype=np.int64)
            
            # ç®€å•ç­–ç•¥ï¼šä¸­é—´30%ä¸ºå…³é”®æ—¶é—´æ®µ
            start_idx = int(action_horizon * 0.35)
            end_idx = int(action_horizon * 0.65)
            critical_labels[start_idx:end_idx] = 1
            
            return torch.from_numpy(critical_labels).long()

    def __getitem__(self, index):
        # For robustness, we will try to load the data until we succeed
        while True:
            data_dict = None
            try:
                if self.use_hdf5:
                    res = self.hdf5_dataset.get_item()
                    content = res["meta"]
                    states = res["state"]
                    actions = res["actions"]
                    state_elem_mask = res["state_indicator"]
                    image_metas = [
                        res["cam_high"],
                        res["cam_high_mask"],
                        res["cam_right_wrist"],
                        res["cam_right_wrist_mask"],
                        res["cam_left_wrist"],
                        res["cam_left_wrist_mask"],
                    ]
                    state_std = res["state_std"]
                    state_mean = res["state_mean"]
                    state_norm = res["state_norm"]
                    
                    # ğŸ†• è·å–qposè½¨è¿¹ç”¨äºå…³é”®æ—¶é—´æ®µåˆ†æ
                    qpos_trajectory = res.get("qpos_trajectory", None)
                else:
                    (
                        content,
                        _,
                        states,
                        _,
                        actions,
                        _,
                        state_elem_mask,
                        *image_metas,
                        state_std,
                        state_mean,
                        state_norm,
                    ) = self._safe_load(index)
                    qpos_trajectory = None  # éHDF5æ¨¡å¼æš‚ä¸æ”¯æŒ

                data_dict = {}
                data_dict["dataset_name"] = content["dataset_name"]
                data_dict["data_idx"] = self.dataset_name2id[data_dict["dataset_name"]]
                data_dict["ctrl_freq"] = (self.control_freq[data_dict["dataset_name"]]
                                          if random.random() > self.cond_mask_prob else 0)

                if self.state_noise_snr is not None:
                    states += np.random.normal(
                        0.0,
                        state_std / np.sqrt(10**(self.state_noise_snr / 10)),
                        states.shape,
                    )
                ds_state_mean = np.array(self.dataset_stat[data_dict["dataset_name"]]["state_mean"])
                ds_state_mean = np.tile(ds_state_mean[None], (states.shape[0], 1))
                
                data_dict["states"] = (states if random.random() > self.cond_mask_prob else ds_state_mean)
                data_dict["actions"] = actions
                data_dict["state_elem_mask"] = (state_elem_mask if random.random() > self.cond_mask_prob else
                                                np.zeros_like(state_elem_mask))
                data_dict["state_norm"] = state_norm

                # # ğŸ†• ç”Ÿæˆå…³é”®æ—¶é—´æ®µæ ‡ç­¾ - ä¿®å¤ç‰ˆæœ¬
                # action_horizon = actions.shape[0]  # åŠ¨ä½œåºåˆ—é•¿åº¦
                
                # if self.enable_critical_annotation:
                #     try:
                #         critical_labels = self._generate_critical_labels(qpos_trajectory, action_horizon)
                        
                #         # ğŸ”§ é¢å¤–éªŒè¯
                #         if critical_labels.shape[0] != action_horizon:
                #             print(f"âš ï¸ æ ·æœ¬ {index}: æ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é… {critical_labels.shape[0]} vs {action_horizon}")
                #             # é‡æ–°ç”Ÿæˆæ­£ç¡®å½¢çŠ¶çš„æ ‡ç­¾
                #             critical_labels = torch.zeros(action_horizon, dtype=torch.long)
                #             critical_labels[action_horizon//3:2*action_horizon//3] = 1
                        
                #         data_dict["critical_labels"] = critical_labels
                        
                #         # å¯é€‰ï¼šè®°å½•æ ‡æ³¨ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                #         if hasattr(self.critical_annotator, 'verbose') and self.critical_annotator and self.critical_annotator.verbose:
                #             critical_ratio = critical_labels.float().mean().item()
                #             print(f"ğŸ“Š æ ·æœ¬ {index}: å…³é”®æ—¶é—´æ®µæ¯”ä¾‹ = {critical_ratio:.3f}, å½¢çŠ¶ = {critical_labels.shape}")
                            
                #     except Exception as e:
                #         print(f"âš ï¸ æ ·æœ¬ {index} å…³é”®æ—¶é—´æ®µæ ‡æ³¨å¤±è´¥: {e}")
                #         # ğŸ”§ å¤±è´¥æ—¶ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æ ‡ç­¾
                #         critical_labels = torch.zeros(action_horizon, dtype=torch.long)
                #         # ç®€å•æ¨¡å¼ï¼šæ¯4ä¸ªæ—¶é—´æ­¥æœ‰1ä¸ªå…³é”®æ—¶é—´æ®µ
                #         critical_labels[::4] = 1
                #         data_dict["critical_labels"] = critical_labels
                # else:
                #     # å¦‚æœæ²¡æœ‰å¯ç”¨æ ‡æ³¨ï¼Œä½¿ç”¨ç®€å•çš„é»˜è®¤æ¨¡å¼
                #     critical_labels = torch.zeros(action_horizon, dtype=torch.long)
                #     # ä½¿ç”¨å¯å‘å¼ï¼šä¸­é—´50%ä¸ºå…³é”®æ—¶é—´æ®µ
                #     start_critical = action_horizon // 4
                #     end_critical = 3 * action_horizon // 4
                #     critical_labels[start_critical:end_critical] = 1
                #     data_dict["critical_labels"] = critical_labels

                # # ğŸ”§ æœ€ç»ˆéªŒè¯critical_labels
                # final_critical_labels = data_dict["critical_labels"]
                # if not isinstance(final_critical_labels, torch.Tensor):
                #     final_critical_labels = torch.tensor(final_critical_labels, dtype=torch.long)
                #     data_dict["critical_labels"] = final_critical_labels
                
                # # éªŒè¯å½¢çŠ¶å’Œç±»å‹
                # assert final_critical_labels.shape == (action_horizon,), f"æ ‡ç­¾å½¢çŠ¶é”™è¯¯: {final_critical_labels.shape} vs ({action_horizon},)"
                # assert final_critical_labels.dtype == torch.long, f"æ ‡ç­¾ç±»å‹é”™è¯¯: {final_critical_labels.dtype}"
                # assert torch.all(final_critical_labels >= 0) and torch.all(final_critical_labels <= 1), f"æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´: {final_critical_labels.unique()}"

                # ... ç»§ç»­å¤„ç†å…¶ä»–æ•°æ®ï¼ˆå›¾åƒç­‰ï¼‰
                # Background image for padding/masking
                background_color = np.array(
                    [int(x * 255) for x in self.image_processor.image_mean],
                    dtype=np.uint8,
                ).reshape(1, 1, 3)
                background_image = (np.ones(
                    (
                        self.image_processor.size["height"],
                        self.image_processor.size["width"],
                        3,
                    ),
                    dtype=np.uint8,
                ) * background_color)

                image_metas = list(self.pairwise(image_metas))
                mask_probs = [self.cond_mask_prob] * self.num_cameras
                if self.cam_ext_mask_prob >= 0.0:
                    mask_probs[0] = self.cam_ext_mask_prob
                
                rearranged_images = []
                for i in range(self.img_history_size):
                    for j in range(self.num_cameras):
                        images, image_mask = image_metas[j]
                        image, valid = images[i], image_mask[i]
                        if (valid and (math.prod(image.shape) > 0) and (random.random() > mask_probs[j])):
                            rearranged_images.append((image, True))
                        else:
                            rearranged_images.append((background_image.copy(), False))

                # ä¸ºDINOv2å‡†å¤‡å•ç‹¬çš„å›¾åƒ
                if self.use_dinov2_features:
                    camera_idx = 0
                    frame_idx = self.img_history_size - 1
                    
                    if camera_idx >= len(image_metas):
                        raise ValueError(f"ç›¸æœºç´¢å¼• {camera_idx} è¶…å‡ºèŒƒå›´")
                    
                    images, image_mask = image_metas[camera_idx]
                    image, valid = images[frame_idx], image_mask[frame_idx]
                    
                    if not valid or math.prod(image.shape) <= 0:
                        raise ValueError(f"DINOv2å›¾åƒæ— æ•ˆ")
                    
                    # é¢„å¤„ç†DINOv2å›¾åƒ
                    pil_image = Image.fromarray(image)
                    pil_image = pil_image.resize((self.dinov2_image_size, self.dinov2_image_size), Image.BILINEAR)
                    image_array = np.array(pil_image).astype(np.float32) / 255.0
                    image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)
                    
                    # ImageNetæ ‡å‡†åŒ–
                    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                    image_tensor = (image_tensor - mean) / std
                    
                    data_dict["dinov2_images"] = image_tensor.unsqueeze(0)

                # ä¸ºDepthAnythingV2å‡†å¤‡æ·±åº¦å›¾åƒ
                if self.use_depth_features:
                    camera_idx = 0
                    frame_idx = self.img_history_size - 1
                    
                    if camera_idx >= len(image_metas):
                        raise ValueError(f"æ·±åº¦ç¼–ç å™¨ç›¸æœºç´¢å¼• {camera_idx} è¶…å‡ºèŒƒå›´")
                    
                    images, image_mask = image_metas[camera_idx]
                    image, valid = images[frame_idx], image_mask[frame_idx]
                    
                    if not valid or math.prod(image.shape) <= 0:
                        raise ValueError(f"æ·±åº¦ç¼–ç å™¨å›¾åƒæ— æ•ˆ")
                    
                    # DepthAnythingV2é¢„å¤„ç†
                    pil_image = Image.fromarray(image)
                    pil_image = pil_image.resize((self.depth_image_size, self.depth_image_size), Image.BILINEAR)
                    image_array = np.array(pil_image).astype(np.float32) / 255.0
                    image_tensor = torch.from_numpy(image_array).permute(2, 0, 1)
                    
                    # ImageNetæ ‡å‡†åŒ–
                    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                    image_tensor = (image_tensor - mean) / std
                    
                    data_dict["depth_images"] = image_tensor.unsqueeze(0)

                # å¤„ç†åŸå§‹å›¾åƒï¼ˆSigLIPç¼–ç ç”¨ï¼‰
                preprocessed_images = []
                processor = self.image_processor
                for image, valid in rearranged_images:
                    image = Image.fromarray(image)
                    if self.image_size is not None:
                        image = transforms.Resize(self.image_size)(image)

                    if valid and self.auto_adjust_image_brightness:
                        pixel_values = list(image.getdata())
                        average_brightness = sum(sum(pixel) for pixel in pixel_values) / (len(pixel_values) * 255.0 * 3)
                        if average_brightness <= 0.15:
                            image = transforms.ColorJitter(brightness=(1.75, 1.75))(image)

                    # å›¾åƒå¢å¼º
                    if valid and self.image_aug and (random.random() > 0.5):
                        aug_type = random.choice(["corrput_only", "color_only", "both"])
                        if aug_type != "corrput_only":
                            image = transforms.ColorJitter(brightness=0.3, contrast=0.4, saturation=0.5,
                                                           hue=0.03)(image)
                        if aug_type != "color_only":
                            image = image_corrupt(image)

                    if self.image_aspect_ratio == "pad":
                        def expand2square(pil_img, background_color):
                            width, height = pil_img.size
                            if width == height:
                                return pil_img
                            elif width > height:
                                result = Image.new(pil_img.mode, (width, width), background_color)
                                result.paste(pil_img, (0, (width - height) // 2))
                                return result
                            else:
                                result = Image.new(pil_img.mode, (height, height), background_color)
                                result.paste(pil_img, ((height - width) // 2, 0))
                                return result

                        image = expand2square(image, tuple(int(x * 255) for x in processor.image_mean))
                    image = processor.preprocess(image, return_tensors="pt")["pixel_values"][0]
                    preprocessed_images.append(image)
                data_dict["images"] = preprocessed_images

                # å¤„ç†è¯­è¨€è¾“å…¥
                if self.use_precomp_lang_embed:
                    if content["instruction"][-1] == ".":
                        content["instruction"] = content["instruction"][:-1]
                    data_dict["lang_embed"] = (torch.load(content["instruction"])
                                               if random.random() > self.cond_mask_prob else self.empty_lang_embed)
                else:
                    instruction = (content["instruction"] if random.random() > self.cond_mask_prob else "")
                    data_dict["input_ids"] = self.tokenizer(
                        instruction,
                        return_tensors="pt",
                        padding="longest",
                        truncation=False,
                    ).input_ids[0]

                    assert (
                        len(data_dict["input_ids"]) <= self.tokenizer_max_length
                    ), f"Instruction length {len(data_dict['input_ids'])} exceeds the maximum length {self.tokenizer_max_length}."

                # è½¬æ¢numpyæ•°ç»„ä¸ºtensor
                for k, v in data_dict.items():
                    if isinstance(v, np.ndarray):
                        data_dict[k] = torch.from_numpy(v)

                # ğŸ”§ æœ€ç»ˆæ£€æŸ¥æ‰€æœ‰tensor
                for k, v in data_dict.items():
                    assert not isinstance(v, np.ndarray), f"key: {k}, value: {v}"

                # ğŸ”§ æœ€ç»ˆéªŒè¯critical_labelsï¼ˆå†æ¬¡æ£€æŸ¥ï¼‰
                if "critical_labels" in data_dict:
                    labels = data_dict["critical_labels"]
                    if not isinstance(labels, torch.Tensor):
                        print(f"âš ï¸ critical_labelsä¸æ˜¯tensor: {type(labels)}")
                        data_dict["critical_labels"] = torch.tensor(labels, dtype=torch.long)
                    elif labels.dtype != torch.long:
                        print(f"âš ï¸ critical_labelsç±»å‹é”™è¯¯: {labels.dtype}")
                        data_dict["critical_labels"] = labels.long()

                return data_dict
                
            except BaseException as e:
                if data_dict is not None:
                    print(f"Error catched when processing sample from {data_dict.get('dataset_name')}: {e}")
                else:
                    print(f"Error catched when processing sample: {e}")
                traceback.print_exc()
                index = (index + 1) % len(self)

# ä¿®å¤ train/dataset.py ä¸­çš„ DataCollatorForVLAConsumerDataset éƒ¨åˆ†

class DataCollatorForVLAConsumerDataset(object):
    """Collate examples for supervised training.
    ğŸ†• æ”¯æŒå…³é”®æ—¶é—´æ®µæ ‡ç­¾çš„æ•°æ®æ”¶é›†
    ğŸ”§ ä¿®å¤ç‰ˆæœ¬ï¼šå¢å¼ºé”™è¯¯å¤„ç†å’Œå½¢çŠ¶éªŒè¯
    """

    def __init__(self, tokenizer: transformers.PreTrainedTokenizer) -> None:
        self.tokenizer = tokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        batch = {
            "states": [],
            "actions": [],
            "state_elem_mask": [],
            "state_norm": [],
            "images": [],
            "data_indices": [],
            "ctrl_freqs": [],
        }
        
        # è§†è§‰ç‰¹å¾æ”¶é›†
        dinov2_images = []
        depth_images = []
        
        # # ğŸ†• å…³é”®æ—¶é—´æ®µæ ‡ç­¾æ”¶é›† - ä¿®å¤ç‰ˆæœ¬
        # critical_labels = []
        
        # è¯­è¨€ç‰¹å¾æ”¶é›†
        input_ids = []
        lang_embeds = []
        lang_embed_lens = []

        for idx, instance in enumerate(instances):
            try:
                # æ”¶é›†åŸºç¡€æ•°æ®
                keys_to_check = ["states", "actions", "state_elem_mask", "state_norm"]
                for key in keys_to_check:
                    if isinstance(instance[key], torch.Tensor):
                        item = instance[key]
                    else:
                        item = torch.from_numpy(instance[key])
                    batch[key].append(item)

                # æ”¶é›†è¯­è¨€æ•°æ®
                if "input_ids" in instance:
                    input_ids.append(instance["input_ids"])
                else:
                    lang_embeds.append(instance["lang_embed"])
                    lang_embed_lens.append(instance["lang_embed"].shape[0])

                # æ”¶é›†å›¾åƒå’Œå…¶ä»–æ•°æ®
                batch["images"].append(torch.stack(instance["images"], dim=0))
                batch["data_indices"].append(instance["data_idx"])
                batch["ctrl_freqs"].append(instance["ctrl_freq"])
                
                # æ”¶é›†DINOv2å›¾åƒ
                if "dinov2_images" in instance:
                    dinov2_images.append(instance["dinov2_images"])
                
                # æ”¶é›†DepthAnythingV2å›¾åƒ
                if "depth_images" in instance:
                    depth_images.append(instance["depth_images"])
                
                # # ğŸ†• æ”¶é›†å…³é”®æ—¶é—´æ®µæ ‡ç­¾ - ä¿®å¤ç‰ˆæœ¬
                # if "critical_labels" in instance:
                #     labels = instance["critical_labels"]
                    
                #     # ğŸ”§ éªŒè¯å’Œè½¬æ¢æ ‡ç­¾
                #     if not isinstance(labels, torch.Tensor):
                #         labels = torch.tensor(labels, dtype=torch.long)
                #     elif labels.dtype != torch.long:
                #         labels = labels.long()
                    
                #     # ğŸ”§ éªŒè¯æ ‡ç­¾å€¼èŒƒå›´
                #     if torch.any(labels < 0) or torch.any(labels > 1):
                #         print(f"âš ï¸ å®ä¾‹ {idx}: æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´ [0,1]: {labels.unique()}")
                #         labels = torch.clamp(labels, 0, 1)
                    
                #     # ğŸ”§ éªŒè¯å½¢çŠ¶
                #     if labels.dim() != 1:
                #         print(f"âš ï¸ å®ä¾‹ {idx}: æ ‡ç­¾ç»´åº¦é”™è¯¯: {labels.shape}ï¼Œåº”è¯¥æ˜¯1D")
                #         if labels.numel() > 0:
                #             labels = labels.flatten()
                #         else:
                #             # å¦‚æœä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æ ‡ç­¾
                #             action_len = instance["actions"].shape[0]
                #             labels = torch.zeros(action_len, dtype=torch.long)
                    
                #     critical_labels.append(labels)
                # else:
                #     # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œåˆ›å»ºé»˜è®¤æ ‡ç­¾
                #     action_len = instance["actions"].shape[0]
                #     default_labels = torch.zeros(action_len, dtype=torch.long)
                #     # ç®€å•å¯å‘å¼ï¼šä¸­é—´éƒ¨åˆ†ä¸ºå…³é”®æ—¶é—´æ®µ
                #     start_critical = action_len // 4
                #     end_critical = 3 * action_len // 4
                #     default_labels[start_critical:end_critical] = 1
                #     critical_labels.append(default_labels)
                    
            except Exception as e:
                print(f"âŒ å¤„ç†å®ä¾‹ {idx} æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                
                # ğŸ”§ åˆ›å»ºé™çº§æ•°æ®
                action_len = 64  # é»˜è®¤é•¿åº¦
                if "actions" in instance:
                    try:
                        action_len = instance["actions"].shape[0]
                    except:
                        pass
                
                # æ·»åŠ é»˜è®¤çš„critical_labels
                default_labels = torch.zeros(action_len, dtype=torch.long)
                default_labels[action_len//3:2*action_len//3] = 1  # ä¸­é—´1/3ä¸ºå…³é”®
                critical_labels.append(default_labels)
                
                # ç»§ç»­å¤„ç†å…¶ä»–å¿…è¦å­—æ®µ...
                continue

        # ğŸ”§ éªŒè¯æ‰€æœ‰å®ä¾‹éƒ½æœ‰æ ‡ç­¾
        # if len(critical_labels)  # ğŸ”´ è§†è§‰èåˆæ¨¡å¼ä¸éœ€è¦ != len(instances):
#            print(f"âš ï¸ æ ‡ç­¾æ•°é‡ä¸åŒ¹é…: {len(critical_labels)} vs {len(instances)}")
            # è¡¥å……ç¼ºå¤±çš„æ ‡ç­¾
#            while len(critical_labels) < len(instances):
#                default_labels = torch.zeros(64, dtype=torch.long)
#                critical_labels.append(default_labels)

        # å †å åŸºç¡€æ•°æ®
        keys_to_stack = ["states", "actions", "state_elem_mask", "state_norm", "images"]
        for key in keys_to_stack:
            batch[key] = torch.stack(batch[key], dim=0)

        batch["ctrl_freqs"] = torch.tensor(batch["ctrl_freqs"])

        # å¤„ç†è¯­è¨€æ•°æ®
        if len(input_ids) > 0:
            input_ids = torch.nn.utils.rnn.pad_sequence(input_ids,
                                                        batch_first=True,
                                                        padding_value=self.tokenizer.pad_token_id)
            batch["input_ids"] = input_ids
            batch["lang_attn_mask"] = input_ids.ne(self.tokenizer.pad_token_id)
        else:
            lang_embeds = torch.nn.utils.rnn.pad_sequence(lang_embeds, batch_first=True, padding_value=0)
            input_lang_attn_mask = torch.zeros(lang_embeds.shape[0], lang_embeds.shape[1], dtype=torch.bool)
            for i, l in enumerate(lang_embed_lens):
                input_lang_attn_mask[i, :l] = True
            batch["lang_embeds"] = lang_embeds
            batch["lang_attn_mask"] = input_lang_attn_mask
        
        # å †å DINOv2å›¾åƒ
        if len(dinov2_images) > 0:
            batch["dinov2_images"] = torch.stack(dinov2_images, dim=0)
        
        # å †å DepthAnythingV2å›¾åƒ
        if len(depth_images) > 0:
            batch["depth_images"] = torch.stack(depth_images, dim=0)
        
        # # ğŸ†• å †å å…³é”®æ—¶é—´æ®µæ ‡ç­¾ - ä¿®å¤ç‰ˆæœ¬
        # if len(critical_labels) > 0:
        #     try:
        #         # ğŸ”§ æ‰¾åˆ°æœ€å¤§é•¿åº¦è¿›è¡Œå¡«å……
        #         max_len = max(labels.shape[0] for labels in critical_labels)
                
        #         # ğŸ”§ éªŒè¯æ‰€æœ‰æ ‡ç­¾éƒ½æ˜¯1D
        #         validated_labels = []
        #         for i, labels in enumerate(critical_labels):
        #             if labels.dim() != 1:
        #                 print(f"âš ï¸ æ ‡ç­¾ {i} ç»´åº¦é”™è¯¯: {labels.shape}")
        #                 labels = labels.flatten()
                    
        #             # ğŸ”§ å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦
        #             if labels.shape[0] < max_len:
        #                 padding_len = max_len - labels.shape[0]
        #                 # ä½¿ç”¨0å¡«å……ï¼ˆéå…³é”®æ—¶é—´æ®µï¼‰
        #                 padding = torch.zeros(padding_len, dtype=labels.dtype)
        #                 labels = torch.cat([labels, padding], dim=0)
        #             elif labels.shape[0] > max_len:
        #                 # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        #                 labels = labels[:max_len]
                    
        #             validated_labels.append(labels)
                
        #         # ğŸ”§ å †å æ ‡ç­¾
        #         batch["critical_labels"] = torch.stack(validated_labels, dim=0)
                
        #         # ğŸ”§ æœ€ç»ˆéªŒè¯
        #         final_shape = batch["critical_labels"].shape
        #         expected_shape = (len(instances), max_len)
                
        #         if final_shape != expected_shape:
        #             print(f"âš ï¸ æœ€ç»ˆæ ‡ç­¾å½¢çŠ¶ä¸åŒ¹é…: {final_shape} vs æœŸæœ›çš„ {expected_shape}")
        #             # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„é»˜è®¤æ ‡ç­¾
        #             batch["critical_labels"] = torch.zeros(len(instances), max_len, dtype=torch.long)
        #             # ä½¿ç”¨ç®€å•æ¨¡å¼å¡«å……
        #             for i in range(len(instances)):
        #                 # ä¸­é—´40%ä¸ºå…³é”®æ—¶é—´æ®µ
        #                 start_idx = int(max_len * 0.3)
        #                 end_idx = int(max_len * 0.7)
        #                 batch["critical_labels"][i, start_idx:end_idx] = 1
                
        #         # ğŸ”§ éªŒè¯æ•°æ®ç±»å‹å’Œå€¼èŒƒå›´
        #         if batch["critical_labels"].dtype != torch.long:
        #             batch["critical_labels"] = batch["critical_labels"].long()
                
        #         # éªŒè¯å€¼åœ¨[0,1]èŒƒå›´å†…
        #         if torch.any(batch["critical_labels"] < 0) or torch.any(batch["critical_labels"] > 1):
        #             print(f"âš ï¸ æ‰¹æ¬¡ä¸­æœ‰æ ‡ç­¾å€¼è¶…å‡ºèŒƒå›´: {batch['critical_labels'].unique()}")
        #             batch["critical_labels"] = torch.clamp(batch["critical_labels"], 0, 1)
        #     except Exception as e:
        #         print(f"âŒ å¤„ç†å…³é”®æ—¶é—´æ®µæ ‡ç­¾æ—¶å‡ºé”™: {e}")
        #         import traceback
        #         traceback.print_exc()
                
        #         # ğŸ”§ åˆ›å»ºå®‰å…¨çš„é»˜è®¤æ ‡ç­¾
        #         print("ğŸ”§ ä½¿ç”¨å®‰å…¨çš„é»˜è®¤æ ‡ç­¾")
        #         batch_size = len(instances)
        #         default_seq_len = 64  # é»˜è®¤åºåˆ—é•¿åº¦
                
        #         # å°è¯•ä»actionsè·å–çœŸå®é•¿åº¦
        #         if "actions" in batch and len(batch["actions"]) > 0:
        #             try:
        #                 default_seq_len = batch["actions"].shape[1]
        #             except:
        #                 pass
                
        #         # åˆ›å»ºé»˜è®¤æ ‡ç­¾å¼ é‡
        #         batch["critical_labels"] = torch.zeros(batch_size, default_seq_len, dtype=torch.long)
                
        #         # ä¸ºæ¯ä¸ªåºåˆ—è®¾ç½®ç®€å•çš„å…³é”®æ—¶é—´æ®µæ¨¡å¼
        #         for i in range(batch_size):
        #             # ç­–ç•¥ï¼šå¼€å§‹25%å’Œç»“æŸ25%ä¸ºå…³é”®æ—¶é—´æ®µ
        #             quarter_len = default_seq_len // 4
        #             batch["critical_labels"][i, :quarter_len] = 1          # å¼€å§‹é˜¶æ®µ
        #             batch["critical_labels"][i, -quarter_len:] = 1         # ç»“æŸé˜¶æ®µ
        
        # else:
        #     # ğŸ”§ å¦‚æœå®Œå…¨æ²¡æœ‰æ ‡ç­¾ï¼Œåˆ›å»ºé»˜è®¤æ‰¹æ¬¡
        #     print("âš ï¸ æ²¡æœ‰ä»»ä½•å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼Œåˆ›å»ºé»˜è®¤æ‰¹æ¬¡")
        #     batch_size = len(instances)
        #     default_seq_len = 64
            
        #     if "actions" in batch and len(batch["actions"]) > 0:
        #         try:
        #             default_seq_len = batch["actions"].shape[1]
        #         except:
        #             pass
            
        #     batch["critical_labels"] = torch.zeros(batch_size, default_seq_len, dtype=torch.long)
        #     # ç®€å•ç­–ç•¥ï¼šä¸­é—´50%ä¸ºå…³é”®æ—¶é—´æ®µ
        #     start_critical = default_seq_len // 4
        #     end_critical = 3 * default_seq_len // 4
        #     batch["critical_labels"][:, start_critical:end_critical] = 1

        return batch