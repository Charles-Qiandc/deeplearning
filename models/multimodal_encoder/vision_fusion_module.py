# models/multimodal_encoder/vision_fusion_module.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple


class CrossAttentionFusion(nn.Module):
    """
    äº¤å‰æ³¨æ„åŠ›èåˆæ¨¡å—
    
    å…³é”®ä¿®æ­£ï¼š
    - SigLIPå¤„ç†å¤šå¼ å›¾ç‰‡(img_history_size Ã— num_cameras)ï¼Œäº§ç”Ÿå¤§é‡tokens
    - DINOv2å’ŒDepthåªå¤„ç†å½“å‰è§‚æµ‹(1å¼ å›¾ç‰‡)ï¼Œäº§ç”Ÿ1369ä¸ªpatch tokens
    - éœ€è¦å°†å½“å‰è§‚æµ‹çš„DINOv2/Depthç‰¹å¾èåˆåˆ°å¯¹åº”çš„SigLIP tokensä¸­
    """
    
    def __init__(
        self,
        siglip_dim: int = 1152,
        dinov2_dim: int = 1024,
        depth_dim: int = 1024,
        num_heads: int = 8,
        dropout: float = 0.1,
        use_layer_norm: bool = True,
        # ğŸ†• æ–°å¢å‚æ•°ï¼šå›¾åƒå†å²å¤§å°å’Œç›¸æœºæ•°é‡
        img_history_size: int = 2,
        num_cameras: int = 3,
    ):
        """
        Args:
            siglip_dim: SigLIPç‰¹å¾ç»´åº¦ï¼ˆæŸ¥è¯¢ï¼‰
            dinov2_dim: DINOv2ç‰¹å¾ç»´åº¦ï¼ˆé”®/å€¼ï¼‰
            depth_dim: DepthAnythingV2ç‰¹å¾ç»´åº¦ï¼ˆé”®/å€¼ï¼‰
            num_heads: æ³¨æ„åŠ›å¤´æ•°
            dropout: Dropoutç‡
            use_layer_norm: æ˜¯å¦ä½¿ç”¨LayerNorm
            img_history_size: å›¾åƒå†å²é•¿åº¦ï¼ˆä¾‹å¦‚2è¡¨ç¤ºå½“å‰+1å¸§å†å²ï¼‰
            num_cameras: ç›¸æœºæ•°é‡ï¼ˆä¾‹å¦‚3è¡¨ç¤º3ä¸ªè§†è§’ï¼‰
        """
        super().__init__()
        
        self.siglip_dim = siglip_dim
        self.dinov2_dim = dinov2_dim
        self.depth_dim = depth_dim
        self.num_heads = num_heads
        self.img_history_size = img_history_size
        self.num_cameras = num_cameras
        
        print(f"ğŸ”§ CrossAttentionFusionåˆå§‹åŒ–:")
        print(f"   - SigLIPç»´åº¦: {siglip_dim}")
        print(f"   - å›¾åƒå†å²: {img_history_size}å¸§")
        print(f"   - ç›¸æœºæ•°é‡: {num_cameras}ä¸ª")
        print(f"   - SigLIPæ€»tokens: {img_history_size * num_cameras}å¼ å›¾ Ã— 729 patches/å›¾")
        print(f"   - DINOv2/Depth: 1å¼ å½“å‰è§‚æµ‹ Ã— 1369 patches")
        
        # æŠ•å½±å±‚ï¼šå°†DINOv2å’ŒDepthç‰¹å¾æŠ•å½±åˆ°SigLIPç©ºé—´
        self.dinov2_proj = nn.Sequential(
            nn.Linear(dinov2_dim, siglip_dim),
            nn.LayerNorm(siglip_dim) if use_layer_norm else nn.Identity(),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        self.depth_proj = nn.Sequential(
            nn.Linear(depth_dim, siglip_dim),
            nn.LayerNorm(siglip_dim) if use_layer_norm else nn.Identity(),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        # äº¤å‰æ³¨æ„åŠ›å±‚1: SigLIP (Q) Ã— DINOv2 (K, V)
        self.cross_attn_dinov2 = nn.MultiheadAttention(
            embed_dim=siglip_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # äº¤å‰æ³¨æ„åŠ›å±‚2: SigLIP (Q) Ã— Depth (K, V)
        self.cross_attn_depth = nn.MultiheadAttention(
            embed_dim=siglip_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        
        # ç‰¹å¾èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(siglip_dim * 3, siglip_dim * 2),
            nn.LayerNorm(siglip_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(siglip_dim * 2, siglip_dim),
            nn.LayerNorm(siglip_dim),
        )
        
        # é—¨æ§æœºåˆ¶ï¼ˆç”¨äºæ®‹å·®è¿æ¥ï¼‰
        self.gate = nn.Sequential(
            nn.Linear(siglip_dim, 1),
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LayerNorm):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(
        self,
        siglip_tokens: torch.Tensor,
        dinov2_tokens: torch.Tensor,
        depth_tokens: torch.Tensor,
        current_frame_idx: int = -1,  # ğŸ†• å½“å‰å¸§åœ¨åºåˆ—ä¸­çš„ä½ç½®
        return_attention_weights: bool = False
    ) -> Tuple[torch.Tensor, Optional[dict]]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            siglip_tokens: (B, N_total, 1152) 
                          N_total = img_history_size Ã— num_cameras Ã— 729
                          ä¾‹å¦‚: 2å¸§ Ã— 3ç›¸æœº Ã— 729patches = 4374 tokens
            dinov2_tokens: (B, 1369, 1024) DINOv2å½“å‰è§‚æµ‹çš„patch tokens
            depth_tokens: (B, 1369, 1024) Depthå½“å‰è§‚æµ‹çš„patch tokens
            current_frame_idx: å½“å‰å¸§çš„ç´¢å¼•ï¼ˆ-1è¡¨ç¤ºæœ€åä¸€å¸§ï¼Œ-2è¡¨ç¤ºå€’æ•°ç¬¬äºŒå¸§ï¼‰
            return_attention_weights: æ˜¯å¦è¿”å›æ³¨æ„åŠ›æƒé‡
            
        Returns:
            fused_tokens: (B, N_total, 1152) èåˆåçš„è§†è§‰tokens
            attention_info: å¯é€‰çš„æ³¨æ„åŠ›æƒé‡ä¿¡æ¯
        """
        B, N_total, D_siglip = siglip_tokens.shape
        
        # ğŸ” è®¡ç®—æ¯å¼ å›¾ç‰‡çš„tokenæ•°é‡
        tokens_per_image = 729  # SigLIP: 27Ã—27
        total_images = self.img_history_size * self.num_cameras
        
        # éªŒè¯è¾“å…¥å½¢çŠ¶
        expected_N_total = total_images * tokens_per_image
        if N_total != expected_N_total:
            print(f"âš ï¸ SigLIP tokensæ•°é‡ä¸åŒ¹é…: æœŸæœ›{expected_N_total}, å®é™…{N_total}")
        
        # 1. æŠ•å½±DINOv2å’ŒDepthç‰¹å¾åˆ°SigLIPç©ºé—´
        dinov2_projected = self.dinov2_proj(dinov2_tokens)  # (B, 1369, 1152)
        depth_projected = self.depth_proj(depth_tokens)      # (B, 1369, 1152)
        
        # 2. ç¡®å®šå½“å‰å¸§å¯¹åº”çš„SigLIP tokensèŒƒå›´
        # å‡è®¾æ’åˆ—é¡ºåº: [frame0_cam0, frame0_cam1, frame0_cam2, frame1_cam0, frame1_cam1, frame1_cam2]
        if current_frame_idx == -1:  # æœ€åä¸€å¸§ï¼ˆå½“å‰è§‚æµ‹ï¼‰
            current_frame_actual_idx = self.img_history_size - 1
        elif current_frame_idx == -2:
            current_frame_actual_idx = self.img_history_size - 2
        else:
            current_frame_actual_idx = current_frame_idx
        
        # è®¡ç®—å½“å‰å¸§çš„tokenèµ·æ­¢ä½ç½®ï¼ˆæ‰€æœ‰ç›¸æœºï¼‰
        start_idx = current_frame_actual_idx * self.num_cameras * tokens_per_image
        end_idx = (current_frame_actual_idx + 1) * self.num_cameras * tokens_per_image
        
        # æå–å½“å‰å¸§çš„SigLIP tokens
        current_frame_tokens = siglip_tokens[:, start_idx:end_idx, :]  # (B, num_cameras*729, 1152)
        
        # 3. äº¤å‰æ³¨æ„åŠ›èåˆ - åªå¯¹å½“å‰å¸§çš„tokensè¿›è¡Œèåˆ
        # èåˆDINOv2ç‰¹å¾
        attn_dinov2_output, attn_dinov2_weights = self.cross_attn_dinov2(
            query=current_frame_tokens,
            key=dinov2_projected,
            value=dinov2_projected,
            need_weights=return_attention_weights
        )  # (B, num_cameras*729, 1152)
        
        # èåˆDepthç‰¹å¾
        attn_depth_output, attn_depth_weights = self.cross_attn_depth(
            query=current_frame_tokens,
            key=depth_projected,
            value=depth_projected,
            need_weights=return_attention_weights
        )  # (B, num_cameras*729, 1152)
        
        # 4. ä¸‰è·¯ç‰¹å¾æ‹¼æ¥ï¼ˆåªå¯¹å½“å‰å¸§ï¼‰
        concatenated = torch.cat([
            current_frame_tokens,  # åŸå§‹SigLIPç‰¹å¾
            attn_dinov2_output,    # DINOv2å¢å¼ºç‰¹å¾
            attn_depth_output      # Depthå¢å¼ºç‰¹å¾
        ], dim=-1)  # (B, num_cameras*729, 1152*3)
        
        # 5. ç‰¹å¾èåˆ
        fused_current_frame = self.fusion_layer(concatenated)  # (B, num_cameras*729, 1152)
        
        # 6. é—¨æ§æ®‹å·®è¿æ¥
        gate_values = self.gate(fused_current_frame)  # (B, num_cameras*729, 1)
        fused_current_frame = (gate_values * fused_current_frame + 
                              (1 - gate_values) * current_frame_tokens)
        
        # 7. å°†èåˆåçš„å½“å‰å¸§tokensæ›¿æ¢å›åŸå§‹åºåˆ—
        fused_tokens = siglip_tokens.clone()
        fused_tokens[:, start_idx:end_idx, :] = fused_current_frame
        
        # 8. è¿”å›ç»“æœ
        if return_attention_weights:
            attention_info = {
                'dinov2_weights': attn_dinov2_weights,
                'depth_weights': attn_depth_weights,
                'gate_values': gate_values,
                'fused_frame_range': (start_idx, end_idx)
            }
            return fused_tokens, attention_info
        else:
            return fused_tokens, None


class SimpleFusionModule(nn.Module):
    """
    ç®€åŒ–ç‰ˆèåˆæ¨¡å—
    å¦‚æœäº¤å‰æ³¨æ„åŠ›å¤ªé‡ï¼Œä½¿ç”¨è¿™ä¸ªæ›´è½»é‡çš„ç‰ˆæœ¬
    """
    
    def __init__(
        self,
        siglip_dim: int = 1152,
        dinov2_dim: int = 1024,
        depth_dim: int = 1024,
        dropout: float = 0.1,
        img_history_size: int = 2,
        num_cameras: int = 3,
    ):
        super().__init__()
        
        self.siglip_dim = siglip_dim
        self.img_history_size = img_history_size
        self.num_cameras = num_cameras
        
        # æŠ•å½±å±‚
        self.dinov2_proj = nn.Linear(dinov2_dim, siglip_dim)
        self.depth_proj = nn.Linear(depth_dim, siglip_dim)
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(siglip_dim * 3, siglip_dim),
            nn.LayerNorm(siglip_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        )
        
        self._init_weights()
    
    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(
        self,
        siglip_tokens: torch.Tensor,
        dinov2_tokens: torch.Tensor,
        depth_tokens: torch.Tensor,
        current_frame_idx: int = -1
    ) -> torch.Tensor:
        """
        ç®€å•èåˆï¼šæŠ•å½± -> å¹³å‡æ± åŒ–å¯¹é½ -> æ‹¼æ¥ -> MLP
        """
        B, N_total, _ = siglip_tokens.shape
        
        # æŠ•å½±åˆ°ç»Ÿä¸€ç©ºé—´
        dinov2_proj = self.dinov2_proj(dinov2_tokens)  # (B, 1369, 1152)
        depth_proj = self.depth_proj(depth_tokens)      # (B, 1369, 1152)
        
        # ç¡®å®šå½“å‰å¸§èŒƒå›´
        tokens_per_image = 729
        if current_frame_idx == -1:
            current_frame_actual_idx = self.img_history_size - 1
        elif current_frame_idx == -2:
            current_frame_actual_idx = self.img_history_size - 2
        else:
            current_frame_actual_idx = current_frame_idx
        
        start_idx = current_frame_actual_idx * self.num_cameras * tokens_per_image
        end_idx = (current_frame_actual_idx + 1) * self.num_cameras * tokens_per_image
        
        current_frame_tokens = siglip_tokens[:, start_idx:end_idx, :]
        N_current = current_frame_tokens.shape[1]  # num_cameras * 729
        
        # é€šè¿‡è‡ªé€‚åº”æ± åŒ–è°ƒæ•´DINOv2/Depthçš„tokenæ•°é‡
        # (B, 1369, 1152) -> (B, 1152, 1369) -> pool -> (B, 1152, N_current) -> (B, N_current, 1152)
        dinov2_aligned = F.adaptive_avg_pool1d(
            dinov2_proj.transpose(1, 2), N_current
        ).transpose(1, 2)
        
        depth_aligned = F.adaptive_avg_pool1d(
            depth_proj.transpose(1, 2), N_current
        ).transpose(1, 2)
        
        # æ‹¼æ¥èåˆ
        concatenated = torch.cat([current_frame_tokens, dinov2_aligned, depth_aligned], dim=-1)
        fused_current_frame = self.fusion(concatenated)
        
        # æ›¿æ¢å›åŸå§‹åºåˆ—
        fused_tokens = siglip_tokens.clone()
        fused_tokens[:, start_idx:end_idx, :] = fused_current_frame
        
        return fused_tokens


def create_vision_fusion_module(
    fusion_type: str = "cross_attention",
    siglip_dim: int = 1152,
    dinov2_dim: int = 1024,
    depth_dim: int = 1024,
    img_history_size: int = 2,
    num_cameras: int = 3,
    **kwargs
):
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºè§†è§‰èåˆæ¨¡å—
    
    Args:
        fusion_type: "cross_attention" æˆ– "simple"
        siglip_dim: SigLIPç‰¹å¾ç»´åº¦
        dinov2_dim: DINOv2ç‰¹å¾ç»´åº¦
        depth_dim: Depthç‰¹å¾ç»´åº¦
        img_history_size: å›¾åƒå†å²é•¿åº¦
        num_cameras: ç›¸æœºæ•°é‡
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        èåˆæ¨¡å—å®ä¾‹
    """
    if fusion_type == "cross_attention":
        return CrossAttentionFusion(
            siglip_dim=siglip_dim,
            dinov2_dim=dinov2_dim,
            depth_dim=depth_dim,
            img_history_size=img_history_size,
            num_cameras=num_cameras,
            **kwargs
        )
    elif fusion_type == "simple":
        return SimpleFusionModule(
            siglip_dim=siglip_dim,
            dinov2_dim=dinov2_dim,
            depth_dim=depth_dim,
            img_history_size=img_history_size,
            num_cameras=num_cameras,
            **kwargs
        )
    else:
        raise ValueError(f"Unknown fusion type: {fusion_type}")


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•è§†è§‰ç‰¹å¾èåˆæ¨¡å—ï¼ˆå¤šå›¾ç‰‡åœºæ™¯ï¼‰")
    
    # æ¨¡æ‹Ÿè¾“å…¥
    B = 2
    img_history_size = 2  # 2å¸§å†å²
    num_cameras = 3       # 3ä¸ªç›¸æœº
    N_siglip = img_history_size * num_cameras * 729  # 2Ã—3Ã—729 = 4374 tokens
    N_dinov2 = 1369   # å½“å‰è§‚æµ‹çš„DINOv2 patches
    N_depth = 1369    # å½“å‰è§‚æµ‹çš„Depth patches
    
    siglip_tokens = torch.randn(B, N_siglip, 1152)
    dinov2_tokens = torch.randn(B, N_dinov2, 1024)
    depth_tokens = torch.randn(B, N_depth, 1024)
    
    # æµ‹è¯•äº¤å‰æ³¨æ„åŠ›èåˆ
    print("\n1ï¸âƒ£ æµ‹è¯•äº¤å‰æ³¨æ„åŠ›èåˆæ¨¡å—:")
    fusion_module = CrossAttentionFusion(
        img_history_size=img_history_size,
        num_cameras=num_cameras
    )
    fused_tokens, attn_info = fusion_module(
        siglip_tokens, 
        dinov2_tokens, 
        depth_tokens,
        current_frame_idx=-1,  # èåˆæœ€åä¸€å¸§ï¼ˆå½“å‰è§‚æµ‹ï¼‰
        return_attention_weights=True
    )
    print(f"   è¾“å…¥: SigLIP {siglip_tokens.shape}, DINOv2 {dinov2_tokens.shape}, Depth {depth_tokens.shape}")
    print(f"   è¾“å‡º: {fused_tokens.shape} (åº”è¯¥å’ŒSigLIPè¾“å…¥ç›¸åŒ)")
    print(f"   èåˆå¸§èŒƒå›´: {attn_info['fused_frame_range']}")
    
    # æµ‹è¯•ç®€åŒ–èåˆ
    print("\n2ï¸âƒ£ æµ‹è¯•ç®€åŒ–èåˆæ¨¡å—:")
    simple_fusion = SimpleFusionModule(
        img_history_size=img_history_size,
        num_cameras=num_cameras
    )
    fused_simple = simple_fusion(siglip_tokens, dinov2_tokens, depth_tokens, current_frame_idx=-1)
    print(f"   è¾“å‡º: {fused_simple.shape}")
    
    # å‚æ•°ç»Ÿè®¡
    print("\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
    cross_attn_params = sum(p.numel() for p in fusion_module.parameters())
    simple_params = sum(p.numel() for p in simple_fusion.parameters())
    print(f"   äº¤å‰æ³¨æ„åŠ›æ¨¡å—: {cross_attn_params:,} å‚æ•°")
    print(f"   ç®€åŒ–æ¨¡å—: {simple_params:,} å‚æ•°")
    
    print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")