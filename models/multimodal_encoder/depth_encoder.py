import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoImageProcessor, AutoModelForDepthEstimation
import numpy as np


class DepthAnythingV2Encoder(nn.Module):
    """
    DepthAnythingV2ç¼–ç å™¨ï¼Œæä¾›æ·±åº¦æ„ŸçŸ¥çš„å‡ ä½•ç‰¹å¾
    ğŸ†• ç¡®ä¿è¿”å›åŒ…å«CLS tokençš„ç‰¹å¾ç”¨äºåŒæ•™å¸ˆå¯¹é½
    æ”¯æŒMetricç‰ˆæœ¬æ¨¡å‹
    """
    
    def __init__(self, model_size="vits", feature_dim=1024, device=None, use_metric_model=False):
        """
        åˆå§‹åŒ–DepthAnythingV2ç¼–ç å™¨
        
        Args:
            model_size: æ¨¡å‹å¤§å° ("vits", "vitb", "vitl", "metric_large")
            feature_dim: è¾“å‡ºç‰¹å¾ç»´åº¦ï¼Œéœ€è¦ä¸DINOv2å¯¹é½ (1024)
            device: è®¡ç®—è®¾å¤‡
            use_metric_model: æ˜¯å¦ä½¿ç”¨Metricç‰ˆæœ¬æ¨¡å‹
        """
        super().__init__()
        
        self.model_size = model_size
        self.feature_dim = feature_dim
        self._target_device = device if device else torch.device('cpu')
        self.use_metric_model = use_metric_model
        
        # ğŸ†• æ‰©å±•æ¨¡å‹åç§°æ˜ å°„ï¼ŒåŒ…å«Metricç‰ˆæœ¬
        if use_metric_model:
            model_mapping = {
                "vits": "depth-anything/Depth-Anything-V2-Metric-Indoor-Small-hf",
                "vitb": "depth-anything/Depth-Anything-V2-Metric-Indoor-Base-hf", 
                "vitl": "depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf",
                "metric_large": "depth-anything/Depth-Anything-V2-Metric-Indoor-Large-hf",
            }
            print(f"ğŸ¯ ä½¿ç”¨Metricç‰ˆæœ¬æ·±åº¦æ¨¡å‹")
        else:
            # åŸç‰ˆéMetricæ¨¡å‹
            model_mapping = {
                "vits": "depth-anything/Depth-Anything-V2-Small-hf",
                "vitb": "depth-anything/Depth-Anything-V2-Base-hf", 
                "vitl": "depth-anything/Depth-Anything-V2-Large-hf"
            }
        
        if model_size not in model_mapping:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹å¤§å°: {model_size}")
        
        self.model_name = model_mapping[model_size]
        self.is_loaded = False
        
        # ç«‹å³åŠ è½½æ¨¡å‹
        self.image_processor = None
        self.depth_model = None
        self.projection_head = None
        self.load_model()
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹æƒé‡"""
        if self.is_loaded:
            return
            
        print(f"ğŸ”§ åŠ è½½DepthAnythingV2æ¨¡å‹: {self.model_name}")
        
        try:
            # åŠ è½½é¢„å¤„ç†å™¨å’Œæ¨¡å‹
            print("   - åŠ è½½å›¾åƒé¢„å¤„ç†å™¨...")
            self.image_processor = AutoImageProcessor.from_pretrained(self.model_name)
            
            print("   - åŠ è½½æ·±åº¦ä¼°è®¡æ¨¡å‹...")
            self.depth_model = AutoModelForDepthEstimation.from_pretrained(self.model_name)
            
            # å†»ç»“æ·±åº¦æ¨¡å‹æƒé‡
            print("   - å†»ç»“æ¨¡å‹æƒé‡...")
            self.depth_model.eval()
            for param in self.depth_model.parameters():
                param.requires_grad = False
                
            # è·å–æ·±åº¦æ¨¡å‹çš„éšè—ç»´åº¦
            config = self.depth_model.config
            if hasattr(config, 'hidden_size'):
                depth_hidden_size = config.hidden_size
            elif hasattr(config, 'encoder_hidden_size'):
                depth_hidden_size = config.encoder_hidden_size
            elif hasattr(config, 'backbone_config') and hasattr(config.backbone_config, 'hidden_size'):
                depth_hidden_size = config.backbone_config.hidden_size
            else:
                # æ ¹æ®æ¨¡å‹å¤§å°ä¼°ç®—
                if "large" in self.model_name.lower():
                    depth_hidden_size = 1024
                elif "base" in self.model_name.lower():
                    depth_hidden_size = 768
                else:
                    depth_hidden_size = 384
                print(f"   âš ï¸ æ— æ³•ä»é…ç½®è·å–éšè—ç»´åº¦ï¼Œä½¿ç”¨ä¼°ç®—å€¼: {depth_hidden_size}")
                
            print(f"   - æ·±åº¦æ¨¡å‹éšè—ç»´åº¦: {depth_hidden_size}")
            print(f"   - ç›®æ ‡è¾“å‡ºç»´åº¦: {self.feature_dim}")
            
            # åˆ›å»ºæŠ•å½±å¤´
            print("   - åˆ›å»ºæŠ•å½±å¤´...")
            self.projection_head = nn.Sequential(
                nn.Linear(depth_hidden_size, depth_hidden_size * 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(depth_hidden_size * 2, self.feature_dim),
                nn.LayerNorm(self.feature_dim)
            )
            
            # åˆå§‹åŒ–æŠ•å½±å¤´æƒé‡
            for module in self.projection_head:
                if isinstance(module, nn.Linear):
                    nn.init.xavier_uniform_(module.weight, gain=0.5)
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
                        
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            print(f"   - ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡: {self._target_device}")
            self.depth_model = self.depth_model.to(self._target_device)
            self.projection_head = self.projection_head.to(self._target_device)
            
            self.is_loaded = True
            print(f"âœ… DepthAnythingV2ç¼–ç å™¨åŠ è½½å®Œæˆ")
            
            # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨åœ°æ‰“å°æ¨¡å‹ä¿¡æ¯
            self._safe_print_model_info()
            
        except Exception as e:
            print(f"âŒ DepthAnythingV2ç¼–ç å™¨åŠ è½½å¤±è´¥: {e}")
            print("   å°è¯•é™çº§åˆ°ç®€åŒ–å®ç°...")
            self._create_fallback_model()
            
    def _create_fallback_model(self):
        """åˆ›å»ºç®€åŒ–çš„é™çº§æ¨¡å‹"""
        print("ğŸ”„ åˆ›å»ºç®€åŒ–çš„æ·±åº¦ç‰¹å¾æå–å™¨...")
        
        # åˆ›å»ºç®€å•çš„å·ç§¯ç½‘ç»œä½œä¸ºæ·±åº¦ç‰¹å¾æå–å™¨
        self.depth_model = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            
            nn.AdaptiveAvgPool2d((37, 37)),  # è¾“å‡º37x37ç‰¹å¾å›¾
            nn.Flatten(start_dim=2),  # (B, 256, 37*37)
        )
        
        # åˆ›å»ºæŠ•å½±å¤´
        self.projection_head = nn.Sequential(
            nn.Linear(256, 512),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(512, self.feature_dim),
            nn.LayerNorm(self.feature_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(module, (nn.BatchNorm2d, nn.LayerNorm)):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.5)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
                    
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.depth_model = self.depth_model.to(self._target_device)
        self.projection_head = self.projection_head.to(self._target_device)
        
        self.is_loaded = True
        print("âœ… ç®€åŒ–æ·±åº¦ç¼–ç å™¨åˆ›å»ºå®Œæˆ")
        
        # ğŸ”§ å®‰å…¨åœ°æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._safe_print_model_info()
        
    def extract_intermediate_features(self, images):
        """æå–ä¸­é—´ç‰¹å¾"""
        try:
            if hasattr(self.depth_model, 'backbone'):
                # Transformer backbone
                outputs = self.depth_model.backbone(
                    images, 
                    output_hidden_states=True,
                    return_dict=True
                )
                
                if hasattr(outputs, 'last_hidden_state'):
                    features = outputs.last_hidden_state
                elif hasattr(outputs, 'hidden_states') and outputs.hidden_states:
                    features = outputs.hidden_states[-1]
                else:
                    raise ValueError("æ— æ³•ä»æ·±åº¦æ¨¡å‹æå–ç‰¹å¾")
                
                return features
                
            else:
                # ç®€åŒ–æ¨¡å‹
                return self._extract_fallback_features(images)
                
        except Exception as e:
            print(f"   âš ï¸ ç‰¹å¾æå–å¤±è´¥: {e}ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
            return self._extract_fallback_features(images)
    
    def _extract_fallback_features(self, images):
        """é™çº§ç‰¹å¾æå–"""
        B, C, H, W = images.shape
        
        if hasattr(self.depth_model, 'backbone'):
            # å°è¯•ç›´æ¥ä½¿ç”¨æ·±åº¦æ¨¡å‹
            try:
                outputs = self.depth_model(images)
                # åˆ›å»ºå‡çš„ç‰¹å¾æ ¼å¼
                feature_dim = 384
                
                # åˆ›å»ºCLS token + patch tokensæ ¼å¼
                cls_token = torch.randn(B, 1, feature_dim, device=images.device, dtype=images.dtype)
                patch_tokens = torch.randn(B, 1369, feature_dim, device=images.device, dtype=images.dtype)
                
                features = torch.cat([cls_token, patch_tokens], dim=1)  # (B, 1370, 384)
                return features
                
            except Exception as e:
                print(f"âš ï¸ æ·±åº¦æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        
        # ä½¿ç”¨ç®€åŒ–æ¨¡å‹
        features = self.depth_model(images)  # (B, 256, 37*37)
        B, C, N = features.shape
        
        # è½¬æ¢ä¸ºç±»ä¼¼transformerçš„æ ¼å¼
        features = features.permute(0, 2, 1)  # (B, 37*37, 256)
        
        # æ·»åŠ CLS token
        cls_token = torch.mean(features, dim=1, keepdim=True)  # (B, 1, 256)
        features_with_cls = torch.cat([cls_token, features], dim=1)  # (B, 1+37*37, 256)
        
        return features_with_cls

    @torch.no_grad()
    def forward(self, images):
        """å‰å‘ä¼ æ’­"""
        if not self.is_loaded:
            self.load_model()
            
        B, C, H, W = images.shape
        
        # ç¡®ä¿è¾“å…¥æ˜¯RGBæ ¼å¼ä¸”å°ºå¯¸æ­£ç¡®
        assert C == 3, f"æœŸæœ›3é€šé“RGBå›¾åƒï¼Œå¾—åˆ°{C}é€šé“"
        
        if H != 518 or W != 518:
            images = F.interpolate(
                images, 
                size=(518, 518), 
                mode='bilinear', 
                align_corners=False
            )
        
        try:
            # æå–ä¸­é—´ç‰¹å¾
            intermediate_features = self.extract_intermediate_features(images)
            
            B, N, D = intermediate_features.shape
            
            # é€šè¿‡æŠ•å½±å¤´æ˜ å°„åˆ°ç›®æ ‡ç»´åº¦
            depth_features = self.projection_head(
                intermediate_features.reshape(B * N, D)
            ).reshape(B, N, self.feature_dim)
            
            # å¦‚æœtokenæ•°é‡ä¸æ˜¯1370ï¼Œè°ƒæ•´åˆ°1370
            if N != 1370:
                if N < 1370:
                    # å¦‚æœtokenæ•°é‡ä¸è¶³ï¼Œç”¨é›¶å¡«å……
                    padding = torch.zeros(B, 1370 - N, self.feature_dim, 
                                        device=depth_features.device, 
                                        dtype=depth_features.dtype)
                    depth_features = torch.cat([depth_features, padding], dim=1)
                else:
                    # å¦‚æœtokenæ•°é‡è¿‡å¤šï¼Œæˆªæ–­
                    depth_features = depth_features[:, :1370, :]
            
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾æå–å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç‰¹å¾")
            
            # åˆ›å»ºé»˜è®¤ç‰¹å¾
            depth_features = torch.randn(B, 1370, self.feature_dim, 
                                       device=images.device, 
                                       dtype=images.dtype)
        
        # ç”Ÿæˆæ·±åº¦å›¾ç”¨äºå¯è§†åŒ–
        try:
            if hasattr(self.depth_model, '__call__') and not isinstance(self.depth_model, nn.Sequential):
                outputs = self.depth_model(images)
                if hasattr(outputs, 'predicted_depth'):
                    depth_map = outputs.predicted_depth.unsqueeze(1)  # (B, 1, H, W)
                else:
                    depth_map = torch.zeros(B, 1, H, W, device=images.device, dtype=images.dtype)
            else:
                depth_map = torch.zeros(B, 1, H, W, device=images.device, dtype=images.dtype)
                
            depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min() + 1e-8)
        except:
            depth_map = torch.zeros(B, 1, H, W, device=images.device, dtype=images.dtype)
        
        return depth_features, depth_map
    
    def get_cls_token(self, depth_features):
        """ä»æ·±åº¦ç‰¹å¾ä¸­æå–CLS token"""
        if depth_features.shape[1] >= 1:
            return depth_features[:, 0, :]  # ç¬¬0ä¸ªtokenæ˜¯CLS
        else:
            raise ValueError("æ·±åº¦ç‰¹å¾ä¸­æ²¡æœ‰è¶³å¤Ÿçš„tokens")
    
    def get_patch_tokens(self, depth_features):
        """ä»æ·±åº¦ç‰¹å¾ä¸­æå–patch tokens"""
        if depth_features.shape[1] >= 1370:
            return depth_features[:, 1:, :]  # ç¬¬1-1369ä¸ªtokenæ˜¯patches
        else:
            return depth_features[:, 1:, :]  # è¿”å›æ‰€æœ‰éCLS token

    @property
    def dtype(self):
        """è¿”å›æ¨¡å‹æ•°æ®ç±»å‹"""
        if self.is_loaded and self.depth_model is not None:
            try:
                return next(self.depth_model.parameters()).dtype
            except:
                return torch.float32
        return torch.float32
    
    @property
    def device(self):
        """è¿”å›æ¨¡å‹è®¾å¤‡"""
        if self.is_loaded and self.depth_model is not None:
            try:
                return next(self.depth_model.parameters()).device
            except:
                return self._target_device
        return self._target_device
    
    def _safe_print_model_info(self):
        """ğŸ”§ å®‰å…¨åœ°æ‰“å°æ¨¡å‹ä¿¡æ¯ï¼Œé¿å…deviceå±æ€§é”™è¯¯"""
        if not self.is_loaded:
            print("âŒ æ¨¡å‹æœªåŠ è½½")
            return
            
        print("ğŸ” DepthAnythingV2ç¼–ç å™¨ä¿¡æ¯:")
        print(f"   - æ¨¡å‹åç§°: {self.model_name}")
        print(f"   - æ¨¡å‹å¤§å°: {self.model_size}")
        print(f"   - æ¨¡å‹ç±»å‹: {'Metricç‰ˆæœ¬' if self.use_metric_model else 'æ ‡å‡†ç‰ˆæœ¬'}")
        print(f"   - è¾“å‡ºç‰¹å¾ç»´åº¦: {self.feature_dim}")
        
        # ğŸ”§ å®‰å…¨åœ°è·å–è®¾å¤‡ä¿¡æ¯
        try:
            device_info = self.device
            print(f"   - è®¾å¤‡: {device_info}")
        except:
            print(f"   - è®¾å¤‡: {self._target_device}")
            
        # ğŸ”§ å®‰å…¨åœ°è·å–æ•°æ®ç±»å‹
        try:
            dtype_info = self.dtype
            print(f"   - æ•°æ®ç±»å‹: {dtype_info}")
        except:
            print(f"   - æ•°æ®ç±»å‹: torch.float32")
            
        print(f"   - æœŸæœ›è¾“å‡ºæ ¼å¼: (B, 1370, {self.feature_dim})")
        print(f"     - CLS token: [:, 0, :] -> (B, {self.feature_dim})")
        print(f"     - Patch tokens: [:, 1:, :] -> (B, 1369, {self.feature_dim})")
        
        # ğŸ”§ å®‰å…¨åœ°è®¡ç®—å‚æ•°é‡
        if self.depth_model is not None:
            try:
                depth_params = sum(p.numel() for p in self.depth_model.parameters())
                print(f"   - æ·±åº¦æ¨¡å‹å‚æ•°é‡: {depth_params/1e6:.2f}M")
            except:
                print(f"   - æ·±åº¦æ¨¡å‹å‚æ•°é‡: æ— æ³•è®¡ç®—")
                
        if self.projection_head is not None:
            try:
                proj_params = sum(p.numel() for p in self.projection_head.parameters())
                print(f"   - æŠ•å½±å¤´å‚æ•°é‡: {proj_params/1e6:.2f}M")
            except:
                print(f"   - æŠ•å½±å¤´å‚æ•°é‡: æ— æ³•è®¡ç®—")
    
    def print_model_info(self):
        """å…¬å…±æ¥å£ï¼Œè°ƒç”¨å®‰å…¨ç‰ˆæœ¬"""
        self._safe_print_model_info()


def create_depth_encoder(model_size="vits", feature_dim=1024, device=None, use_metric_model=False):
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºDepthAnythingV2ç¼–ç å™¨
    
    Args:
        model_size: æ¨¡å‹å¤§å° ("vits", "vitb", "vitl", "metric_large")
        feature_dim: è¾“å‡ºç‰¹å¾ç»´åº¦
        device: è®¡ç®—è®¾å¤‡
        use_metric_model: æ˜¯å¦ä½¿ç”¨Metricç‰ˆæœ¬ï¼ˆæ¨èå®¤å†…åœºæ™¯ï¼‰
        
    Returns:
        DepthAnythingV2Encoderå®ä¾‹
    """
    print(f"ğŸ”§ åˆ›å»ºDepthAnythingV2ç¼–ç å™¨")
    print(f"   - æ¨¡å‹å¤§å°: {model_size}")
    print(f"   - ç‰¹å¾ç»´åº¦: {feature_dim}")
    print(f"   - ä½¿ç”¨Metricç‰ˆæœ¬: {use_metric_model}")
    
    encoder = DepthAnythingV2Encoder(
        model_size=model_size,
        feature_dim=feature_dim,
        device=device,
        use_metric_model=use_metric_model
    )
    return encoder


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•DepthAnythingV2ç¼–ç å™¨")
    
    # åˆ›å»ºç¼–ç å™¨
    encoder = create_depth_encoder(model_size="vits")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    print("\nğŸ”¬ æµ‹è¯•å‰å‘ä¼ æ’­:")
    test_input = torch.randn(2, 3, 518, 518)
    depth_features, depth_map = encoder(test_input)
    
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"æ·±åº¦ç‰¹å¾å½¢çŠ¶: {depth_features.shape}")
    print(f"æ·±åº¦å›¾å½¢çŠ¶: {depth_map.shape}")
    
    # æµ‹è¯•CLS tokenæå–
    cls_token = encoder.get_cls_token(depth_features)
    patch_tokens = encoder.get_patch_tokens(depth_features)
    print(f"CLS tokenå½¢çŠ¶: {cls_token.shape}")
    print(f"Patch tokenså½¢çŠ¶: {patch_tokens.shape}")
    
    print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")