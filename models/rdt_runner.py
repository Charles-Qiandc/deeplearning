import re
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler

from models.hub_mixin import CompatiblePyTorchModelHubMixin
from models.rdt.model import RDT


class RDTRunner(nn.Module, CompatiblePyTorchModelHubMixin, 
               repo_url="https://huggingface.co/robotics-diffusion-transformer/rdt-1b"):
    """
    ğŸ”„ é›†æˆåŒæ•™å¸ˆREPAå¯¹é½æŸå¤±çš„RDTè¿è¡Œå™¨
    æ”¯æŒDINOv2ï¼ˆå…¨å±€è¯­ä¹‰ï¼‰å’ŒDepthAnythingV2ï¼ˆæ·±åº¦å‡ ä½•ï¼‰çš„é€‰æ‹©æ€§å¯¹é½
    """
    def __init__(self, *, action_dim, pred_horizon, config, 
                 lang_token_dim, img_token_dim, state_token_dim, 
                 max_lang_cond_len, img_cond_len, lang_pos_embed_config=None, 
                 img_pos_embed_config=None, dtype=torch.bfloat16,
                 # REPAç›¸å…³å‚æ•°ï¼ˆç°æœ‰ï¼‰
                 enable_repa_loss=True, repa_loss_weight=0.2,
                 # ğŸ†• åŒæ•™å¸ˆç›¸å…³å‚æ•°
                 use_dual_teachers=False, routing_loss_weight=0.1):
        super(RDTRunner, self).__init__()
        
        # REPAæŸå¤±é…ç½®ï¼ˆç°æœ‰ï¼‰
        self.enable_repa_loss = enable_repa_loss
        self.repa_loss_weight = repa_loss_weight
        self.dtype = dtype
        
        # ğŸ†• åŒæ•™å¸ˆé…ç½®
        self.use_dual_teachers = use_dual_teachers
        self.routing_loss_weight = routing_loss_weight
        
        print(f"ğŸ”§ RDTRunneråˆå§‹åŒ–:")
        print(f"   - REPAæŸå¤±å¯ç”¨: {enable_repa_loss}")
        print(f"   - REPAæŸå¤±æƒé‡: {repa_loss_weight}")
        print(f"   - åŒæ•™å¸ˆæ¨¡å¼: {use_dual_teachers}")
        print(f"   - è·¯ç”±æŸå¤±æƒé‡: {routing_loss_weight}")
        print(f"   - æ•°æ®ç±»å‹: {dtype}")
        
        # åˆ›å»ºæ‰©æ•£æ¨¡å‹ï¼ˆä¿®æ”¹ä»¥æ”¯æŒåŒæ•™å¸ˆï¼‰
        hidden_size = config['rdt']['hidden_size']
        self.model = RDT(
            output_dim=action_dim,
            horizon=pred_horizon,
            hidden_size=hidden_size,
            depth=config['rdt']['depth'], 
            num_heads=config['rdt']['num_heads'],
            max_lang_cond_len=max_lang_cond_len,
            img_cond_len=img_cond_len,
            lang_pos_embed_config=lang_pos_embed_config,
            img_pos_embed_config=img_pos_embed_config,
            dtype=dtype,
            enable_repa_loss=enable_repa_loss,
            use_dual_teachers=use_dual_teachers,  # ğŸ†•
        )

        # ç°æœ‰çš„é€‚é…å™¨åˆ›å»ºä¿æŒä¸å˜
        self.lang_adaptor = self.build_condition_adapter(
            config['lang_adaptor'], 
            in_features=lang_token_dim, 
            out_features=hidden_size
        )
        self.img_adaptor = self.build_condition_adapter(
            config['img_adaptor'], 
            in_features=img_token_dim, 
            out_features=hidden_size
        )
        self.state_adaptor = self.build_condition_adapter(
            config['state_adaptor'], 
            in_features=state_token_dim * 2,    
            out_features=hidden_size
        )
        
        # è½¬æ¢é€‚é…å™¨åˆ°æ­£ç¡®çš„æ•°æ®ç±»å‹ï¼ˆç°æœ‰ï¼‰
        self.lang_adaptor = self.lang_adaptor.to(dtype)
        self.img_adaptor = self.img_adaptor.to(dtype)
        self.state_adaptor = self.state_adaptor.to(dtype)
        
        # ç°æœ‰çš„å™ªå£°è°ƒåº¦å™¨åˆ›å»ºä¿æŒä¸å˜
        noise_scheduler_config = config['noise_scheduler']
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
            clip_sample=noise_scheduler_config['clip_sample'],
        )
        self.noise_scheduler_sample = DPMSolverMultistepScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
        )

        self.num_train_timesteps = noise_scheduler_config['num_train_timesteps']
        self.num_inference_timesteps = noise_scheduler_config['num_inference_timesteps']
        self.prediction_type = noise_scheduler_config['prediction_type']

        self.pred_horizon = pred_horizon
        self.action_dim = action_dim

        print("Diffusion params: %e" % sum(
            [p.numel() for p in self.model.parameters()] + 
            [p.numel() for p in self.lang_adaptor.parameters()] + 
            [p.numel() for p in self.img_adaptor.parameters()] + 
            [p.numel() for p in self.state_adaptor.parameters()]))

    def compute_global_alignment_loss(self, action_tokens, cls_token):
        """
        è®¡ç®—ä¸DINOv2å…¨å±€ç‰¹å¾çš„å¯¹é½æŸå¤±
        
        Args:
            action_tokens: (B, T, hidden_size) åŠ¨ä½œtokens
            cls_token: (B, 1, dinov2_dim) DINOv2 CLS token
            
        Returns:
            loss: æ ‡é‡å¯¹é½æŸå¤±
        """
        B, T, hidden_size = action_tokens.shape
        
        # æ—¶é—´å¹³å‡å¾—åˆ°æ•´ä½“åŠ¨ä½œè¡¨ç¤º
        action_mean = action_tokens.mean(dim=1)  # (B, hidden_size)
        
        # æŠ•å½±åˆ°è§†è§‰ç‰¹å¾ç©ºé—´
        projected_action = self.model.action_to_vision_projector(action_mean)  # (B, dinov2_dim)
        
        # å¤„ç†è§†è§‰ç‰¹å¾
        cls_token_squeezed = cls_token.squeeze(1)  # (B, dinov2_dim)
        
        # L2å½’ä¸€åŒ–
        projected_action = F.normalize(projected_action, p=2, dim=-1)
        cls_token_norm = F.normalize(cls_token_squeezed, p=2, dim=-1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_similarity = F.cosine_similarity(projected_action, cls_token_norm, dim=-1)
        mean_similarity = cosine_similarity.mean()
        
        # è½¬æ¢ä¸ºæŸå¤±ï¼ˆæœ€å¤§åŒ–ç›¸ä¼¼åº¦ = æœ€å°åŒ–æŸå¤±ï¼‰
        loss = 1.0 - mean_similarity
        
        return loss

    def compute_selective_alignment_loss(self, action_tokens, cls_token, depth_features, critical_labels):
        """
        ğŸ†• æ ¹æ®å…³é”®æ—¶é—´æ®µé€‰æ‹©æ€§è®¡ç®—å¯¹é½æŸå¤±
        
        Args:
            action_tokens: (B, T, hidden_size) åŠ¨ä½œtokens
            cls_token: (B, 1, dinov2_dim) DINOv2 CLS token (å…¨å±€è¯­ä¹‰ç‰¹å¾)
            depth_features: (B, N_patches, depth_dim) æ·±åº¦ç‰¹å¾ï¼Œå…¶ä¸­ç¬¬0ä¸ªæ˜¯CLS token
            critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾ (0=éå…³é”®, 1=å…³é”®)
            
        Returns:
            loss: æ ‡é‡å¯¹é½æŸå¤±
        """
        if critical_labels is None:
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå›é€€åˆ°å…¨å±€å¯¹é½
            return self.compute_global_alignment_loss(action_tokens, cls_token)
        
        B, T, hidden_size = action_tokens.shape
        total_loss = torch.tensor(0.0, device=action_tokens.device, dtype=action_tokens.dtype)
        
        # æŠ•å½±æ‰€æœ‰åŠ¨ä½œtokensåˆ°è§†è§‰ç‰¹å¾ç©ºé—´
        projected_actions = self.model.action_to_vision_projector(
            action_tokens.reshape(B * T, hidden_size)
        ).reshape(B, T, -1)  # (B, T, 1024)
        
        # ğŸ†• æå–æ·±åº¦CLS token
        if depth_features.shape[1] == 1370:  # åŒ…å«CLS token
            depth_cls_token = depth_features[:, 0, :]  # (B, depth_dim) - ç¬¬0ä¸ªtokenæ˜¯CLS
        else:
            # å¦‚æœæ²¡æœ‰CLS tokenï¼ŒæŠ¥é”™æˆ–å›é€€
            raise ValueError("æ·±åº¦ç‰¹å¾ä¸­æ²¡æœ‰æ‰¾åˆ°CLS tokenï¼Œè¯·æ£€æŸ¥DepthAnythingV2çš„è¾“å‡ºæ ¼å¼")
        
        # å‡†å¤‡ç›®æ ‡ç‰¹å¾
        # å…¨å±€è¯­ä¹‰ç‰¹å¾ï¼šDINOv2 CLS token
        global_cls_token = cls_token.squeeze(1)  # (B, dinov2_dim)
        global_targets = global_cls_token.unsqueeze(1).expand(-1, T, -1)  # (B, T, dinov2_dim)
        
        # æ·±åº¦å‡ ä½•ç‰¹å¾ï¼šDepthAnythingV2 CLS token
        depth_targets = depth_cls_token.unsqueeze(1).expand(-1, T, -1)  # (B, T, depth_dim)
        
        # åˆ†åˆ«å¤„ç†å…³é”®å’Œéå…³é”®æ—¶é—´æ®µ
        critical_mask = critical_labels.bool()  # (B, T)
        non_critical_mask = ~critical_mask      # (B, T)
        
        valid_loss_computed = False
        
        # å¤„ç†éå…³é”®æ—¶é—´æ®µï¼šåŠ¨ä½œtokenä¸DINOv2 CLS tokenå¯¹é½
        if non_critical_mask.any():
            # è·å–éå…³é”®æ—¶é—´æ®µçš„tokenså’Œç›®æ ‡
            non_critical_actions = projected_actions[non_critical_mask]  # (N_non_critical, 1024)
            non_critical_targets = global_targets[non_critical_mask]     # (N_non_critical, 1024)
            
            if non_critical_actions.numel() > 0:
                # L2å½’ä¸€åŒ–
                non_critical_actions_norm = F.normalize(non_critical_actions, p=2, dim=-1)
                non_critical_targets_norm = F.normalize(non_critical_targets, p=2, dim=-1)
                
                # ä½™å¼¦ç›¸ä¼¼åº¦
                non_critical_similarity = F.cosine_similarity(
                    non_critical_actions_norm, non_critical_targets_norm, dim=-1
                )
                non_critical_loss = 1.0 - non_critical_similarity.mean()
                total_loss = total_loss + non_critical_loss
                valid_loss_computed = True
        
        # å¤„ç†å…³é”®æ—¶é—´æ®µï¼šåŠ¨ä½œtokenä¸DepthAnythingV2 CLS tokenå¯¹é½
        if critical_mask.any():
            # è·å–å…³é”®æ—¶é—´æ®µçš„tokenså’Œç›®æ ‡
            critical_actions = projected_actions[critical_mask]  # (N_critical, 1024)
            critical_targets = depth_targets[critical_mask]      # (N_critical, 1024)
            
            if critical_actions.numel() > 0:
                # L2å½’ä¸€åŒ–
                critical_actions_norm = F.normalize(critical_actions, p=2, dim=-1)
                critical_targets_norm = F.normalize(critical_targets, p=2, dim=-1)
                
                # ä½™å¼¦ç›¸ä¼¼åº¦
                critical_similarity = F.cosine_similarity(
                    critical_actions_norm, critical_targets_norm, dim=-1
                )
                critical_loss = 1.0 - critical_similarity.mean()
                total_loss = total_loss + critical_loss
                valid_loss_computed = True
        
        # å¦‚æœæ²¡æœ‰è®¡ç®—ä»»ä½•æœ‰æ•ˆæŸå¤±ï¼Œè¿”å›é›¶æŸå¤±
        if not valid_loss_computed:
            total_loss = torch.tensor(0.0, device=action_tokens.device, dtype=action_tokens.dtype)
        
        return total_loss

    def compute_routing_loss(self, routing_weights, critical_labels):
        """
        ğŸ†• è®¡ç®—è·¯ç”±ç›‘ç£æŸå¤±
        
        Args:
            routing_weights: (B, T, 2) è·¯ç”±æƒé‡ [å…¨å±€ä¸“å®¶, æ·±åº¦ä¸“å®¶]
            critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾ (0=éå…³é”®, 1=å…³é”®)
            
        Returns:
            loss: æ ‡é‡è·¯ç”±æŸå¤±
        """
        if critical_labels is None:
            return torch.tensor(0.0, device=routing_weights.device, dtype=routing_weights.dtype)
        
        # å°†å…³é”®æ ‡ç­¾è½¬æ¢ä¸ºone-hotæ ¼å¼
        # 0 -> [1, 0] (ä½¿ç”¨å…¨å±€ä¸“å®¶)
        # 1 -> [0, 1] (ä½¿ç”¨æ·±åº¦ä¸“å®¶)
        target_weights = torch.zeros_like(routing_weights)
        target_weights[:, :, 0] = 1 - critical_labels.float()  # å…¨å±€ä¸“å®¶æƒé‡
        target_weights[:, :, 1] = critical_labels.float()      # æ·±åº¦ä¸“å®¶æƒé‡
        
        # äº¤å‰ç†µæŸå¤±
        loss = F.binary_cross_entropy(routing_weights, target_weights, reduction='mean')
        
        return loss

    def compute_loss(self, lang_tokens, lang_attn_mask, img_tokens, 
                     state_tokens, action_gt, action_mask, ctrl_freqs,
                     cls_token=None, depth_features=None, critical_labels=None):
        """
        ğŸ”„ ä¿®æ”¹ï¼šè®¡ç®—æ€»æŸå¤±ï¼ŒåŒ…æ‹¬æ‰©æ•£æŸå¤±ã€REPAå¯¹é½æŸå¤±å’Œè·¯ç”±æŸå¤±
        
        Returns:
            tuple: (total_loss, diffusion_loss, repa_loss, routing_loss)
        """
        batch_size = lang_tokens.shape[0]
        device = lang_tokens.device

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹ï¼ˆç°æœ‰ä»£ç ä¿æŒä¸å˜ï¼‰
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_gt = action_gt.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        # åŸæœ‰çš„æ‰©æ•£æŸå¤±è®¡ç®—é€»è¾‘ï¼ˆç°æœ‰ä»£ç ä¿æŒä¸å˜ï¼‰
        noise = torch.randn(action_gt.shape, dtype=action_gt.dtype, device=device)
        timesteps = torch.randint(0, self.num_train_timesteps, (batch_size,), device=device).long()
        noisy_action = self.noise_scheduler.add_noise(action_gt, noise, timesteps)
        
        state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)
        action_mask = action_mask.expand(-1, state_action_traj.shape[1], -1)
        state_action_traj = torch.cat([state_action_traj, action_mask], dim=2)
        
        lang_cond, img_cond, state_action_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_action_traj)
        
        # è·å–æ¨¡å‹é¢„æµ‹å’Œä¸­é—´æ¿€æ´»ï¼ˆç°æœ‰ä»£ç ä¿æŒä¸å˜ï¼‰
        pred, intermediate_activations = self.model(
            state_action_traj, ctrl_freqs, timesteps, lang_cond, img_cond, 
            lang_mask=lang_attn_mask
        )

        # è®¡ç®—æ‰©æ•£æŸå¤±ï¼ˆç°æœ‰ä»£ç ä¿æŒä¸å˜ï¼‰
        pred_type = self.prediction_type 
        if pred_type == 'epsilon':
            target = noise
        elif pred_type == 'sample':
            target = action_gt
        else:
            raise ValueError(f"Unsupported prediction type {pred_type}")

        diffusion_loss = F.mse_loss(pred, target)
        
        # ğŸ”„ ä¿®æ”¹ï¼šè®¡ç®—åŒæ•™å¸ˆREPAå¯¹é½æŸå¤±å’Œè·¯ç”±æŸå¤±
        repa_loss = torch.tensor(0.0, device=device, dtype=diffusion_loss.dtype)
        routing_loss = torch.tensor(0.0, device=device, dtype=diffusion_loss.dtype)
        
        if self.enable_repa_loss and 'action_tokens_for_repa' in intermediate_activations:
            action_tokens = intermediate_activations['action_tokens_for_repa']
            
            if self.use_dual_teachers:
                # ğŸ†• åŒæ•™å¸ˆæ¨¡å¼ï¼šæ ¹æ®å…³é”®æ—¶é—´æ®µé€‰æ‹©CLS tokenå¯¹é½
                if cls_token is not None and depth_features is not None:
                    cls_token = cls_token.to(self.dtype)
                    depth_features = depth_features.to(self.dtype)
                    
                    # ä½¿ç”¨æ·±åº¦CLS tokençš„é€‰æ‹©æ€§å¯¹é½
                    repa_loss = self.compute_selective_alignment_loss(
                        action_tokens, cls_token, depth_features, critical_labels
                    )
                
                # è®¡ç®—è·¯ç”±ç›‘ç£æŸå¤±ï¼ˆå¦‚æœæœ‰è·¯ç”±æƒé‡ï¼‰
                if 'routing_weights' in intermediate_activations and critical_labels is not None:
                    routing_weights = intermediate_activations['routing_weights']
                    routing_loss = self.compute_routing_loss(routing_weights, critical_labels)
                    
            else:
                # ä¿æŒåŸæœ‰çš„å•æ•™å¸ˆæ¨¡å¼
                if cls_token is not None:
                    cls_token = cls_token.to(self.dtype)
                    repa_loss = self.compute_global_alignment_loss(action_tokens, cls_token)
        
        # æ€»æŸå¤±
        total_loss = (diffusion_loss + 
                     self.repa_loss_weight * repa_loss + 
                     self.routing_loss_weight * routing_loss)
        
        return total_loss, diffusion_loss, repa_loss, routing_loss

    # ç°æœ‰çš„å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜
    def conditional_sample(self, lang_cond, lang_attn_mask, img_cond, 
                           state_traj, action_mask, ctrl_freqs):
        """ç°æœ‰ä»£ç ä¿æŒä¸å˜"""
        device = state_traj.device
        dtype = state_traj.dtype
        noisy_action = torch.randn(
            size=(state_traj.shape[0], self.pred_horizon, self.action_dim), 
            dtype=dtype, device=device)
        action_mask = action_mask.expand(-1, self.pred_horizon, -1)
    
        self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)
        
        for t in self.noise_scheduler_sample.timesteps:
            action_traj = torch.cat([noisy_action, action_mask], dim=2)
            action_traj = self.state_adaptor(action_traj)
            state_action_traj = torch.cat([state_traj, action_traj], dim=1)
            
            model_output, _ = self.model(state_action_traj, ctrl_freqs,
                                        t.unsqueeze(-1).to(device),
                                        lang_cond, img_cond, lang_mask=lang_attn_mask)
            
            noisy_action = self.noise_scheduler_sample.step(
                model_output, t, noisy_action).prev_sample
            noisy_action = noisy_action.to(state_traj.dtype)
        
        noisy_action = noisy_action * action_mask
        return noisy_action
    
    def build_condition_adapter(self, projector_type, in_features, out_features):
        """ç°æœ‰ä»£ç ä¿æŒä¸å˜"""
        projector = None
        if projector_type == 'linear':
            projector = nn.Linear(in_features, out_features)
        else:
            mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu, projector_type)
            if mlp_gelu_match:
                mlp_depth = int(mlp_gelu_match.group(1))
                modules = [nn.Linear(in_features, out_features)]
                for _ in range(1, mlp_depth):
                    modules.append(nn.GELU(approximate="tanh"))
                    modules.append(nn.Linear(out_features, out_features))
                projector = nn.Sequential(*modules)
        if projector is None:
            raise ValueError(f'Unknown projector type: {projector_type}')
        return projector
    
    def adapt_conditions(self, lang_tokens, img_tokens, state_tokens):
        """ç°æœ‰ä»£ç ä¿æŒä¸å˜"""
        adapted_lang = self.lang_adaptor(lang_tokens)
        adapted_img = self.img_adaptor(img_tokens)
        adapted_state = self.state_adaptor(state_tokens)
        return adapted_lang, adapted_img, adapted_state

    def predict_action(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens,
                       action_mask, ctrl_freqs, vision_features=None):
        """ç°æœ‰ä»£ç ä¿æŒä¸å˜"""
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        state_tokens = torch.cat([state_tokens, action_mask], dim=2)
        lang_cond, img_cond, state_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_tokens)
        
        action_pred = self.conditional_sample(
            lang_cond, lang_attn_mask, img_cond, 
            state_traj, action_mask, ctrl_freqs,
        )
        
        return action_pred
    
    def forward(self, *args, **kwargs) -> torch.Tensor:
        """ğŸ”„ ä¿®æ”¹ï¼šä¿æŒå…¼å®¹æ€§ï¼Œåªè¿”å›æ€»æŸå¤±"""
        total_loss, _, _, _ = self.compute_loss(*args, **kwargs)
        return total_loss