import re
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusers.schedulers.scheduling_dpmsolver_multistep import \
    DPMSolverMultistepScheduler

from models.hub_mixin import CompatiblePyTorchModelHubMixin
from models.rdt.model import RDT


class RDTRunner(
        nn.Module, 
        CompatiblePyTorchModelHubMixin, 
        repo_url="https://huggingface.co/robotics-diffusion-transformer/rdt-1b"
    ):
    """
    ğŸ”„ ä¿®æ”¹ï¼šé›†æˆREPAå¯¹é½æŸå¤±çš„RDTè¿è¡Œå™¨
    """
    def __init__(self, *, action_dim, pred_horizon, config, 
                 lang_token_dim, img_token_dim, state_token_dim, 
                 max_lang_cond_len, img_cond_len, lang_pos_embed_config=None, 
                 img_pos_embed_config=None, dtype=torch.bfloat16,
                 # ğŸ†• REPAç›¸å…³å‚æ•°
                 enable_repa_loss=True, repa_loss_weight=0.2):
        super(RDTRunner, self).__init__()
        
        # ğŸ†• REPAæŸå¤±é…ç½®
        self.enable_repa_loss = enable_repa_loss
        self.repa_loss_weight = repa_loss_weight
        self.dtype = dtype  # ä¿å­˜æ•°æ®ç±»å‹
        
        print(f"ğŸ”§ RDTRunneråˆå§‹åŒ–:")
        print(f"   - REPAæŸå¤±å¯ç”¨: {enable_repa_loss}")
        print(f"   - REPAæŸå¤±æƒé‡: {repa_loss_weight}")
        print(f"   - æ•°æ®ç±»å‹: {dtype}")
        
        # åˆ›å»ºæ‰©æ•£æ¨¡å‹
        hidden_size = config['rdt']['hidden_size']
        self.model = RDT(
            output_dim=action_dim,
            horizon=pred_horizon,
            hidden_size=hidden_size,
            depth=8,  # ğŸ”„ ä¿®æ”¹ï¼šå›ºå®š8å±‚
            num_heads=config['rdt']['num_heads'],
            max_lang_cond_len=max_lang_cond_len,
            img_cond_len=img_cond_len,
            lang_pos_embed_config=lang_pos_embed_config,
            img_pos_embed_config=img_pos_embed_config,
            dtype=dtype,
            enable_repa_loss=enable_repa_loss,  # ğŸ†•
        )

        # åˆ›å»ºå„ç§æ¡ä»¶è¾“å…¥çš„é€‚é…å™¨
        self.lang_adaptor = self.build_condition_adapter(
            config['lang_adaptor'], 
            in_features=lang_token_dim, 
            out_features=hidden_size
        )
        self.img_adaptor = self.build_condition_adapter(
            config['img_adaptor'], 
            in_features=img_token_dim, 
            out_features=hidden_size
        )
        # stateåŒ…å«çŠ¶æ€å’ŒçŠ¶æ€æ©ç 
        self.state_adaptor = self.build_condition_adapter(
            config['state_adaptor'], 
            in_features=state_token_dim * 2,    
            out_features=hidden_size
        )
        
        # ğŸ†• å°†é€‚é…å™¨è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹
        self.lang_adaptor = self.lang_adaptor.to(dtype)
        self.img_adaptor = self.img_adaptor.to(dtype)
        self.state_adaptor = self.state_adaptor.to(dtype)
        
        # åˆ›å»ºå™ªå£°è°ƒåº¦å™¨
        noise_scheduler_config = config['noise_scheduler']
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
            clip_sample=noise_scheduler_config['clip_sample'],
        )
        self.noise_scheduler_sample = DPMSolverMultistepScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
        )

        self.num_train_timesteps = noise_scheduler_config['num_train_timesteps']
        self.num_inference_timesteps = noise_scheduler_config['num_inference_timesteps']
        self.prediction_type = noise_scheduler_config['prediction_type']

        self.pred_horizon = pred_horizon
        self.action_dim = action_dim

        print("Diffusion params: %e" % sum(
            [p.numel() for p in self.model.parameters()] + 
            [p.numel() for p in self.lang_adaptor.parameters()] + 
            [p.numel() for p in self.img_adaptor.parameters()] + 
            [p.numel() for p in self.state_adaptor.parameters()]))

    def compute_repa_loss(self, action_tokens, vision_features):
        """
        ğŸ†• æ ¸å¿ƒæ–¹æ³•ï¼šè®¡ç®—REPAé£æ ¼çš„å¯¹é½æŸå¤±
        
        Args:
            action_tokens: (B, horizon, hidden_size) ç¬¬4å±‚çš„åŠ¨ä½œtoken
            vision_features: (B, N_patches, dinov2_dim) DINOv2è§†è§‰ç‰¹å¾
        
        Returns:
            repa_loss: æ ‡é‡æŸå¤±å€¼
        """
        if not self.enable_repa_loss or vision_features is None:
            return torch.tensor(0.0, device=action_tokens.device, dtype=action_tokens.dtype)
        
        B, horizon, hidden_size = action_tokens.shape
        B_v, N_patches, dinov2_dim = vision_features.shape
        
        # éªŒè¯è¾“å…¥ç»´åº¦
        assert B == B_v, f"æ‰¹æ¬¡å¤§å°ä¸åŒ¹é…: {B} vs {B_v}"
        
        print(f"ğŸ” REPAæŸå¤±è®¡ç®—:")
        print(f"   - åŠ¨ä½œtoken: {action_tokens.shape}")
        print(f"   - è§†è§‰ç‰¹å¾: {vision_features.shape}")
        
        # ç¡®ä¿è§†è§‰ç‰¹å¾æ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹
        vision_features = vision_features.to(action_tokens.dtype)
        
        # Step 1: æŠ•å½±åŠ¨ä½œtokenåˆ°è§†è§‰ç‰¹å¾ç©ºé—´
        action_tokens_flat = action_tokens.reshape(-1, hidden_size)  # (B*horizon, hidden_size)
        projected_actions = self.model.action_to_vision_projector(action_tokens_flat)  # (B*horizon, dinov2_dim)
        projected_actions = projected_actions.reshape(B, horizon, dinov2_dim)  # (B, horizon, dinov2_dim)
        
        print(f"   - æŠ•å½±ååŠ¨ä½œ: {projected_actions.shape}")
        
        # Step 2: L2å½’ä¸€åŒ–ç‰¹å¾
        projected_actions = F.normalize(projected_actions, dim=-1)  # (B, horizon, dinov2_dim)
        vision_features = F.normalize(vision_features, dim=-1)      # (B, N_patches, dinov2_dim)
        
        # Step 3: è®¡ç®—å¯¹é½æŸå¤± (å…¨å±€å¯¹é½ç­–ç•¥)
        total_loss = 0.0
        similarities = []
        
        for b in range(B):
            # æ¯ä¸ªbatchå•ç‹¬è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            action_feat = projected_actions[b]  # (horizon, dinov2_dim)
            vision_feat = vision_features[b]    # (N_patches, dinov2_dim)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ: (horizon, N_patches)
            similarity_matrix = torch.mm(action_feat, vision_feat.t())
            
            # å¯¹æ¯ä¸ªåŠ¨ä½œtokenï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„è§†è§‰patch
            max_similarity, best_patch_idx = similarity_matrix.max(dim=1)  # (horizon,)
            similarities.append(max_similarity.mean().item())
            
            # è´Ÿç›¸ä¼¼åº¦ä½œä¸ºæŸå¤± (é¼“åŠ±é«˜ç›¸ä¼¼åº¦)
            batch_loss = -max_similarity.mean()
            total_loss += batch_loss
        
        repa_loss = total_loss / B
        
        print(f"   - å¹³å‡ç›¸ä¼¼åº¦: {sum(similarities)/len(similarities):.4f}")
        print(f"   - REPAæŸå¤±: {repa_loss.item():.4f}")
        
        return repa_loss

    def compute_loss(self, lang_tokens, lang_attn_mask, img_tokens, 
                     state_tokens, action_gt, action_mask, ctrl_freqs,
                     vision_features=None  # ğŸ†• DINOv2è§†è§‰ç‰¹å¾
                    ):
        """
        ğŸ”„ ä¿®æ”¹ï¼šè®¡ç®—æ€»æŸå¤±ï¼ŒåŒ…æ‹¬æ‰©æ•£æŸå¤±å’ŒREPAå¯¹é½æŸå¤±
        
        Args:
            vision_features: (B, N_patches, dinov2_dim) DINOv2æå–çš„è§†è§‰ç‰¹å¾
            
        Returns:
            tuple: (total_loss, diffusion_loss, repa_loss) è¯¦ç»†æŸå¤±ä¿¡æ¯
        """
        batch_size = lang_tokens.shape[0]
        device = lang_tokens.device  

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_gt = action_gt.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        # åŸæœ‰çš„æ‰©æ•£æŸå¤±è®¡ç®—é€»è¾‘
        # é‡‡æ ·å™ªå£°
        noise = torch.randn(
            action_gt.shape, dtype=action_gt.dtype, device=device
        )
        # é‡‡æ ·éšæœºæ‰©æ•£æ—¶é—´æ­¥
        timesteps = torch.randint(
            0, self.num_train_timesteps, 
            (batch_size,), device=device
        ).long()
        # æ ¹æ®å™ªå£°å¤§å°åœ¨æ¯ä¸ªæ—¶é—´æ­¥æ·»åŠ å™ªå£°åˆ°å¹²å‡€åŠ¨ä½œ
        noisy_action = self.noise_scheduler.add_noise(
            action_gt, noise, timesteps)
        
        # æ‹¼æ¥çŠ¶æ€å’ŒåŠ¨ä½œtokenå½¢æˆè¾“å…¥åºåˆ—
        state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)
        # å°†åŠ¨ä½œæ©ç æ·»åŠ åˆ°è¾“å…¥åºåˆ—
        action_mask = action_mask.expand(-1, state_action_traj.shape[1], -1)
        state_action_traj = torch.cat([state_action_traj, action_mask], dim=2)
        
        # å°†ç»´åº¦å¯¹é½åˆ°éšè—å¤§å°
        lang_cond, img_cond, state_action_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_action_traj)
        
        # ğŸ”„ ä¿®æ”¹ï¼šè·å–æ¨¡å‹é¢„æµ‹å’Œä¸­é—´æ¿€æ´»
        pred, intermediate_activations = self.model(
            state_action_traj, ctrl_freqs, 
            timesteps, lang_cond, img_cond, 
            lang_mask=lang_attn_mask
        )

        # è®¡ç®—æ‰©æ•£æŸå¤±
        pred_type = self.prediction_type 
        if pred_type == 'epsilon':
            target = noise
        elif pred_type == 'sample':
            target = action_gt
        else:
            raise ValueError(f"Unsupported prediction type {pred_type}")

        diffusion_loss = F.mse_loss(pred, target)
        
        # ğŸ†• è®¡ç®—REPAå¯¹é½æŸå¤±
        repa_loss = torch.tensor(0.0, device=device, dtype=diffusion_loss.dtype)
        if self.enable_repa_loss and 'action_tokens_for_repa' in intermediate_activations:
            action_tokens = intermediate_activations['action_tokens_for_repa']
            if vision_features is not None:
                vision_features = vision_features.to(self.dtype)
            repa_loss = self.compute_repa_loss(action_tokens, vision_features)
        
        # æ€»æŸå¤± = æ‰©æ•£æŸå¤± + åŠ æƒå¯¹é½æŸå¤±
        total_loss = diffusion_loss + self.repa_loss_weight * repa_loss
        
        print(f"ğŸ’° æŸå¤±è¯¦æƒ…:")
        print(f"   - æ‰©æ•£æŸå¤±: {diffusion_loss.item():.4f}")
        print(f"   - REPAæŸå¤±: {repa_loss.item():.4f}")
        print(f"   - æ€»æŸå¤±: {total_loss.item():.4f}")
        
        return total_loss, diffusion_loss, repa_loss

    def conditional_sample(self, lang_cond, lang_attn_mask, img_cond, 
                           state_traj, action_mask, ctrl_freqs):
        """
        ğŸ”„ ä¿®æ”¹ï¼šæ¡ä»¶é‡‡æ ·ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰
        æ³¨æ„ï¼šæ¨ç†æ—¶æš‚ä¸ä½¿ç”¨REPAæŸå¤±ï¼Œåªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨
        """
        device = state_traj.device
        dtype = state_traj.dtype
        noisy_action = torch.randn(
            size=(state_traj.shape[0], self.pred_horizon, self.action_dim), 
            dtype=dtype, device=device)
        action_mask = action_mask.expand(-1, self.pred_horizon, -1)
    
        # è®¾ç½®é‡‡æ ·æ—¶é—´æ­¥
        self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)
        
        for t in self.noise_scheduler_sample.timesteps:
            # å‡†å¤‡çŠ¶æ€-åŠ¨ä½œè½¨è¿¹
            action_traj = torch.cat([noisy_action, action_mask], dim=2)
            action_traj = self.state_adaptor(action_traj)
            state_action_traj = torch.cat([state_traj, action_traj], dim=1)
            
            # ğŸ”„ ä¿®æ”¹ï¼šæ¨¡å‹é¢„æµ‹ï¼Œå¿½ç•¥ä¸­é—´æ¿€æ´»
            model_output, _ = self.model(state_action_traj, ctrl_freqs,
                                        t.unsqueeze(-1).to(device),
                                        lang_cond, img_cond, lang_mask=lang_attn_mask)
            
            # è®¡ç®—å‰ä¸€æ­¥åŠ¨ä½œ: x_t -> x_t-1
            noisy_action = self.noise_scheduler_sample.step(
                model_output, t, noisy_action).prev_sample
            noisy_action = noisy_action.to(state_traj.dtype)
        
        # æœ€ååº”ç”¨åŠ¨ä½œæ©ç 
        noisy_action = noisy_action * action_mask

        return noisy_action
    
    def build_condition_adapter(
        self, projector_type, in_features, out_features):
        """æ„å»ºæ¡ä»¶é€‚é…å™¨"""
        projector = None
        if projector_type == 'linear':
            projector = nn.Linear(in_features, out_features)
        else:
            # ä¿®å¤ï¼šé—­åˆå­—ç¬¦ä¸²å¹¶ä¿®æ­£æ­£åˆ™è¡¨è¾¾å¼
            mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu$', projector_type)
            if mlp_gelu_match:
                mlp_depth = int(mlp_gelu_match.group(1))
                modules = [nn.Linear(in_features, out_features)]
                for _ in range(1, mlp_depth):
                    modules.append(nn.GELU(approximate="tanh"))
                    modules.append(nn.Linear(out_features, out_features))
                projector = nn.Sequential(*modules)
        if projector is None:
            raise ValueError(f'Unknown projector type: {projector_type}')
        return projector
    
    def adapt_conditions(self, lang_tokens, img_tokens, state_tokens):
        """é€‚é…æ¡ä»¶è¾“å…¥çš„ç»´åº¦"""
        adapted_lang = self.lang_adaptor(lang_tokens)
        adapted_img = self.img_adaptor(img_tokens)
        adapted_state = self.state_adaptor(state_tokens)

        return adapted_lang, adapted_img, adapted_state

    def predict_action(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens,
                       action_mask, ctrl_freqs, vision_features=None):
        """
        ğŸ”„ ä¿®æ”¹ï¼šé¢„æµ‹åŠ¨ä½œï¼ˆæ¨ç†æ¥å£ï¼‰
        
        Args:
            vision_features: æ¨ç†æ—¶å¯é€‰çš„è§†è§‰ç‰¹å¾ï¼ˆæš‚ä¸ä½¿ç”¨ï¼‰
        """
        # ç¡®ä¿è¾“å…¥æ•°æ®ç±»å‹æ­£ç¡®
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        # å‡†å¤‡çŠ¶æ€å’Œæ¡ä»¶
        state_tokens = torch.cat([state_tokens, action_mask], dim=2)
        lang_cond, img_cond, state_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_tokens)
        
        # è¿è¡Œé‡‡æ ·
        action_pred = self.conditional_sample(
            lang_cond, lang_attn_mask, img_cond, 
            state_traj, action_mask, ctrl_freqs,
        )
        
        return action_pred
    
    def forward(self, *args, **kwargs) -> torch.Tensor:
        """ä¿æŒå…¼å®¹æ€§ï¼Œåªè¿”å›æ€»æŸå¤±"""
        total_loss, _, _ = self.compute_loss(*args, **kwargs)
        return total_loss