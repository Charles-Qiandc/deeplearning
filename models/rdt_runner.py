import re
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler

from models.hub_mixin import CompatiblePyTorchModelHubMixin
from models.rdt.model import RDT


class ConstrainedAdaptiveWeightLearner(nn.Module):
    """
    åŸºäºå…³é”®æ—¶é—´æ®µçº¦æŸçš„è‡ªé€‚åº”æƒé‡å­¦ä¹ å™¨
    å…³é”®æ—¶é—´æ®µï¼šå¼ºåˆ¶æ·±åº¦æƒé‡å ä¸»å¯¼ï¼Œä½†å…·ä½“æ¯”ä¾‹å¯å­¦ä¹ 
    éå…³é”®æ—¶é—´æ®µï¼šå¼ºåˆ¶å…¨å±€æƒé‡å ä¸»å¯¼ï¼Œä½†å…·ä½“æ¯”ä¾‹å¯å­¦ä¹ 
    """
    
    def __init__(self, 
                 action_dim: int = 2048,
                 hidden_dim: int = 512,
                 temperature_init: float = 1.0,
                 critical_depth_min: float = 0.6,      # å…³é”®æ—¶é—´æ®µæ·±åº¦æƒé‡æœ€å°å€¼
                 critical_depth_max: float = 0.9,      # å…³é”®æ—¶é—´æ®µæ·±åº¦æƒé‡æœ€å¤§å€¼
                 non_critical_global_min: float = 0.6, # éå…³é”®æ—¶é—´æ®µå…¨å±€æƒé‡æœ€å°å€¼
                 non_critical_global_max: float = 0.9): # éå…³é”®æ—¶é—´æ®µå…¨å±€æƒé‡æœ€å¤§å€¼
        super().__init__()
        
        # æƒé‡é¢„æµ‹ç½‘ç»œï¼šè¾“å‡ºå•ä¸ªlogitï¼Œç”¨äºåœ¨çº¦æŸèŒƒå›´å†…è°ƒèŠ‚æƒé‡
        self.weight_predictor = nn.Sequential(
            nn.Linear(action_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, 1),  # åªè¾“å‡º1ä¸ªå€¼ç”¨äºæƒé‡è°ƒèŠ‚
        )
        
        # å¯å­¦ä¹ å‚æ•°
        self.temperature = nn.Parameter(torch.tensor(temperature_init))
        
        # çº¦æŸèŒƒå›´
        self.critical_depth_min = critical_depth_min
        self.critical_depth_max = critical_depth_max
        self.non_critical_global_min = non_critical_global_min
        self.non_critical_global_max = non_critical_global_max
        
        self._initialize_weights()
        
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in self.weight_predictor:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.1)  # å°çš„åˆå§‹åŒ–
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, action_tokens: torch.Tensor, routing_weights: torch.Tensor):
        """
        Args:
            action_tokens: (B, T, action_dim)
            routing_weights: (B, T, 2) è·¯ç”±ç½‘ç»œè¾“å‡ºçš„è½¯æƒé‡ [å…¨å±€, æ·±åº¦]
            
        Returns:
            dict: åŒ…å«çº¦æŸåçš„è‡ªé€‚åº”æƒé‡
        """
        B, T, D = action_tokens.shape
        device = action_tokens.device
        dtype = action_tokens.dtype  # ä½¿ç”¨è¾“å…¥å¼ é‡çš„æ•°æ®ç±»å‹
        
        # 1. é¢„æµ‹æƒé‡è°ƒèŠ‚å‚æ•° - ä¿®å¤å†…å­˜å¸ƒå±€é—®é¢˜
        flat_tokens = action_tokens.contiguous().view(B * T, D)  # ç¡®ä¿å¼ é‡è¿ç»­
        raw_adjustment = self.weight_predictor(flat_tokens)  # (B*T, 1)
        raw_adjustment = raw_adjustment.view(B, T)  # (B, T)
        
        # 2. åº”ç”¨æ¸©åº¦ç¼©æ”¾çš„sigmoidï¼Œæ˜ å°„åˆ° [0, 1]
        temp = torch.clamp(self.temperature, min=0.1, max=5.0)
        adjustment_factor = torch.sigmoid(raw_adjustment / temp)  # (B, T) âˆˆ [0, 1]
        
        # 3. æ ¹æ®è·¯ç”±æƒé‡åˆ¤æ–­æ—¶é—´æ®µç±»å‹
        # routing_weights[:,:,1] > routing_weights[:,:,0] è¡¨ç¤ºå€¾å‘äºæ·±åº¦ä¸“å®¶ï¼ˆå…³é”®æ—¶é—´æ®µï¼‰
        depth_preference = routing_weights[:, :, 1] > routing_weights[:, :, 0]  # (B, T)
        
        # 4. æ ¹æ®è·¯ç”±åå¥½åº”ç”¨ä¸åŒçš„æƒé‡çº¦æŸ
        global_weights = torch.zeros_like(adjustment_factor, dtype=dtype, device=device)  # (B, T)
        depth_weights = torch.zeros_like(adjustment_factor, dtype=dtype, device=device)   # (B, T)
        
        # æ·±åº¦åå¥½æ—¶é—´æ®µï¼šæ·±åº¦æƒé‡åœ¨çº¦æŸèŒƒå›´å†…å¯å­¦ä¹ 
        if depth_preference.any():
            depth_pref_adjustment = adjustment_factor[depth_preference]
            
            # å°†è°ƒèŠ‚å› å­æ˜ å°„åˆ°æ·±åº¦æƒé‡èŒƒå›´ - ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            constrained_depth_weights = (
                torch.tensor(self.critical_depth_min, dtype=dtype, device=device) + 
                depth_pref_adjustment * (self.critical_depth_max - self.critical_depth_min)
            )
            constrained_global_weights = torch.tensor(1.0, dtype=dtype, device=device) - constrained_depth_weights
            
            depth_weights[depth_preference] = constrained_depth_weights
            global_weights[depth_preference] = constrained_global_weights
        
        # å…¨å±€åå¥½æ—¶é—´æ®µï¼šå…¨å±€æƒé‡åœ¨çº¦æŸèŒƒå›´å†…å¯å­¦ä¹ 
        global_preference = ~depth_preference
        if global_preference.any():
            global_pref_adjustment = adjustment_factor[global_preference]
            
            # å°†è°ƒèŠ‚å› å­æ˜ å°„åˆ°å…¨å±€æƒé‡èŒƒå›´ - ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            constrained_global_weights = (
                torch.tensor(self.non_critical_global_min, dtype=dtype, device=device) + 
                global_pref_adjustment * (self.non_critical_global_max - self.non_critical_global_min)
            )
            constrained_depth_weights = torch.tensor(1.0, dtype=dtype, device=device) - constrained_global_weights
            
            global_weights[global_preference] = constrained_global_weights
            depth_weights[global_preference] = constrained_depth_weights
        
        # 5. ç»„è£…æœ€ç»ˆæƒé‡
        adaptive_weights = torch.stack([global_weights, depth_weights], dim=-1)  # (B, T, 2)
        
        # 6. è®¡ç®—å¤šæ ·æ€§æŸå¤±ï¼šé¼“åŠ±æƒé‡åœ¨çº¦æŸèŒƒå›´å†…æœ‰æ‰€å˜åŒ–
        diversity_loss = self._compute_diversity_loss(adjustment_factor, depth_preference, dtype=dtype)
        
        return {
            'adaptive_weights': adaptive_weights,           # (B, T, 2)
            'adjustment_factor': adjustment_factor,         # (B, T)
            'diversity_loss': diversity_loss,              # æ ‡é‡
            'temperature': temp.item(),                    # å½“å‰æ¸©åº¦å€¼
            'depth_preference_ratio': depth_preference.float().mean().item(),
        }
    
    def _compute_diversity_loss(self, adjustment_factor: torch.Tensor, depth_preference: torch.Tensor, dtype=None):
        """è®¡ç®—å¤šæ ·æ€§æŸå¤±ï¼šé¼“åŠ±æƒé‡åœ¨çº¦æŸèŒƒå›´å†…çš„å¤šæ ·æ€§"""
        if dtype is None:
            dtype = adjustment_factor.dtype
        device = adjustment_factor.device
        
        diversity_loss = torch.tensor(0.0, dtype=dtype, device=device)
        
        if depth_preference.any():
            depth_pref_adjustments = adjustment_factor[depth_preference]
            if len(depth_pref_adjustments) > 1:
                depth_var = torch.var(depth_pref_adjustments)
                target_var = torch.tensor(0.1, dtype=dtype, device=device)
                diversity_loss += F.mse_loss(depth_var, target_var)
        
        global_preference = ~depth_preference
        if global_preference.any():
            global_pref_adjustments = adjustment_factor[global_preference]
            if len(global_pref_adjustments) > 1:
                global_var = torch.var(global_pref_adjustments)
                target_var = torch.tensor(0.1, dtype=dtype, device=device)
                diversity_loss += F.mse_loss(global_var, target_var)
        
        return diversity_loss


class RDTRunner(nn.Module, CompatiblePyTorchModelHubMixin, 
               repo_url="https://huggingface.co/robotics-diffusion-transformer/rdt-1b"):
    """
    é›†æˆåŒæ•™å¸ˆREPAå¯¹é½æŸå¤±å’Œçº¦æŸè‡ªé€‚åº”æƒé‡å­¦ä¹ çš„RDTè¿è¡Œå™¨
    """
    def __init__(self, *, action_dim, pred_horizon, config, 
                 lang_token_dim, img_token_dim, state_token_dim, 
                 max_lang_cond_len, img_cond_len, lang_pos_embed_config=None, 
                 img_pos_embed_config=None, dtype=torch.bfloat16,
                 # REPAç›¸å…³å‚æ•°
                 enable_repa_loss=True, repa_loss_weight=0.2,
                 # åŒæ•™å¸ˆç›¸å…³å‚æ•°
                 use_dual_teachers=False, routing_loss_weight=0.1,
                 # ğŸ†• çº¦æŸè‡ªé€‚åº”æƒé‡å‚æ•°
                 enable_constrained_weights=True,
                 constrained_weight_config=None):
        super(RDTRunner, self).__init__()
        
        # REPAæŸå¤±é…ç½®
        self.enable_repa_loss = enable_repa_loss
        self.repa_loss_weight = repa_loss_weight
        self.dtype = dtype
        
        # åŒæ•™å¸ˆé…ç½®
        self.use_dual_teachers = use_dual_teachers
        self.routing_loss_weight = routing_loss_weight
        
        # ğŸ†• çº¦æŸè‡ªé€‚åº”æƒé‡é…ç½®
        self.enable_constrained_weights = enable_constrained_weights
        
        print(f"ğŸ”§ RDTRunneråˆå§‹åŒ–:")
        print(f"   - REPAæŸå¤±å¯ç”¨: {enable_repa_loss}")
        print(f"   - REPAæŸå¤±æƒé‡: {repa_loss_weight}")
        print(f"   - åŒæ•™å¸ˆæ¨¡å¼: {use_dual_teachers}")
        print(f"   - è·¯ç”±æŸå¤±æƒé‡: {routing_loss_weight}")
        print(f"   - çº¦æŸè‡ªé€‚åº”æƒé‡: {enable_constrained_weights}")
        print(f"   - æ•°æ®ç±»å‹: {dtype}")
        
        # åˆ›å»ºæ‰©æ•£æ¨¡å‹
        hidden_size = config['rdt']['hidden_size']
        self.model = RDT(
            output_dim=action_dim,
            horizon=pred_horizon,
            hidden_size=hidden_size,
            depth=config['rdt']['depth'], 
            num_heads=config['rdt']['num_heads'],
            max_lang_cond_len=max_lang_cond_len,
            img_cond_len=img_cond_len,
            lang_pos_embed_config=lang_pos_embed_config,
            img_pos_embed_config=img_pos_embed_config,
            dtype=dtype,
            enable_repa_loss=enable_repa_loss,
            use_dual_teachers=use_dual_teachers,
            dinov2_feature_dim=1024,
            depth_feature_dim=1024,
        )

        # é€‚é…å™¨åˆ›å»º
        self.lang_adaptor = self.build_condition_adapter(
            config['lang_adaptor'], 
            in_features=lang_token_dim, 
            out_features=hidden_size
        )
        self.img_adaptor = self.build_condition_adapter(
            config['img_adaptor'], 
            in_features=img_token_dim, 
            out_features=hidden_size
        )
        self.state_adaptor = self.build_condition_adapter(
            config['state_adaptor'], 
            in_features=state_token_dim * 2,    
            out_features=hidden_size
        )
        
        # è½¬æ¢é€‚é…å™¨åˆ°æ­£ç¡®çš„æ•°æ®ç±»å‹
        self.lang_adaptor = self.lang_adaptor.to(dtype)
        self.img_adaptor = self.img_adaptor.to(dtype)
        self.state_adaptor = self.state_adaptor.to(dtype)
        
        # ğŸ†• çº¦æŸæƒé‡å­¦ä¹ å™¨åˆå§‹åŒ–
        if self.enable_constrained_weights and enable_repa_loss:
            default_config = {
                'action_dim': hidden_size,
                'hidden_dim': 512,
                'temperature_init': 1.0,
                'critical_depth_min': 0.6,
                'critical_depth_max': 0.9,
                'non_critical_global_min': 0.6,
                'non_critical_global_max': 0.9,
            }
            if constrained_weight_config:
                default_config.update(constrained_weight_config)
            
            self.constrained_weight_config = default_config
            self.constrained_weight_learner = None  # å»¶è¿Ÿåˆå§‹åŒ–
            
            print(f"ğŸ¯ çº¦æŸæƒé‡å­¦ä¹ å™¨é…ç½®: {default_config}")
        
        # å™ªå£°è°ƒåº¦å™¨åˆ›å»º
        noise_scheduler_config = config['noise_scheduler']
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
            clip_sample=noise_scheduler_config['clip_sample'],
        )
        self.noise_scheduler_sample = DPMSolverMultistepScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
        )

        self.num_train_timesteps = noise_scheduler_config['num_train_timesteps']
        self.num_inference_timesteps = noise_scheduler_config['num_inference_timesteps']
        self.prediction_type = noise_scheduler_config['prediction_type']

        self.pred_horizon = pred_horizon
        self.action_dim = action_dim

        print("Diffusion params: %e" % sum(
            [p.numel() for p in self.model.parameters()] + 
            [p.numel() for p in self.lang_adaptor.parameters()] + 
            [p.numel() for p in self.img_adaptor.parameters()] + 
            [p.numel() for p in self.state_adaptor.parameters()]))

    def compute_constrained_dual_teacher_alignment_loss(self, action_tokens, dinov2_cls_token, 
                                                  depth_cls_token, routing_weights, critical_labels=None):
        """
        åŸºäºè·¯ç”±ç½‘ç»œå’Œçº¦æŸæƒé‡å­¦ä¹ çš„åŒæ•™å¸ˆå¯¹é½æŸå¤±
        
        Args:
            action_tokens: (B, T, hidden_size) åŠ¨ä½œtokens
            dinov2_cls_token: (B, 1, 1024) DINOv2å…¨å±€ç‰¹å¾
            depth_cls_token: (B, 1024) æ·±åº¦ç‰¹å¾
            routing_weights: (B, T, 2) è·¯ç”±ç½‘ç»œè¾“å‡ºæƒé‡
            critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼ˆå¯é€‰ï¼Œç”¨äºåˆ†æï¼‰
        """
        B, T, hidden_size = action_tokens.shape
        device = action_tokens.device
        dtype = action_tokens.dtype  # è·å–æ­£ç¡®çš„æ•°æ®ç±»å‹
        
        # 1. å»¶è¿Ÿåˆå§‹åŒ–çº¦æŸæƒé‡å­¦ä¹ å™¨ï¼Œç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        if self.constrained_weight_learner is None:
            self.constrained_weight_learner = ConstrainedAdaptiveWeightLearner(
                **self.constrained_weight_config
            ).to(device, dtype=dtype)  # ç¡®ä¿æƒé‡å­¦ä¹ å™¨ä½¿ç”¨æ­£ç¡®çš„æ•°æ®ç±»å‹
            print(f"çº¦æŸæƒé‡å­¦ä¹ å™¨å·²åˆå§‹åŒ–ï¼Œæ•°æ®ç±»å‹: {dtype}")
        
        # 2. æŠ•å½±è§†è§‰ç‰¹å¾åˆ°åŠ¨ä½œç©ºé—´
        dinov2_cls_squeezed = dinov2_cls_token.squeeze(1) if dinov2_cls_token.dim() == 3 else dinov2_cls_token
        depth_cls_squeezed = depth_cls_token.squeeze(1) if depth_cls_token.dim() == 3 else depth_cls_token
        
        projected_dinov2 = self.model.dinov2_to_action_projector(dinov2_cls_squeezed)  # (B, 2048)
        projected_depth = self.model.depth_to_action_projector(depth_cls_squeezed)      # (B, 2048)
        
        # æ‰©å±•åˆ°æ—¶é—´ç»´åº¦
        projected_dinov2_expanded = projected_dinov2.unsqueeze(1).expand(-1, T, -1)  # (B, T, 2048)
        projected_depth_expanded = projected_depth.unsqueeze(1).expand(-1, T, -1)    # (B, T, 2048)
        
        # 3. è®¡ç®—åŸºç¡€å¯¹é½æŸå¤±
        action_norm = F.normalize(action_tokens, p=2, dim=-1)
        dinov2_norm = F.normalize(projected_dinov2_expanded, p=2, dim=-1)
        depth_norm = F.normalize(projected_depth_expanded, p=2, dim=-1)
        
        # ä½™å¼¦ç›¸ä¼¼åº¦
        global_similarity = torch.sum(action_norm * dinov2_norm, dim=-1)  # (B, T)
        depth_similarity = torch.sum(action_norm * depth_norm, dim=-1)    # (B, T)
        
        # è½¬æ¢ä¸ºæŸå¤±
        global_loss = torch.tensor(1.0, dtype=dtype, device=device) - global_similarity  # (B, T)
        depth_loss = torch.tensor(1.0, dtype=dtype, device=device) - depth_similarity    # (B, T)
        
        # 4. è·å–çº¦æŸåçš„è‡ªé€‚åº”æƒé‡
        weight_results = self.constrained_weight_learner(action_tokens, routing_weights)
        adaptive_weights = weight_results['adaptive_weights']  # (B, T, 2)
        diversity_loss = weight_results['diversity_loss']
        
        # 5. åº”ç”¨çº¦æŸæƒé‡åŠ æƒæŸå¤±
        weighted_global_loss = adaptive_weights[:, :, 0] * global_loss  # (B, T)
        weighted_depth_loss = adaptive_weights[:, :, 1] * depth_loss    # (B, T)
        
        # ä¸»å¯¹é½æŸå¤±
        alignment_loss = (weighted_global_loss + weighted_depth_loss).mean()
        
        # 6. æ—¶åºå¹³æ»‘æ€§æ­£åˆ™åŒ–
        smoothness_loss = torch.tensor(0.0, device=device, dtype=dtype)
        if T > 1:
            weight_diff = torch.diff(adaptive_weights, dim=1)  # (B, T-1, 2)
            smoothness_loss = torch.mean(weight_diff ** 2)
        
        # 7. æ€»æŸå¤±ç»„åˆ
        total_loss = (
            alignment_loss + 
            torch.tensor(0.05, dtype=dtype, device=device) * diversity_loss +      # é¼“åŠ±æƒé‡å¤šæ ·æ€§
            torch.tensor(0.02, dtype=dtype, device=device) * smoothness_loss       # æ—¶åºå¹³æ»‘æ€§
        )
        
        # 8. æ„å»ºè¯¦ç»†æŒ‡æ ‡
        metrics_dict = {
            # ä¸»è¦æŸå¤±ç»„ä»¶
            'alignment_loss': alignment_loss.item(),
            'diversity_loss': diversity_loss.item(),
            'smoothness_loss': smoothness_loss.item(),
            'total_constrained_loss': total_loss.item(),
            
            # åŸºç¡€æŸå¤±ï¼ˆæœªåŠ æƒï¼‰
            'raw_global_loss': global_loss.mean().item(),
            'raw_depth_loss': depth_loss.mean().item(),
            
            # æƒé‡å­¦ä¹ å™¨çŠ¶æ€
            'weight_temperature': weight_results['temperature'],
            'depth_preference_ratio': weight_results['depth_preference_ratio'],
            
            # æƒé‡ç»Ÿè®¡
            'constrained_avg_global_weight': adaptive_weights[:, :, 0].mean().item(),
            'constrained_avg_depth_weight': adaptive_weights[:, :, 1].mean().item(),
            'constrained_global_weight_std': adaptive_weights[:, :, 0].std().item(),
            'constrained_depth_weight_std': adaptive_weights[:, :, 1].std().item(),
            
            # è·¯ç”±æƒé‡ç»Ÿè®¡ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
            'routing_avg_global_weight': routing_weights[:, :, 0].mean().item(),
            'routing_avg_depth_weight': routing_weights[:, :, 1].mean().item(),
            
            # å­˜å‚¨æƒé‡ç”¨äºåç»­åˆ†æ
            'adaptive_weights_tensor': adaptive_weights.detach(),
            'routing_weights_tensor': routing_weights.detach(),
        }
        
        # 9. å¦‚æœæœ‰å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼Œè¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æ
        if critical_labels is not None:
            print(f"ğŸ” å¼€å§‹åˆ†ç±»ç»Ÿè®¡åˆ†æ")
            critical_mask = critical_labels.bool()
            
            print(f"ğŸ” critical_mask shape: {critical_mask.shape}")
            print(f"ğŸ” critical_mask sum: {critical_mask.sum().item()}")
            print(f"ğŸ” adaptive_weights shape: {adaptive_weights.shape}")
            
            if critical_mask.any():
                critical_adaptive_weights = adaptive_weights[critical_mask]
                critical_routing_weights = routing_weights[critical_mask]
                
                print(f"ğŸ” critical_adaptive_weights shape: {critical_adaptive_weights.shape}")
                print(f"ğŸ” å…³é”®æ—¶é—´æ®µæƒé‡ - å…¨å±€: {critical_adaptive_weights[:, 0].mean().item():.3f}")
                print(f"ğŸ” å…³é”®æ—¶é—´æ®µæƒé‡ - æ·±åº¦: {critical_adaptive_weights[:, 1].mean().item():.3f}")
                
                metrics_dict.update({
                    'critical_constrained_global': critical_adaptive_weights[:, 0].mean().item(),
                    'critical_constrained_depth': critical_adaptive_weights[:, 1].mean().item(),
                    'critical_routing_global': critical_routing_weights[:, 0].mean().item(),
                    'critical_routing_depth': critical_routing_weights[:, 1].mean().item(),
                })
                print(f"ğŸ” å·²æ·»åŠ å…³é”®æ—¶é—´æ®µæŒ‡æ ‡åˆ°metrics_dict")
            else:
                print(f"âš ï¸ æ²¡æœ‰å…³é”®æ—¶é—´æ®µ")
            
            non_critical_mask = ~critical_mask
            print(f"ğŸ” non_critical_mask sum: {non_critical_mask.sum().item()}")
            
            if non_critical_mask.any():
                non_critical_adaptive_weights = adaptive_weights[non_critical_mask]
                non_critical_routing_weights = routing_weights[non_critical_mask]
                
                print(f"ğŸ” éå…³é”®æ—¶é—´æ®µæƒé‡ - å…¨å±€: {non_critical_adaptive_weights[:, 0].mean().item():.3f}")
                print(f"ğŸ” éå…³é”®æ—¶é—´æ®µæƒé‡ - æ·±åº¦: {non_critical_adaptive_weights[:, 1].mean().item():.3f}")
                
                metrics_dict.update({
                    'non_critical_constrained_global': non_critical_adaptive_weights[:, 0].mean().item(),
                    'non_critical_constrained_depth': non_critical_adaptive_weights[:, 1].mean().item(),
                    'non_critical_routing_global': non_critical_routing_weights[:, 0].mean().item(),
                    'non_critical_routing_depth': non_critical_routing_weights[:, 1].mean().item(),
                })
                print(f"ğŸ” å·²æ·»åŠ éå…³é”®æ—¶é—´æ®µæŒ‡æ ‡åˆ°metrics_dict")
                
                # è®¡ç®—è·¯ç”±ç½‘ç»œä¸å…³é”®æ—¶é—´æ®µæ ‡ç­¾çš„ä¸€è‡´æ€§
                critical_routing_correct = (critical_routing_weights[:, 1] > critical_routing_weights[:, 0]).float().mean()
                non_critical_routing_correct = (non_critical_routing_weights[:, 0] > non_critical_routing_weights[:, 1]).float().mean()
                
                metrics_dict.update({
                    'routing_critical_accuracy': critical_routing_correct.item(),
                    'routing_non_critical_accuracy': non_critical_routing_correct.item(),
                    'routing_overall_accuracy': (critical_routing_correct + non_critical_routing_correct).item() / 2,
                })
                print(f"ğŸ” å·²æ·»åŠ è·¯ç”±å‡†ç¡®ç‡æŒ‡æ ‡")
            else:
                print(f"âš ï¸ æ²¡æœ‰éå…³é”®æ—¶é—´æ®µ")
            
            print(f"ğŸ” metrics_dict keys: {list(metrics_dict.keys())}")
                
        return total_loss, metrics_dict

    def compute_loss(self, lang_tokens, lang_attn_mask, img_tokens, 
                     state_tokens, action_gt, action_mask, ctrl_freqs,
                     cls_token=None, depth_features=None, critical_labels=None):
        """
        è®¡ç®—æ€»æŸå¤±ï¼ŒåŒ…æ‹¬æ‰©æ•£æŸå¤±ã€è·¯ç”±æŸå¤±å’Œçº¦æŸæƒé‡å¯¹é½æŸå¤±
        """
        batch_size = lang_tokens.shape[0]
        device = lang_tokens.device

        # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_gt = action_gt.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        # æ‰©æ•£æŸå¤±è®¡ç®—
        noise = torch.randn(action_gt.shape, dtype=action_gt.dtype, device=device)
        timesteps = torch.randint(0, self.num_train_timesteps, (batch_size,), device=device).long()
        noisy_action = self.noise_scheduler.add_noise(action_gt, noise, timesteps)
        
        state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)
        action_mask = action_mask.expand(-1, state_action_traj.shape[1], -1)
        state_action_traj = torch.cat([state_action_traj, action_mask], dim=2)
        
        lang_cond, img_cond, state_action_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_action_traj)
        
        # è·å–æ¨¡å‹é¢„æµ‹å’Œä¸­é—´æ¿€æ´»
        pred, intermediate_activations = self.model(
            state_action_traj, ctrl_freqs, timesteps, lang_cond, img_cond, 
            lang_mask=lang_attn_mask
        )

        # è®¡ç®—æ‰©æ•£æŸå¤±
        pred_type = self.prediction_type 
        if pred_type == 'epsilon':
            target = noise
        elif pred_type == 'sample':
            target = action_gt
        else:
            raise ValueError(f"Unsupported prediction type {pred_type}")

        diffusion_loss = F.mse_loss(pred, target)
        
        # ğŸ†• è®¡ç®—çº¦æŸæƒé‡åŒæ•™å¸ˆREPAå¯¹é½æŸå¤±
        repa_loss = torch.tensor(0.0, device=device, dtype=diffusion_loss.dtype)
        routing_loss = torch.tensor(0.0, device=device, dtype=diffusion_loss.dtype)
        detailed_metrics = {}
        
        if self.enable_repa_loss and 'action_tokens_for_repa' in intermediate_activations:
            action_tokens = intermediate_activations['action_tokens_for_repa']
            
            if self.use_dual_teachers and cls_token is not None and depth_features is not None:
                cls_token = cls_token.to(self.dtype)
                depth_features = depth_features.to(self.dtype)
                
                # æå–æ·±åº¦CLS token
                if depth_features.shape[1] >= 1:
                    depth_cls_token = depth_features[:, 0, :]  # (B, 1024)
                else:
                    depth_cls_token = cls_token.squeeze(1)  # Fallback
                
                # è·å–è·¯ç”±æƒé‡
                routing_weights = intermediate_activations.get('routing_weights', None)
                
                if routing_weights is not None:
                    if self.enable_constrained_weights:
                        # ğŸ†• ä½¿ç”¨çº¦æŸæƒé‡æ¨¡å¼
                        repa_loss, detailed_metrics = self.compute_constrained_dual_teacher_alignment_loss(
                            action_tokens, 
                            cls_token,       
                            depth_cls_token, 
                            routing_weights,
                            critical_labels
                        )
                    else:
                        # åŸå§‹åŒæ•™å¸ˆæ¨¡å¼
                        repa_loss, routing_loss, loss_dict = self.compute_dual_teacher_alignment_loss(
                            action_tokens, 
                            cls_token,       
                            depth_cls_token, 
                            routing_weights,
                            critical_labels
                        )
                        detailed_metrics = loss_dict
                    
                    # è·¯ç”±æŸå¤±ï¼ˆç›‘ç£è·¯ç”±ç½‘ç»œå­¦ä¹ ï¼‰
                    if critical_labels is not None:
                        target_routing = torch.zeros_like(routing_weights)
                        target_routing[:, :, 0] = 1 - critical_labels.float()  # å…¨å±€ä¸“å®¶æƒé‡
                        target_routing[:, :, 1] = critical_labels.float()      # æ·±åº¦ä¸“å®¶æƒé‡
                        
                        routing_loss = F.binary_cross_entropy(routing_weights, target_routing, reduction='mean')
                        detailed_metrics['routing_supervision_loss'] = routing_loss.item()
        
        # æ€»æŸå¤±
        total_loss = (
            diffusion_loss + 
            self.repa_loss_weight * repa_loss + 
            self.routing_loss_weight * routing_loss
        )
        
        return total_loss, diffusion_loss, repa_loss, routing_loss, detailed_metrics

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜...
    def conditional_sample(self, lang_cond, lang_attn_mask, img_cond, state_traj, action_mask, ctrl_freqs):
        """æ¨ç†æ—¶çš„æ¡ä»¶é‡‡æ ·ï¼Œä¸æ¶‰åŠå¯¹é½æœºåˆ¶"""
        device = state_traj.device
        dtype = state_traj.dtype
        noisy_action = torch.randn(
            size=(state_traj.shape[0], self.pred_horizon, self.action_dim), 
            dtype=dtype, device=device)
        action_mask = action_mask.expand(-1, self.pred_horizon, -1)
    
        self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)
        
        for t in self.noise_scheduler_sample.timesteps:
            action_traj = torch.cat([noisy_action, action_mask], dim=2)
            action_traj = self.state_adaptor(action_traj)
            state_action_traj = torch.cat([state_traj, action_traj], dim=1)
            
            model_output, _ = self.model(state_action_traj, ctrl_freqs,
                                        t.unsqueeze(-1).to(device),
                                        lang_cond, img_cond, lang_mask=lang_attn_mask)
            
            noisy_action = self.noise_scheduler_sample.step(
                model_output, t, noisy_action).prev_sample
            noisy_action = noisy_action.to(state_traj.dtype)
        
        noisy_action = noisy_action * action_mask
        return noisy_action

    def build_condition_adapter(self, projector_type, in_features, out_features):
        """æ„å»ºæ¡ä»¶é€‚é…å™¨"""
        projector = None
        if projector_type == 'linear':
            projector = nn.Linear(in_features, out_features)
        else:
            mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu', projector_type)
            if mlp_gelu_match:
                mlp_depth = int(mlp_gelu_match.group(1))
                modules = [nn.Linear(in_features, out_features)]
                for _ in range(1, mlp_depth):
                    modules.append(nn.GELU(approximate="tanh"))
                    modules.append(nn.Linear(out_features, out_features))
                projector = nn.Sequential(*modules)
        if projector is None:
            raise ValueError(f'Unknown projector type: {projector_type}')
        return projector

    def adapt_conditions(self, lang_tokens, img_tokens, state_tokens):
        """é€‚é…æ¡ä»¶è¾“å…¥"""
        adapted_lang = self.lang_adaptor(lang_tokens)
        adapted_img = self.img_adaptor(img_tokens)
        adapted_state = self.state_adaptor(state_tokens)
        return adapted_lang, adapted_img, adapted_state

    def predict_action(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens,
                    action_mask, ctrl_freqs, vision_features=None):
        """é¢„æµ‹åŠ¨ä½œï¼Œæ¨ç†æ—¶ä¸éœ€è¦è§†è§‰å¯¹é½ç‰¹å¾"""
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        state_tokens = torch.cat([state_tokens, action_mask], dim=2)
        lang_cond, img_cond, state_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_tokens)
        
        action_pred = self.conditional_sample(
            lang_cond, lang_attn_mask, img_cond, 
            state_traj, action_mask, ctrl_freqs,
        )
        
        return action_pred

    def forward(self, *args, **kwargs) -> torch.Tensor:
        """ä¿æŒå…¼å®¹æ€§ï¼Œåªè¿”å›æ€»æŸå¤±"""
        result = self.compute_loss(*args, **kwargs)
        if isinstance(result, tuple) and len(result) >= 1:
            return result[0]  # åªè¿”å›total_loss
        return result