# import re
# from pathlib import Path

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
# from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler

# from models.hub_mixin import CompatiblePyTorchModelHubMixin
# from models.rdt.model import RDT
# from models.rdt.binary_soft_routing import SimpleDualTeacherModel


# class RDTRunner(nn.Module, CompatiblePyTorchModelHubMixin, 
#                repo_url="https://huggingface.co/robotics-diffusion-transformer/rdt-1b"):
#     """
#     é›†æˆè½¯è·¯ç”±åŒæ•™å¸ˆREPAå¯¹é½æŸå¤±çš„RDTè¿è¡Œå™¨
    
#     æ ¸å¿ƒåˆ›æ–°ï¼š
#     1. åŸºäºå…³é”®æ—¶é—´æ®µäºŒå…ƒæ ‡ç­¾çš„è½¯è·¯ç”±æƒé‡åˆ†é…
#     2. è§„åˆ™é©±åŠ¨çš„æƒé‡æ˜ å°„ï¼šå…³é”®æ—¶é—´æ®µåå‘æ·±åº¦ï¼Œéå…³é”®æ—¶é—´æ®µåå‘å…¨å±€
#     3. å¯é€‰çš„ç¥ç»ç½‘ç»œå¾®è°ƒå’Œæ—¶åºå¹³æ»‘
#     4. å®Œæ•´çš„å¯¹æ¯”å­¦ä¹ å’Œç»Ÿè®¡åˆ†æ
#     """
#     def __init__(self, *, action_dim, pred_horizon, config, 
#                  lang_token_dim, img_token_dim, state_token_dim, 
#                  max_lang_cond_len, img_cond_len, lang_pos_embed_config=None, 
#                  img_pos_embed_config=None, dtype=torch.bfloat16,
#                  # è½¯è·¯ç”±åŒæ•™å¸ˆREPAå‚æ•°
#                  enable_soft_routing_repa=True, 
#                  soft_routing_repa_weight=0.2,
#                  dinov2_feature_dim=1024,
#                  depth_feature_dim=1024,
#                  # è½¯è·¯ç”±é…ç½®
#                  soft_routing_config=None):
#         super(RDTRunner, self).__init__()
        
#         # è½¯è·¯ç”±åŒæ•™å¸ˆREPAé…ç½®
#         self.enable_soft_routing_repa = enable_soft_routing_repa
#         self.soft_routing_repa_weight = soft_routing_repa_weight
#         self.dtype = dtype
#         self.dinov2_feature_dim = dinov2_feature_dim
#         self.depth_feature_dim = depth_feature_dim
        
#         print(f"ğŸ”§ è½¯è·¯ç”±åŒæ•™å¸ˆRDTRunneråˆå§‹åŒ–:")
#         print(f"   - è½¯è·¯ç”±REPAæŸå¤±å¯ç”¨: {enable_soft_routing_repa}")
#         print(f"   - è½¯è·¯ç”±REPAæŸå¤±æƒé‡: {soft_routing_repa_weight}")
#         print(f"   - DINOv2ç‰¹å¾ç»´åº¦: {dinov2_feature_dim}")
#         print(f"   - æ·±åº¦ç‰¹å¾ç»´åº¦: {depth_feature_dim}")
#         print(f"   - æ•°æ®ç±»å‹: {dtype}")
        
#         # åˆ›å»ºæ‰©æ•£æ¨¡å‹
#         hidden_size = config['rdt']['hidden_size']
#         self.model = RDT(
#             output_dim=action_dim,
#             horizon=pred_horizon,
#             hidden_size=hidden_size,
#             depth=config['rdt']['depth'], 
#             num_heads=config['rdt']['num_heads'],
#             max_lang_cond_len=max_lang_cond_len,
#             img_cond_len=img_cond_len,
#             lang_pos_embed_config=lang_pos_embed_config,
#             img_pos_embed_config=img_pos_embed_config,
#             dtype=dtype,
#             enable_repa_loss=enable_soft_routing_repa,
#             repa_activation_layer=21,  # åœ¨ç¬¬21å±‚æå–åŠ¨ä½œtokens
#             dinov2_feature_dim=dinov2_feature_dim,
#             depth_feature_dim=depth_feature_dim,
#         )

#         # é€‚é…å™¨åˆ›å»º
#         self.lang_adaptor = self.build_condition_adapter(
#             config['lang_adaptor'], 
#             in_features=lang_token_dim, 
#             out_features=hidden_size
#         )
#         self.img_adaptor = self.build_condition_adapter(
#             config['img_adaptor'], 
#             in_features=img_token_dim, 
#             out_features=hidden_size
#         )
#         self.state_adaptor = self.build_condition_adapter(
#             config['state_adaptor'], 
#             in_features=state_token_dim * 2,    
#             out_features=hidden_size
#         )
        
#         # è½¬æ¢é€‚é…å™¨åˆ°æ­£ç¡®çš„æ•°æ®ç±»å‹
#         self.lang_adaptor = self.lang_adaptor.to(dtype)
#         self.img_adaptor = self.img_adaptor.to(dtype)
#         self.state_adaptor = self.state_adaptor.to(dtype)
        
#         # ğŸ†• åˆ›å»ºè½¯è·¯ç”±åŒæ•™å¸ˆå¯¹é½æ¨¡å‹
#         if self.enable_soft_routing_repa:
#             # è®¾ç½®é»˜è®¤è½¯è·¯ç”±é…ç½®
#             default_soft_routing_config = {
#                 'action_dim': hidden_size,
#                 'dinov2_dim': dinov2_feature_dim,
#                 'depth_dim': depth_feature_dim,
#                 'router_config': {
#                     'action_dim': hidden_size,
#                     'critical_global_weight': 0.25,      # å…³é”®æ—¶é—´æ®µï¼šå…¨å±€25%ï¼Œæ·±åº¦75%
#                     'critical_depth_weight': 0.75,
#                     'non_critical_global_weight': 0.75,  # éå…³é”®æ—¶é—´æ®µï¼šå…¨å±€75%ï¼Œæ·±åº¦25%
#                     'non_critical_depth_weight': 0.25,
#                     'enable_neural_adjustment': True,    # å¯ç”¨ç¥ç»ç½‘ç»œå¾®è°ƒ
#                     'adjustment_strength': 0.1,          # å¾®è°ƒå¼ºåº¦
#                     'temporal_smoothing': 0.9,           # æ—¶åºå¹³æ»‘ç³»æ•°
#                     'temperature': 1.0,                  # softmaxæ¸©åº¦
#                 }
#             }
            
#             # åˆå¹¶ç”¨æˆ·é…ç½®
#             if soft_routing_config:
#                 default_soft_routing_config.update(soft_routing_config)
#                 if 'router_config' in soft_routing_config:
#                     default_soft_routing_config['router_config'].update(soft_routing_config['router_config'])
            
#             self.soft_routing_config = default_soft_routing_config
#             self.dual_teacher_model = SimpleDualTeacherModel(**default_soft_routing_config)
#             self.dual_teacher_model.to(dtype)
            
#             print(f"ğŸ¯ è½¯è·¯ç”±åŒæ•™å¸ˆå¯¹é½æ¨¡å‹é…ç½®:")
#             router_config = default_soft_routing_config['router_config']
#             print(f"   - å…³é”®æ—¶é—´æ®µæƒé‡: å…¨å±€{router_config['critical_global_weight']:.2f}, æ·±åº¦{router_config['critical_depth_weight']:.2f}")
#             print(f"   - éå…³é”®æ—¶é—´æ®µæƒé‡: å…¨å±€{router_config['non_critical_global_weight']:.2f}, æ·±åº¦{router_config['non_critical_depth_weight']:.2f}")
#             print(f"   - ç¥ç»ç½‘ç»œå¾®è°ƒ: {router_config['enable_neural_adjustment']}")
#             print(f"   - æ—¶åºå¹³æ»‘ç³»æ•°: {router_config['temporal_smoothing']}")
#             print(f"   - å¾®è°ƒå¼ºåº¦: {router_config['adjustment_strength']}")
        
#         # å™ªå£°è°ƒåº¦å™¨åˆ›å»º
#         noise_scheduler_config = config['noise_scheduler']
#         self.noise_scheduler = DDPMScheduler(
#             num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
#             beta_schedule=noise_scheduler_config['beta_schedule'],
#             prediction_type=noise_scheduler_config['prediction_type'],
#             clip_sample=noise_scheduler_config['clip_sample'],
#         )
#         self.noise_scheduler_sample = DPMSolverMultistepScheduler(
#             num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
#             beta_schedule=noise_scheduler_config['beta_schedule'],
#             prediction_type=noise_scheduler_config['prediction_type'],
#         )

#         self.num_train_timesteps = noise_scheduler_config['num_train_timesteps']
#         self.num_inference_timesteps = noise_scheduler_config['num_inference_timesteps']
#         self.prediction_type = noise_scheduler_config['prediction_type']

#         self.pred_horizon = pred_horizon
#         self.action_dim = action_dim
        
#         # ç”¨äºè¿½è¸ªè®­ç»ƒçŠ¶æ€
#         self.training_step = 0
#         self.batch_count = 0

#         print("Diffusion params: %e" % sum(
#             [p.numel() for p in self.model.parameters()] + 
#             [p.numel() for p in self.lang_adaptor.parameters()] + 
#             [p.numel() for p in self.img_adaptor.parameters()] + 
#             [p.numel() for p in self.state_adaptor.parameters()]))
        
#         if self.enable_soft_routing_repa:
#             dual_teacher_params = sum(p.numel() for p in self.dual_teacher_model.parameters())
#             print(f"Soft routing dual teacher params: {dual_teacher_params:e}")

#     def compute_soft_routing_dual_teacher_repa_loss(self, action_tokens, dinov2_cls_token, 
#                                                    depth_cls_token, critical_labels):
#         """
#         è®¡ç®—åŸºäºè½¯è·¯ç”±çš„åŒæ•™å¸ˆREPAå¯¹é½æŸå¤±
        
#         Args:
#             action_tokens: (B, T, hidden_size) åŠ¨ä½œtokens
#             dinov2_cls_token: (B, 1, dinov2_dim) æˆ– (B, dinov2_dim) DINOv2å…¨å±€ç‰¹å¾
#             depth_cls_token: (B, depth_dim) æ·±åº¦ç‰¹å¾
#             critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼ˆ0/1ï¼‰
        
#         Returns:
#             total_loss: æ€»æŸå¤±
#             detailed_metrics: è¯¦ç»†æŒ‡æ ‡å­—å…¸
#         """
#         B, T, hidden_size = action_tokens.shape
#         device = action_tokens.device
#         dtype = action_tokens.dtype
        
#         # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®
#         if dinov2_cls_token.dim() == 3:
#             dinov2_cls_squeezed = dinov2_cls_token.squeeze(1)  # (B, dinov2_dim)
#         else:
#             dinov2_cls_squeezed = dinov2_cls_token
        
#         if depth_cls_token.dim() == 3:
#             depth_cls_squeezed = depth_cls_token.squeeze(1)  # (B, depth_dim)
#         else:
#             depth_cls_squeezed = depth_cls_token
        
#         # ç¡®ä¿å…³é”®æ—¶é—´æ®µæ ‡ç­¾ç±»å‹æ­£ç¡®
#         if critical_labels.dtype != torch.long:
#             critical_labels = critical_labels.long()
        
#         # åˆ¤æ–­æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªbatchï¼ˆç”¨äºæ—¶åºå¹³æ»‘ï¼‰
#         is_first_batch = (self.batch_count == 0)
        
#         try:
#             # ä½¿ç”¨è½¯è·¯ç”±åŒæ•™å¸ˆæ¨¡å‹è®¡ç®—å¯¹é½æŸå¤±
#             results = self.dual_teacher_model(
#                 action_tokens=action_tokens,
#                 dinov2_features=dinov2_cls_squeezed,
#                 depth_features=depth_cls_squeezed,
#                 critical_labels=critical_labels,
#                 is_first_batch=is_first_batch
#             )
            
#             # æå–æŸå¤±å’Œç»Ÿè®¡ä¿¡æ¯
#             total_loss = results['total_loss']
            
#             # æ„å»ºè¯¦ç»†æŒ‡æ ‡
#             detailed_metrics = {
#                 # ä¸»è¦æŸå¤±ç»„ä»¶
#                 'soft_routing_total_loss': total_loss.item(),
#                 'soft_routing_alignment_loss': results['alignment_loss'].item(),
#                 'soft_routing_contrastive_loss': results['contrastive_loss'].item(),
                
#                 # åŸå§‹æŸå¤±ï¼ˆæœªåŠ æƒï¼‰
#                 'global_loss_raw': results['global_loss_raw'].item(),
#                 'depth_loss_raw': results['depth_loss_raw'].item(),
                
#                 # åŠ æƒæŸå¤±
#                 'weighted_global_loss': results['weighted_global_loss'].item(),
#                 'weighted_depth_loss': results['weighted_depth_loss'].item(),
                
#                 # ç›¸ä¼¼åº¦æŒ‡æ ‡
#                 'global_similarity_avg': results['global_similarity_avg'].item(),
#                 'depth_similarity_avg': results['depth_similarity_avg'].item(),
                
#                 # æ¸©åº¦å‚æ•°
#                 'alignment_temperature': results['alignment_temperature'],
#             }
            
#             # è·¯ç”±ç»Ÿè®¡ä¿¡æ¯
#             router_stats = results['router_statistics']
#             detailed_metrics.update({
#                 'critical_ratio': router_stats['critical_ratio'],
#                 'avg_global_weight': router_stats['avg_global_weight'],
#                 'avg_depth_weight': router_stats['avg_depth_weight'],
#                 'weight_std_global': router_stats['weight_std_global'],
#                 'weight_std_depth': router_stats['weight_std_depth'],
#                 'routing_temperature': router_stats['temperature'],
#             })
            
#             # åˆ†ç±»ç»Ÿè®¡
#             if 'critical_avg_global' in router_stats:
#                 detailed_metrics.update({
#                     'critical_avg_global_weight': router_stats['critical_avg_global'],
#                     'critical_avg_depth_weight': router_stats['critical_avg_depth'],
#                 })
            
#             if 'non_critical_avg_global' in router_stats:
#                 detailed_metrics.update({
#                     'non_critical_avg_global_weight': router_stats['non_critical_avg_global'],
#                     'non_critical_avg_depth_weight': router_stats['non_critical_avg_depth'],
#                 })
            
#             # å¾®è°ƒç»Ÿè®¡
#             if 'weight_drift' in router_stats:
#                 detailed_metrics['weight_drift'] = router_stats['weight_drift']
            
#             # å­˜å‚¨è·¯ç”±æƒé‡ç”¨äºåˆ†æ
#             detailed_metrics['routing_weights_tensor'] = results['routing_weights'].detach()
#             detailed_metrics['base_weights_tensor'] = results['base_weights'].detach()
            
#             return total_loss, detailed_metrics
            
#         except Exception as e:
#             print(f"âš ï¸ è½¯è·¯ç”±åŒæ•™å¸ˆREPAæŸå¤±è®¡ç®—å¤±è´¥: {e}")
#             import traceback
#             traceback.print_exc()
            
#             # è¿”å›é›¶æŸå¤±å’ŒåŸºç¡€æŒ‡æ ‡
#             zero_loss = torch.tensor(0.0, device=device, dtype=dtype)
#             fallback_metrics = {
#                 'soft_routing_total_loss': 0.0,
#                 'soft_routing_alignment_loss': 0.0,
#                 'error': str(e),
#             }
#             return zero_loss, fallback_metrics

#     def compute_loss(self, lang_tokens, lang_attn_mask, img_tokens, 
#                      state_tokens, action_gt, action_mask, ctrl_freqs,
#                      cls_token=None, depth_features=None, critical_labels=None):
#         """
#         è®¡ç®—æ€»æŸå¤±ï¼ŒåŒ…æ‹¬æ‰©æ•£æŸå¤±å’Œè½¯è·¯ç”±åŒæ•™å¸ˆREPAæŸå¤±
#         """
#         batch_size = lang_tokens.shape[0]
#         device = lang_tokens.device

#         # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½è½¬æ¢ä¸ºæ­£ç¡®çš„æ•°æ®ç±»å‹
#         lang_tokens = lang_tokens.to(self.dtype)
#         img_tokens = img_tokens.to(self.dtype)
#         state_tokens = state_tokens.to(self.dtype)
#         action_gt = action_gt.to(self.dtype)
#         action_mask = action_mask.to(self.dtype)
        
#         # æ‰©æ•£æŸå¤±è®¡ç®—
#         noise = torch.randn(action_gt.shape, dtype=action_gt.dtype, device=device)
#         timesteps = torch.randint(0, self.num_train_timesteps, (batch_size,), device=device).long()
#         noisy_action = self.noise_scheduler.add_noise(action_gt, noise, timesteps)
        
#         state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)
#         action_mask = action_mask.expand(-1, state_action_traj.shape[1], -1)
#         state_action_traj = torch.cat([state_action_traj, action_mask], dim=2)
        
#         lang_cond, img_cond, state_action_traj = self.adapt_conditions(
#             lang_tokens, img_tokens, state_action_traj)
        
#         # è·å–æ¨¡å‹é¢„æµ‹å’Œä¸­é—´æ¿€æ´»
#         pred, intermediate_activations = self.model(
#             state_action_traj, ctrl_freqs, timesteps, lang_cond, img_cond, 
#             lang_mask=lang_attn_mask
#         )

#         # è®¡ç®—æ‰©æ•£æŸå¤±
#         pred_type = self.prediction_type 
#         if pred_type == 'epsilon':
#             target = noise
#         elif pred_type == 'sample':
#             target = action_gt
#         else:
#             raise ValueError(f"Unsupported prediction type {pred_type}")

#         diffusion_loss = F.mse_loss(pred, target)
        
#         # ğŸ†• è®¡ç®—è½¯è·¯ç”±åŒæ•™å¸ˆREPAå¯¹é½æŸå¤±
#         repa_loss = torch.tensor(0.0, device=device, dtype=diffusion_loss.dtype)
#         detailed_metrics = {}
        
#         if (self.enable_soft_routing_repa and 
#             'action_tokens_for_repa' in intermediate_activations and
#             cls_token is not None and 
#             depth_features is not None and 
#             critical_labels is not None):
            
#             action_tokens = intermediate_activations['action_tokens_for_repa']
#             cls_token = cls_token.to(self.dtype)
#             depth_features = depth_features.to(self.dtype)
            
#             # æå–æ·±åº¦CLS token
#             if depth_features.shape[1] >= 1:
#                 depth_cls_token = depth_features[:, 0, :]  # (B, depth_dim)
#             else:
#                 depth_cls_token = cls_token.squeeze(1) if cls_token.dim() == 3 else cls_token
            
#             # è®¡ç®—è½¯è·¯ç”±åŒæ•™å¸ˆå¯¹é½æŸå¤±
#             repa_loss, detailed_metrics = self.compute_soft_routing_dual_teacher_repa_loss(
#                 action_tokens=action_tokens,
#                 dinov2_cls_token=cls_token,
#                 depth_cls_token=depth_cls_token,
#                 critical_labels=critical_labels
#             )
        
#         # æ€»æŸå¤±
#         total_loss = diffusion_loss + self.soft_routing_repa_weight * repa_loss
        
#         # æ›´æ–°è®­ç»ƒçŠ¶æ€
#         self.training_step += 1
#         self.batch_count += 1
        
#         return total_loss, diffusion_loss, repa_loss, detailed_metrics

#     def conditional_sample(self, lang_cond, lang_attn_mask, img_cond, state_traj, action_mask, ctrl_freqs):
#         """æ¨ç†æ—¶çš„æ¡ä»¶é‡‡æ ·ï¼Œä¸æ¶‰åŠå¯¹é½æœºåˆ¶"""
#         device = state_traj.device
#         dtype = state_traj.dtype
#         noisy_action = torch.randn(
#             size=(state_traj.shape[0], self.pred_horizon, self.action_dim), 
#             dtype=dtype, device=device)
#         action_mask = action_mask.expand(-1, self.pred_horizon, -1)
    
#         self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)
        
#         for t in self.noise_scheduler_sample.timesteps:
#             action_traj = torch.cat([noisy_action, action_mask], dim=2)
#             action_traj = self.state_adaptor(action_traj)
#             state_action_traj = torch.cat([state_traj, action_traj], dim=1)
            
#             model_output, _ = self.model(state_action_traj, ctrl_freqs,
#                                         t.unsqueeze(-1).to(device),
#                                         lang_cond, img_cond, lang_mask=lang_attn_mask)
            
#             noisy_action = self.noise_scheduler_sample.step(
#                 model_output, t, noisy_action).prev_sample
#             noisy_action = noisy_action.to(state_traj.dtype)
        
#         noisy_action = noisy_action * action_mask
#         return noisy_action

#     def build_condition_adapter(self, projector_type, in_features, out_features):
#         """æ„å»ºæ¡ä»¶é€‚é…å™¨"""
#         projector = None
#         if projector_type == 'linear':
#             projector = nn.Linear(in_features, out_features)
#         else:
#             mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu', projector_type)
#             if mlp_gelu_match:
#                 mlp_depth = int(mlp_gelu_match.group(1))
#                 modules = [nn.Linear(in_features, out_features)]
#                 for _ in range(1, mlp_depth):
#                     modules.append(nn.GELU(approximate="tanh"))
#                     modules.append(nn.Linear(out_features, out_features))
#                 projector = nn.Sequential(*modules)
#         if projector is None:
#             raise ValueError(f'Unknown projector type: {projector_type}')
#         return projector

#     def adapt_conditions(self, lang_tokens, img_tokens, state_tokens):
#         """é€‚é…æ¡ä»¶è¾“å…¥"""
#         adapted_lang = self.lang_adaptor(lang_tokens)
#         adapted_img = self.img_adaptor(img_tokens)
#         adapted_state = self.state_adaptor(state_tokens)
#         return adapted_lang, adapted_img, adapted_state

#     def predict_action(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens,
#                     action_mask, ctrl_freqs, vision_features=None):
#         """é¢„æµ‹åŠ¨ä½œï¼Œæ¨ç†æ—¶ä¸éœ€è¦è§†è§‰å¯¹é½ç‰¹å¾"""
#         lang_tokens = lang_tokens.to(self.dtype)
#         img_tokens = img_tokens.to(self.dtype)
#         state_tokens = state_tokens.to(self.dtype)
#         action_mask = action_mask.to(self.dtype)
        
#         state_tokens = torch.cat([state_tokens, action_mask], dim=2)
#         lang_cond, img_cond, state_traj = self.adapt_conditions(
#             lang_tokens, img_tokens, state_tokens)
        
#         action_pred = self.conditional_sample(
#             lang_cond, lang_attn_mask, img_cond, 
#             state_traj, action_mask, ctrl_freqs,
#         )
        
#         return action_pred

#     def reset_batch_count(self):
#         """é‡ç½®batchè®¡æ•°å™¨ï¼ˆç”¨äºæ–°epochï¼‰"""
#         self.batch_count = 0

#     def get_soft_routing_statistics(self):
#         """è·å–è½¯è·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
#         if not self.enable_soft_routing_repa:
#             return {}
        
#         stats = {
#             'training_step': self.training_step,
#             'batch_count': self.batch_count,
#             'soft_routing_config': self.soft_routing_config,
#         }
        
#         # å¦‚æœæœ‰è·¯ç”±å™¨ï¼Œè·å–å…¶ç»Ÿè®¡ä¿¡æ¯
#         if hasattr(self.dual_teacher_model, 'soft_router'):
#             router = self.dual_teacher_model.soft_router
#             stats.update({
#                 'routing_temperature': router.temperature.item(),
#                 'enable_neural_adjustment': router.enable_neural_adjustment,
#                 'temporal_smoothing': router.temporal_smoothing,
#                 'adjustment_strength': router.adjustment_strength,
#             })
        
#         return stats

#     def forward(self, *args, **kwargs) -> torch.Tensor:
#         """ä¿æŒå…¼å®¹æ€§ï¼Œåªè¿”å›æ€»æŸå¤±"""
#         result = self.compute_loss(*args, **kwargs)
#         if isinstance(result, tuple) and len(result) >= 1:
#             return result[0]  # åªè¿”å›total_loss
#         return result
    
    
    
#è¾“å…¥æµ‹èåˆç‰ˆ  
import re
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler

from models.hub_mixin import CompatiblePyTorchModelHubMixin
from models.rdt.model import RDT
from models.rdt.binary_soft_routing import SimpleDualTeacherModel
from models.multimodal_encoder.vision_fusion_module import create_vision_fusion_module

class RDTRunner(nn.Module, CompatiblePyTorchModelHubMixin):
    """
    RDTè¿è¡Œå™¨ - é›†æˆè§†è§‰ç‰¹å¾èåˆæ¨¡å—
    """
    def __init__(
        self, 
        *,
        action_dim, 
        pred_horizon, 
        config, 
        lang_token_dim, 
        img_token_dim, 
        state_token_dim, 
        max_lang_cond_len, 
        img_cond_len, 
        lang_pos_embed_config=None, 
        img_pos_embed_config=None, 
        dtype=torch.bfloat16,
        # ğŸ†• è§†è§‰èåˆé…ç½®
        enable_vision_fusion=True,
        vision_fusion_type="cross_attention",
        dinov2_feature_dim=1024,
        depth_feature_dim=1024,
        fusion_num_heads=8,
        fusion_dropout=0.1,
        img_history_size=2,  # ğŸ†• ä»configä¼ å…¥
        num_cameras=3,       # ğŸ†• ä»configä¼ å…¥
    ):
        super(RDTRunner, self).__init__()
        
        self.dtype = dtype
        self.enable_vision_fusion = enable_vision_fusion
        self.dinov2_feature_dim = dinov2_feature_dim
        self.depth_feature_dim = depth_feature_dim
        self.img_history_size = img_history_size
        self.num_cameras = num_cameras
        
        print(f"ğŸ”§ RDTRunneråˆå§‹åŒ– (è§†è§‰èåˆæ¨¡å¼):")
        print(f"   - è§†è§‰èåˆå¯ç”¨: {enable_vision_fusion}")
        print(f"   - èåˆç±»å‹: {vision_fusion_type}")
        print(f"   - å›¾åƒå†å²: {img_history_size}å¸§")
        print(f"   - ç›¸æœºæ•°é‡: {num_cameras}ä¸ª")
        print(f"   - SigLIPæ€»tokens: {img_history_size * num_cameras * 729}")
        print(f"   - DINOv2ç‰¹å¾ç»´åº¦: {dinov2_feature_dim}")
        print(f"   - æ·±åº¦ç‰¹å¾ç»´åº¦: {depth_feature_dim}")
        
        # åˆ›å»ºåŸå§‹RDTæ¨¡å‹ï¼ˆä¸å«REPAï¼‰
        hidden_size = config['rdt']['hidden_size']
        self.model = RDT(
            output_dim=action_dim,
            horizon=pred_horizon,
            hidden_size=hidden_size,
            depth=config['rdt']['depth'], 
            num_heads=config['rdt']['num_heads'],
            max_lang_cond_len=max_lang_cond_len,
            img_cond_len=img_cond_len,
            lang_pos_embed_config=lang_pos_embed_config,
            img_pos_embed_config=img_pos_embed_config,
            dtype=dtype,
            # ğŸ”´ å…³é—­REPA
            enable_repa_loss=False,
        )

        # é€‚é…å™¨
        self.lang_adaptor = self.build_condition_adapter(
            config['lang_adaptor'], 
            in_features=lang_token_dim, 
            out_features=hidden_size
        )
        self.img_adaptor = self.build_condition_adapter(
            config['img_adaptor'], 
            in_features=img_token_dim,
            out_features=hidden_size
        )
        self.state_adaptor = self.build_condition_adapter(
            config['state_adaptor'], 
            in_features=state_token_dim * 2,    
            out_features=hidden_size
        )
        
        self.lang_adaptor = self.lang_adaptor.to(dtype)
        self.img_adaptor = self.img_adaptor.to(dtype)
        self.state_adaptor = self.state_adaptor.to(dtype)
        
        # ğŸ†• åˆ›å»ºè§†è§‰ç‰¹å¾èåˆæ¨¡å—
        if self.enable_vision_fusion:
            self.vision_fusion_module = create_vision_fusion_module(
                fusion_type=vision_fusion_type,
                siglip_dim=img_token_dim,
                dinov2_dim=dinov2_feature_dim,
                depth_dim=depth_feature_dim,
                img_history_size=img_history_size,
                num_cameras=num_cameras,
                num_heads=fusion_num_heads,
                dropout=fusion_dropout
            )
            self.vision_fusion_module.to(dtype)
            
            fusion_params = sum(p.numel() for p in self.vision_fusion_module.parameters())
            print(f"   - è§†è§‰èåˆæ¨¡å—å‚æ•°é‡: {fusion_params:,}")
        else:
            self.vision_fusion_module = None

        # å™ªå£°è°ƒåº¦å™¨
        noise_scheduler_config = config['noise_scheduler']
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
            clip_sample=noise_scheduler_config['clip_sample'],
        )
        self.noise_scheduler_sample = DPMSolverMultistepScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
        )

        self.num_train_timesteps = noise_scheduler_config['num_train_timesteps']
        self.num_inference_timesteps = noise_scheduler_config['num_inference_timesteps']
        self.prediction_type = noise_scheduler_config['prediction_type']
        self.pred_horizon = pred_horizon
        self.action_dim = action_dim

        print("Diffusion params: %e" % sum(
            [p.numel() for p in self.model.parameters()] + 
            [p.numel() for p in self.lang_adaptor.parameters()] + 
            [p.numel() for p in self.img_adaptor.parameters()] + 
            [p.numel() for p in self.state_adaptor.parameters()]))
        
    def compute_soft_routing_dual_teacher_repa_loss(self, action_tokens, dinov2_cls_token, 
                                                   depth_cls_token, critical_labels):
        """
        è®¡ç®—åŸºäºè½¯è·¯ç”±çš„åŒæ•™å¸ˆREPAå¯¹é½æŸå¤±
        
        Args:
            action_tokens: (B, T, hidden_size) åŠ¨ä½œtokens
            dinov2_cls_token: (B, 1, dinov2_dim) æˆ– (B, dinov2_dim) DINOv2å…¨å±€ç‰¹å¾
            depth_cls_token: (B, depth_dim) æ·±åº¦ç‰¹å¾
            critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼ˆ0/1ï¼‰
        
        Returns:
            total_loss: æ€»æŸå¤±
            detailed_metrics: è¯¦ç»†æŒ‡æ ‡å­—å…¸
        """
        B, T, hidden_size = action_tokens.shape
        device = action_tokens.device
        dtype = action_tokens.dtype
        
        # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®
        if dinov2_cls_token.dim() == 3:
            dinov2_cls_squeezed = dinov2_cls_token.squeeze(1)  # (B, dinov2_dim)
        else:
            dinov2_cls_squeezed = dinov2_cls_token
        
        if depth_cls_token.dim() == 3:
            depth_cls_squeezed = depth_cls_token.squeeze(1)  # (B, depth_dim)
        else:
            depth_cls_squeezed = depth_cls_token
        
        # ç¡®ä¿å…³é”®æ—¶é—´æ®µæ ‡ç­¾ç±»å‹æ­£ç¡®
        if critical_labels.dtype != torch.long:
            critical_labels = critical_labels.long()
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªbatchï¼ˆç”¨äºæ—¶åºå¹³æ»‘ï¼‰
        is_first_batch = (self.batch_count == 0)
        
        try:
            # ä½¿ç”¨è½¯è·¯ç”±åŒæ•™å¸ˆæ¨¡å‹è®¡ç®—å¯¹é½æŸå¤±
            results = self.dual_teacher_model(
                action_tokens=action_tokens,
                dinov2_features=dinov2_cls_squeezed,
                depth_features=depth_cls_squeezed,
                critical_labels=critical_labels,
                is_first_batch=is_first_batch
            )
            
            # æå–æŸå¤±å’Œç»Ÿè®¡ä¿¡æ¯
            total_loss = results['total_loss']
            
            # æ„å»ºè¯¦ç»†æŒ‡æ ‡
            detailed_metrics = {
                # ä¸»è¦æŸå¤±ç»„ä»¶
                'soft_routing_total_loss': total_loss.item(),
                'soft_routing_alignment_loss': results['alignment_loss'].item(),
                'soft_routing_contrastive_loss': results['contrastive_loss'].item(),
                
                # åŸå§‹æŸå¤±ï¼ˆæœªåŠ æƒï¼‰
                'global_loss_raw': results['global_loss_raw'].item(),
                'depth_loss_raw': results['depth_loss_raw'].item(),
                
                # åŠ æƒæŸå¤±
                'weighted_global_loss': results['weighted_global_loss'].item(),
                'weighted_depth_loss': results['weighted_depth_loss'].item(),
                
                # ç›¸ä¼¼åº¦æŒ‡æ ‡
                'global_similarity_avg': results['global_similarity_avg'].item(),
                'depth_similarity_avg': results['depth_similarity_avg'].item(),
                
                # æ¸©åº¦å‚æ•°
                'alignment_temperature': results['alignment_temperature'],
            }
            
            # è·¯ç”±ç»Ÿè®¡ä¿¡æ¯
            router_stats = results['router_statistics']
            detailed_metrics.update({
                'critical_ratio': router_stats['critical_ratio'],
                'avg_global_weight': router_stats['avg_global_weight'],
                'avg_depth_weight': router_stats['avg_depth_weight'],
                'weight_std_global': router_stats['weight_std_global'],
                'weight_std_depth': router_stats['weight_std_depth'],
                'routing_temperature': router_stats['temperature'],
            })
            
            # åˆ†ç±»ç»Ÿè®¡
            if 'critical_avg_global' in router_stats:
                detailed_metrics.update({
                    'critical_avg_global_weight': router_stats['critical_avg_global'],
                    'critical_avg_depth_weight': router_stats['critical_avg_depth'],
                })
            
            if 'non_critical_avg_global' in router_stats:
                detailed_metrics.update({
                    'non_critical_avg_global_weight': router_stats['non_critical_avg_global'],
                    'non_critical_avg_depth_weight': router_stats['non_critical_avg_depth'],
                })
            
            # å¾®è°ƒç»Ÿè®¡
            if 'weight_drift' in router_stats:
                detailed_metrics['weight_drift'] = router_stats['weight_drift']
            
            # å­˜å‚¨è·¯ç”±æƒé‡ç”¨äºåˆ†æ
            detailed_metrics['routing_weights_tensor'] = results['routing_weights'].detach()
            detailed_metrics['base_weights_tensor'] = results['base_weights'].detach()
            
            return total_loss, detailed_metrics
            
        except Exception as e:
            print(f"âš ï¸ è½¯è·¯ç”±åŒæ•™å¸ˆREPAæŸå¤±è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›é›¶æŸå¤±å’ŒåŸºç¡€æŒ‡æ ‡
            zero_loss = torch.tensor(0.0, device=device, dtype=dtype)
            fallback_metrics = {
                'soft_routing_total_loss': 0.0,
                'soft_routing_alignment_loss': 0.0,
                'error': str(e),
            }
            return zero_loss, fallback_metrics

    def compute_loss(
        self, 
        lang_tokens, 
        lang_attn_mask, 
        img_tokens,
        state_tokens, 
        action_gt, 
        action_mask, 
        ctrl_freqs,
        # ğŸ†• æ–°å¢å‚æ•°
        dinov2_features=None,
        depth_features=None,
    ):
        """è®¡ç®—æŸå¤±ï¼ˆä»…æ‰©æ•£æŸå¤±ï¼‰"""
        batch_size = lang_tokens.shape[0]
        device = lang_tokens.device

        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_gt = action_gt.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        # ğŸ†• è§†è§‰ç‰¹å¾èåˆ
        if self.enable_vision_fusion and dinov2_features is not None and depth_features is not None:
            dinov2_features = dinov2_features.to(self.dtype)
            depth_features = depth_features.to(self.dtype)
            
            # å»é™¤CLS tokenï¼Œåªä¿ç•™patch tokens
            if dinov2_features.shape[1] == 1370:
                dinov2_patch_tokens = dinov2_features[:, 1:, :]
            else:
                dinov2_patch_tokens = dinov2_features
            
            if depth_features.shape[1] == 1370:
                depth_patch_tokens = depth_features[:, 1:, :]
            else:
                depth_patch_tokens = depth_features
            
            # èåˆç‰¹å¾ï¼ˆèåˆåˆ°æœ€åä¸€å¸§ï¼Œå³å½“å‰è§‚æµ‹ï¼‰
            img_tokens = self.vision_fusion_module(
                siglip_tokens=img_tokens,
                dinov2_tokens=dinov2_patch_tokens,
                depth_tokens=depth_patch_tokens,
                current_frame_idx=-1  # èåˆæœ€åä¸€å¸§
            )[0]
        
        # æ‰©æ•£æŸå¤±è®¡ç®—
        noise = torch.randn(action_gt.shape, dtype=action_gt.dtype, device=device)
        timesteps = torch.randint(0, self.num_train_timesteps, (batch_size,), device=device).long()
        noisy_action = self.noise_scheduler.add_noise(action_gt, noise, timesteps)
        
        state_action_traj = torch.cat([state_tokens, noisy_action], dim=1)
        action_mask = action_mask.expand(-1, state_action_traj.shape[1], -1)
        state_action_traj = torch.cat([state_action_traj, action_mask], dim=2)
        lang_cond, img_cond, state_action_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_action_traj
        )
        
        # RDTå‰å‘ä¼ æ’­
        pred, _ = self.model(
            state_action_traj, ctrl_freqs, timesteps, lang_cond, img_cond, 
            lang_mask=lang_attn_mask
        )

        # è®¡ç®—æ‰©æ•£æŸå¤±
        pred_type = self.prediction_type 
        if pred_type == 'epsilon':
            target = noise
        elif pred_type == 'sample':
            target = action_gt
        else:
            raise ValueError(f"Unsupported prediction type {pred_type}")

        diffusion_loss = F.mse_loss(pred, target)
        
        return diffusion_loss

    def conditional_sample(self, lang_cond, lang_attn_mask, img_cond, state_traj, action_mask, ctrl_freqs):
        """æ¨ç†æ—¶çš„æ¡ä»¶é‡‡æ ·ï¼Œä¸æ¶‰åŠå¯¹é½æœºåˆ¶"""
        device = state_traj.device
        dtype = state_traj.dtype
        noisy_action = torch.randn(
            size=(state_traj.shape[0], self.pred_horizon, self.action_dim), 
            dtype=dtype, device=device)
        action_mask = action_mask.expand(-1, self.pred_horizon, -1)
    
        self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)
        
        for t in self.noise_scheduler_sample.timesteps:
            action_traj = torch.cat([noisy_action, action_mask], dim=2)
            action_traj = self.state_adaptor(action_traj)
            state_action_traj = torch.cat([state_traj, action_traj], dim=1)
            
            model_output, _ = self.model(state_action_traj, ctrl_freqs,
                                        t.unsqueeze(-1).to(device),
                                        lang_cond, img_cond, lang_mask=lang_attn_mask)
            
            noisy_action = self.noise_scheduler_sample.step(
                model_output, t, noisy_action).prev_sample
            noisy_action = noisy_action.to(state_traj.dtype)
        
        noisy_action = noisy_action * action_mask
        return noisy_action

    def build_condition_adapter(self, projector_type, in_features, out_features):
        """æ„å»ºæ¡ä»¶é€‚é…å™¨"""
        projector = None
        if projector_type == 'linear':
            projector = nn.Linear(in_features, out_features)
        else:
            mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu', projector_type)
            if mlp_gelu_match:
                mlp_depth = int(mlp_gelu_match.group(1))
                modules = [nn.Linear(in_features, out_features)]
                for _ in range(1, mlp_depth):
                    modules.append(nn.GELU(approximate="tanh"))
                    modules.append(nn.Linear(out_features, out_features))
                projector = nn.Sequential(*modules)
        if projector is None:
            raise ValueError(f'Unknown projector type: {projector_type}')
        return projector

    def adapt_conditions(self, lang_tokens, img_tokens, state_tokens):
        """é€‚é…æ¡ä»¶è¾“å…¥"""
        adapted_lang = self.lang_adaptor(lang_tokens)
        adapted_img = self.img_adaptor(img_tokens)
        adapted_state = self.state_adaptor(state_tokens)
        return adapted_lang, adapted_img, adapted_state

    def predict_action(
        self, 
        lang_tokens, 
        lang_attn_mask, 
        img_tokens,
        state_tokens,
        action_mask, 
        ctrl_freqs, 
        # ğŸ†• æ–°å¢å‚æ•°
        dinov2_features=None,
        depth_features=None
        ):
        """é¢„æµ‹åŠ¨ä½œ"""
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        # ğŸ†• è§†è§‰ç‰¹å¾èåˆï¼ˆæ¨ç†æ—¶ï¼‰
        if self.enable_vision_fusion and dinov2_features is not None and depth_features is not None:
            dinov2_features = dinov2_features.to(self.dtype)
            depth_features = depth_features.to(self.dtype)
            
            # å»é™¤CLS token
            if dinov2_features.shape[1] == 1370:
                dinov2_patch_tokens = dinov2_features[:, 1:, :]
            else:
                dinov2_patch_tokens = dinov2_features
            
            if depth_features.shape[1] == 1370:
                depth_patch_tokens = depth_features[:, 1:, :]
            else:
                depth_patch_tokens = depth_features
            
            # èåˆ
            img_tokens = self.vision_fusion_module(
                siglip_tokens=img_tokens,
                dinov2_tokens=dinov2_patch_tokens,
                depth_tokens=depth_patch_tokens,
                current_frame_idx=-1
            )[0]
        
        state_tokens = torch.cat([state_tokens, action_mask], dim=2)
        lang_cond, img_cond, state_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_tokens
        )
        
        action_pred = self.conditional_sample(
            lang_cond, lang_attn_mask, img_cond, 
            state_traj, action_mask, ctrl_freqs,
        )
        
        return action_pred

    def reset_batch_count(self):
        """é‡ç½®batchè®¡æ•°å™¨ï¼ˆç”¨äºæ–°epochï¼‰"""
        self.batch_count = 0

    def get_soft_routing_statistics(self):
        """è·å–è½¯è·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_soft_routing_repa:
            return {}
        
        stats = {
            'training_step': self.training_step,
            'batch_count': self.batch_count,
            'soft_routing_config': self.soft_routing_config,
        }
        
        # å¦‚æœæœ‰è·¯ç”±å™¨ï¼Œè·å–å…¶ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self.dual_teacher_model, 'soft_router'):
            router = self.dual_teacher_model.soft_router
            stats.update({
                'routing_temperature': router.temperature.item(),
                'enable_neural_adjustment': router.enable_neural_adjustment,
                'temporal_smoothing': router.temporal_smoothing,
                'adjustment_strength': router.adjustment_strength,
            })
        
        return stats

    def forward(self, *args, **kwargs) -> torch.Tensor:
        """ä¿æŒå…¼å®¹æ€§ï¼Œåªè¿”å›æ€»æŸå¤±"""
        result = self.compute_loss(*args, **kwargs)
        if isinstance(result, tuple) and len(result) >= 1:
            return result[0]  # åªè¿”å›total_loss
        return result