# models/rdt_runner_eval.py - ä¸“é—¨ç”¨äºè¯„æµ‹çš„RDTRunnerç‰ˆæœ¬

import re
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
from diffusers.schedulers.scheduling_dpmsolver_multistep import DPMSolverMultistepScheduler

from models.hub_mixin import CompatiblePyTorchModelHubMixin
from models.rdt.model import RDT


class RDTRunnerEval(nn.Module, CompatiblePyTorchModelHubMixin, 
                   repo_url="https://huggingface.co/robotics-diffusion-transformer/rdt-1b"):
    """
    ä¸“é—¨ç”¨äºè¯„æµ‹çš„RDTè¿è¡Œå™¨
    
    å…³é”®æ”¹åŠ¨ï¼š
    1. ç§»é™¤æ‰€æœ‰REPAå¯¹é½ç›¸å…³ç»„ä»¶
    2. ç§»é™¤åŒæ•™å¸ˆè·¯ç”±ç½‘ç»œ
    3. ç§»é™¤æ·±åº¦ç‰¹å¾å¤„ç†
    4. ç®€åŒ–ä¸ºçº¯æ¨ç†æ¨¡å¼
    """
    def __init__(self, *, action_dim, pred_horizon, config, 
                 lang_token_dim, img_token_dim, state_token_dim, 
                 max_lang_cond_len, img_cond_len, lang_pos_embed_config=None, 
                 img_pos_embed_config=None, dtype=torch.bfloat16):
        super(RDTRunnerEval, self).__init__()
        
        self.dtype = dtype
        
        print(f"ğŸ”§ è¯„æµ‹ä¸“ç”¨RDTRunneråˆå§‹åŒ–:")
        print(f"   - ç§»é™¤REPAå¯¹é½ç»„ä»¶")
        print(f"   - ç§»é™¤åŒæ•™å¸ˆè·¯ç”±ç½‘ç»œ")
        print(f"   - çº¯æ¨ç†æ¨¡å¼")
        print(f"   - æ•°æ®ç±»å‹: {dtype}")
        
        # åˆ›å»ºæ‰©æ•£æ¨¡å‹ - ç¦ç”¨æ‰€æœ‰REPAç›¸å…³åŠŸèƒ½
        hidden_size = config['rdt']['hidden_size']
        self.model = RDT(
            output_dim=action_dim,
            horizon=pred_horizon,
            hidden_size=hidden_size,
            depth=config['rdt']['depth'], 
            num_heads=config['rdt']['num_heads'],
            max_lang_cond_len=max_lang_cond_len,
            img_cond_len=img_cond_len,
            lang_pos_embed_config=lang_pos_embed_config,
            img_pos_embed_config=img_pos_embed_config,
            dtype=dtype,
            # ğŸ”§ å…³é”®ï¼šç¦ç”¨æ‰€æœ‰REPAåŠŸèƒ½
            enable_repa_loss=False,
            use_dual_teachers=False,
        )

        # é€‚é…å™¨åˆ›å»º
        self.lang_adaptor = self.build_condition_adapter(
            config['lang_adaptor'], 
            in_features=lang_token_dim, 
            out_features=hidden_size
        )
        self.img_adaptor = self.build_condition_adapter(
            config['img_adaptor'], 
            in_features=img_token_dim, 
            out_features=hidden_size
        )
        self.state_adaptor = self.build_condition_adapter(
            config['state_adaptor'], 
            in_features=state_token_dim * 2,    
            out_features=hidden_size
        )
        
        # è½¬æ¢é€‚é…å™¨åˆ°æ­£ç¡®çš„æ•°æ®ç±»å‹
        self.lang_adaptor = self.lang_adaptor.to(dtype)
        self.img_adaptor = self.img_adaptor.to(dtype)
        self.state_adaptor = self.state_adaptor.to(dtype)
        
        # å™ªå£°è°ƒåº¦å™¨åˆ›å»º
        noise_scheduler_config = config['noise_scheduler']
        self.noise_scheduler = DDPMScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
            clip_sample=noise_scheduler_config['clip_sample'],
        )
        self.noise_scheduler_sample = DPMSolverMultistepScheduler(
            num_train_timesteps=noise_scheduler_config['num_train_timesteps'],
            beta_schedule=noise_scheduler_config['beta_schedule'],
            prediction_type=noise_scheduler_config['prediction_type'],
        )

        self.num_train_timesteps = noise_scheduler_config['num_train_timesteps']
        self.num_inference_timesteps = noise_scheduler_config['num_inference_timesteps']
        self.prediction_type = noise_scheduler_config['prediction_type']

        self.pred_horizon = pred_horizon
        self.action_dim = action_dim

        print("Diffusion params: %e" % sum(
            [p.numel() for p in self.model.parameters()] + 
            [p.numel() for p in self.lang_adaptor.parameters()] + 
            [p.numel() for p in self.img_adaptor.parameters()] + 
            [p.numel() for p in self.state_adaptor.parameters()]))

    def load_pretrained_weights(self, checkpoint_path):
        """
        æ™ºèƒ½åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œè‡ªåŠ¨è¿‡æ»¤ä¸åŒ¹é…çš„å‚æ•°
        """
        print(f"ğŸ”§ å¼€å§‹åŠ è½½æƒé‡: {checkpoint_path}")
        
        try:
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
            if isinstance(checkpoint, dict):
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'module' in checkpoint:
                    state_dict = checkpoint['module']
                else:
                    state_dict = checkpoint
            else:
                state_dict = checkpoint
            
            print(f"âœ… CheckpointåŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(state_dict)} ä¸ªé¡¶çº§é”®")
            
            # è·å–å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸
            current_state_dict = self.state_dict()
            print(f"ğŸ“‹ å½“å‰æ¨¡å‹åŒ…å« {len(current_state_dict)} ä¸ªå‚æ•°")
            
            # æ™ºèƒ½è¿‡æ»¤æƒé‡
            filtered_state_dict = {}
            skipped_keys = []
            missing_keys = []
            shape_mismatched_keys = []
            
            # æ£€æŸ¥checkpointä¸­çš„æ¯ä¸ªå‚æ•°
            for key, value in state_dict.items():
                if key in current_state_dict:
                    current_shape = current_state_dict[key].shape
                    checkpoint_shape = value.shape
                    
                    if current_shape == checkpoint_shape:
                        filtered_state_dict[key] = value
                    else:
                        shape_mismatched_keys.append(f"{key} (å½¢çŠ¶ä¸åŒ¹é…: {checkpoint_shape} vs {current_shape})")
                else:
                    # è·³è¿‡ä¸å­˜åœ¨çš„å‚æ•°ï¼ˆå¦‚REPAç›¸å…³å‚æ•°ï¼‰
                    if any(keyword in key for keyword in [
                        'dual_teacher_model', 'soft_router', 'routing_network', 
                        'dinov2_to_action_projector', 'depth_to_action_projector',
                        'routing_temperature']):
                        skipped_keys.append(key)
                    else:
                        skipped_keys.append(key)
            
            # æ£€æŸ¥æ¨¡å‹ä¸­ç¼ºå¤±çš„å‚æ•°
            for key in current_state_dict.keys():
                if key not in state_dict:
                    missing_keys.append(key)
            
            # åŠ è½½è¿‡æ»¤åçš„æƒé‡
            missing_keys_after_load, unexpected_keys = self.load_state_dict(filtered_state_dict, strict=False)
            
            # è¯¦ç»†æŠ¥å‘Š
            print(f"âœ… æˆåŠŸåŠ è½½ {len(filtered_state_dict)} ä¸ªå‚æ•°")
            
            if skipped_keys:
                print(f"âš ï¸  è·³è¿‡ {len(skipped_keys)} ä¸ªå…¶ä»–å‚æ•°")
                for key in skipped_keys[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"     - {key}")
                if len(skipped_keys) > 5:
                    print(f"     - ... è¿˜æœ‰ {len(skipped_keys) - 5} ä¸ªå‚æ•°")
            
            if shape_mismatched_keys:
                print(f"âš ï¸  å½¢çŠ¶ä¸åŒ¹é…çš„å‚æ•° ({len(shape_mismatched_keys)} ä¸ª):")
                for key in shape_mismatched_keys:
                    print(f"     - {key}")
            
            if missing_keys_after_load:
                print(f"âš ï¸  {len(missing_keys_after_load)} ä¸ªæ¨¡å‹å‚æ•°æœªæ‰¾åˆ°å¯¹åº”æƒé‡ï¼ˆä¿æŒé»˜è®¤åˆå§‹åŒ–ï¼‰")
                # åªåœ¨è°ƒè¯•æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                # for key in missing_keys_after_load[:3]:
                #     print(f"     - {key}")
            
            print(f"âœ… æƒé‡åŠ è½½å®Œæˆï¼Œæ¨¡å‹å¯ä»¥æ­£å¸¸è¯„ä¼°")
            return True
            
        except Exception as e:
            print(f"âŒ æƒé‡åŠ è½½å¤±è´¥: {e}")
            return False

    def conditional_sample(self, lang_cond, lang_attn_mask, img_cond, state_traj, action_mask, ctrl_freqs):
        """æ¨ç†æ—¶çš„æ¡ä»¶é‡‡æ · - çº¯å‡€ç‰ˆæœ¬"""
        device = state_traj.device
        dtype = state_traj.dtype
        noisy_action = torch.randn(
            size=(state_traj.shape[0], self.pred_horizon, self.action_dim), 
            dtype=dtype, device=device)
        action_mask = action_mask.expand(-1, self.pred_horizon, -1)
    
        self.noise_scheduler_sample.set_timesteps(self.num_inference_timesteps)
        
        for t in self.noise_scheduler_sample.timesteps:
            action_traj = torch.cat([noisy_action, action_mask], dim=2)
            action_traj = self.state_adaptor(action_traj)
            state_action_traj = torch.cat([state_traj, action_traj], dim=1)
            
            # ğŸ”§ åªè·å–åŠ¨ä½œé¢„æµ‹ï¼Œå¿½ç•¥ä¸­é—´æ¿€æ´»
            model_output, _ = self.model(state_action_traj, ctrl_freqs,
                                        t.unsqueeze(-1).to(device),
                                        lang_cond, img_cond, lang_mask=lang_attn_mask)
            
            noisy_action = self.noise_scheduler_sample.step(
                model_output, t, noisy_action).prev_sample
            noisy_action = noisy_action.to(state_traj.dtype)
        
        noisy_action = noisy_action * action_mask
        return noisy_action

    def build_condition_adapter(self, projector_type, in_features, out_features):
        """æ„å»ºæ¡ä»¶é€‚é…å™¨"""
        projector = None
        if projector_type == 'linear':
            projector = nn.Linear(in_features, out_features)
        else:
            mlp_gelu_match = re.match(r'^mlp(\d+)x_gelu', projector_type)
            if mlp_gelu_match:
                mlp_depth = int(mlp_gelu_match.group(1))
                modules = [nn.Linear(in_features, out_features)]
                for _ in range(1, mlp_depth):
                    modules.append(nn.GELU(approximate="tanh"))
                    modules.append(nn.Linear(out_features, out_features))
                projector = nn.Sequential(*modules)
        if projector is None:
            raise ValueError(f'Unknown projector type: {projector_type}')
        return projector

    def adapt_conditions(self, lang_tokens, img_tokens, state_tokens):
        """é€‚é…æ¡ä»¶è¾“å…¥"""
        adapted_lang = self.lang_adaptor(lang_tokens)
        adapted_img = self.img_adaptor(img_tokens)
        adapted_state = self.state_adaptor(state_tokens)
        return adapted_lang, adapted_img, adapted_state

    def predict_action(self, lang_tokens, lang_attn_mask, img_tokens, state_tokens,
                    action_mask, ctrl_freqs, vision_features=None):
        """é¢„æµ‹åŠ¨ä½œ - çº¯æ¨ç†ç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨ä»»ä½•å¯¹é½ç‰¹å¾"""
        lang_tokens = lang_tokens.to(self.dtype)
        img_tokens = img_tokens.to(self.dtype)
        state_tokens = state_tokens.to(self.dtype)
        action_mask = action_mask.to(self.dtype)
        
        state_tokens = torch.cat([state_tokens, action_mask], dim=2)
        lang_cond, img_cond, state_traj = self.adapt_conditions(
            lang_tokens, img_tokens, state_tokens)
        
        action_pred = self.conditional_sample(
            lang_cond, lang_attn_mask, img_cond, 
            state_traj, action_mask, ctrl_freqs,
        )
        
        return action_pred

    def forward(self, *args, **kwargs) -> torch.Tensor:
        """ç®€åŒ–çš„å‰å‘ä¼ æ’­ï¼Œåªç”¨äºæ¨ç†"""
        return self.predict_action(*args, **kwargs)


# å·¥å‚å‡½æ•°ï¼šæ ¹æ®æ¨¡å¼åˆ›å»ºåˆé€‚çš„RDTRunner
def create_rdt_runner(mode="eval", **kwargs):
    """
    åˆ›å»ºRDTRunnerçš„å·¥å‚å‡½æ•°
    
    Args:
        mode: "train" æˆ– "eval"
        **kwargs: RDTRunnerçš„åˆå§‹åŒ–å‚æ•°
    
    Returns:
        RDTRunnerå®ä¾‹
    """
    if mode == "eval":
        print("ğŸ”§ åˆ›å»ºè¯„æµ‹ä¸“ç”¨RDTRunner")
        return RDTRunnerEval(**kwargs)
    elif mode == "train":
        print("ğŸ”§ åˆ›å»ºè®­ç»ƒç”¨RDTRunner")
        from models.rdt_runner import RDTRunner  # å¯¼å…¥åŸå§‹è®­ç»ƒç‰ˆæœ¬
        return RDTRunner(**kwargs)
    else:
        raise ValueError(f"Unknown mode: {mode}, expected 'train' or 'eval'")