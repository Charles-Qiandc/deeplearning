# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
# --------------------------------------------------------
# References:
# DiT: https://github.com/facebookresearch/DiT
# GLIDE: https://github.com/openai/glide-text2im
# MAE: https://github.com/facebookresearch/mae/blob/main/models_mae.py
# --------------------------------------------------------
from collections import OrderedDict

import torch
import torch.nn as nn

from models.rdt.blocks import (FinalLayer, RDTBlock, TimestepEmbedder,
                               get_1d_sincos_pos_embed_from_grid,
                               get_multimodal_cond_pos_embed)


class RDTAblation(nn.Module):
    """
    æ¶ˆèå®éªŒç‰ˆRDTæ¨¡å‹ï¼šç§»é™¤REPAå¯¹é½ï¼Œå°†DINOv2å’ŒDepthAnythingçš„ç‰¹å¾æ³¨å…¥DITè¾“å…¥ä¾§
    
    å…³é”®æ”¹åŠ¨ï¼š
    1. ç§»é™¤æ‰€æœ‰REPAç›¸å…³ç»„ä»¶
    2. æ·»åŠ DINOv2/DepthAnythingç‰¹å¾çš„è¾“å…¥æŠ•å½±å™¨
    3. åœ¨å›¾åƒæ¡ä»¶ä¸­èåˆå¤šæ¨¡æ€è§†è§‰ç‰¹å¾
    4. ä¿æŒåŸæœ‰çš„transformerç»“æ„ä¸å˜
    """

    def __init__(
        self,
        output_dim=128,
        horizon=64,
        hidden_size=2048,
        depth=28,
        num_heads=32,
        max_lang_cond_len=1024,
        img_cond_len=4096,
        lang_pos_embed_config=None,
        img_pos_embed_config=None,
        dtype=torch.bfloat16,
        # ğŸ†• å¤šæ¨¡æ€è§†è§‰ç‰¹å¾å‚æ•°
        use_dinov2_features=False,
        dinov2_feature_dim=1024,
        use_depth_features=False,
        depth_feature_dim=1024,
        # ğŸ†• ç‰¹å¾èåˆç­–ç•¥
        visual_fusion_strategy="concat",  # "concat", "add", "gate"
    ):
        super().__init__()
        self.horizon = horizon
        self.hidden_size = hidden_size
        self.max_lang_cond_len = max_lang_cond_len
        self.img_cond_len = img_cond_len
        self.dtype = dtype
        self.lang_pos_embed_config = lang_pos_embed_config
        self.img_pos_embed_config = img_pos_embed_config
        
        # ğŸ†• å¤šæ¨¡æ€è§†è§‰ç‰¹å¾é…ç½®
        self.use_dinov2_features = use_dinov2_features
        self.dinov2_feature_dim = dinov2_feature_dim
        self.use_depth_features = use_depth_features
        self.depth_feature_dim = depth_feature_dim
        self.visual_fusion_strategy = visual_fusion_strategy
        
        print(f"ğŸ”§ RDTæ¶ˆèå®éªŒæ¨¡å‹åˆå§‹åŒ–:")
        print(f"   - DINOv2ç‰¹å¾: {use_dinov2_features}")
        print(f"   - æ·±åº¦ç‰¹å¾: {use_depth_features}")
        print(f"   - è§†è§‰èåˆç­–ç•¥: {visual_fusion_strategy}")

        # åµŒå…¥å™¨ç»„ä»¶
        self.t_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        self.freq_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        
        # ğŸ†• è®¡ç®—å®é™…çš„å›¾åƒæ¡ä»¶é•¿åº¦ï¼ˆåŒ…å«é¢å¤–çš„è§†è§‰ç‰¹å¾ï¼‰
        self.actual_img_cond_len = img_cond_len
        extra_tokens = 0
        if use_dinov2_features:
            extra_tokens += 1  # DINOv2 CLS token
        if use_depth_features:
            extra_tokens += 1  # DepthAnything CLS token
        self.actual_img_cond_len += extra_tokens
        
        print(f"   - åŸå§‹å›¾åƒæ¡ä»¶é•¿åº¦: {img_cond_len}")
        print(f"   - é¢å¤–è§†è§‰tokens: {extra_tokens}")
        print(f"   - å®é™…å›¾åƒæ¡ä»¶é•¿åº¦: {self.actual_img_cond_len}")
        
        # ä½ç½®ç¼–ç å‚æ•°
        self.x_pos_embed = nn.Parameter(torch.zeros(1, horizon+3, hidden_size))
        self.lang_cond_pos_embed = nn.Parameter(torch.zeros(1, max_lang_cond_len, hidden_size))
        self.img_cond_pos_embed = nn.Parameter(torch.zeros(1, self.actual_img_cond_len, hidden_size))

        # Transformerå—
        self.blocks = nn.ModuleList([
            RDTBlock(hidden_size, num_heads) for _ in range(depth)
        ])
        
        # ğŸ†• å¤šæ¨¡æ€è§†è§‰ç‰¹å¾æŠ•å½±å™¨
        self.extra_visual_projectors = nn.ModuleDict()
        
        if use_dinov2_features:
            # DINOv2ç‰¹å¾æŠ•å½±åˆ°hidden_sizeç»´åº¦
            self.extra_visual_projectors['dinov2'] = nn.Sequential(
                nn.Linear(dinov2_feature_dim, hidden_size // 2),
                nn.LayerNorm(hidden_size // 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_size // 2, hidden_size),
                nn.LayerNorm(hidden_size),
            )
            print(f"   - DINOv2æŠ•å½±å™¨: {dinov2_feature_dim} -> {hidden_size}")
            
        if use_depth_features:
            # DepthAnythingç‰¹å¾æŠ•å½±åˆ°hidden_sizeç»´åº¦
            self.extra_visual_projectors['depth'] = nn.Sequential(
                nn.Linear(depth_feature_dim, hidden_size // 2),
                nn.LayerNorm(hidden_size // 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_size // 2, hidden_size),
                nn.LayerNorm(hidden_size),
            )
            print(f"   - æ·±åº¦ç‰¹å¾æŠ•å½±å™¨: {depth_feature_dim} -> {hidden_size}")
        
        # ğŸ†• è§†è§‰ç‰¹å¾èåˆå±‚ï¼ˆå¦‚æœä½¿ç”¨gateç­–ç•¥ï¼‰
        if visual_fusion_strategy == "gate" and (use_dinov2_features or use_depth_features):
            fusion_input_dim = 1  # SigLIPé»˜è®¤
            if use_dinov2_features:
                fusion_input_dim += 1
            if use_depth_features:
                fusion_input_dim += 1
                
            self.visual_fusion_gate = nn.Sequential(
                nn.Linear(hidden_size * fusion_input_dim, hidden_size),
                nn.GELU(),
                nn.Linear(hidden_size, fusion_input_dim),
                nn.Softmax(dim=-1)
            )
            print(f"   - é—¨æ§èåˆå±‚: {fusion_input_dim}ä¸ªè§†è§‰æº")
        
        # æœ€ç»ˆå±‚
        self.final_layer = FinalLayer(hidden_size, output_dim)
        self.initialize_weights()

    def initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # åŸºç¡€æƒé‡åˆå§‹åŒ–
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        self.apply(_basic_init)

        # ä½ç½®ç¼–ç åˆå§‹åŒ–
        x_pos_embed = get_multimodal_cond_pos_embed(
            embed_dim=self.hidden_size,
            mm_cond_lens=OrderedDict([
                ('timestep', 1),
                ('ctrl_freq', 1),
                ('state', 1),
                ('action', self.horizon),
            ])
        )
        self.x_pos_embed.data.copy_(torch.from_numpy(x_pos_embed).float().unsqueeze(0))

        # è¯­è¨€æ¡ä»¶ä½ç½®ç¼–ç 
        if self.lang_pos_embed_config is None:
            lang_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(
                self.hidden_size, torch.arange(self.max_lang_cond_len))
        else:
            lang_cond_pos_embed = get_multimodal_cond_pos_embed(
                embed_dim=self.hidden_size,
                mm_cond_lens=OrderedDict(self.lang_pos_embed_config),
                embed_modality=False
            )
        self.lang_cond_pos_embed.data.copy_(
            torch.from_numpy(lang_cond_pos_embed).float().unsqueeze(0))
        
        # ğŸ†• å›¾åƒæ¡ä»¶ä½ç½®ç¼–ç ï¼ˆåŒ…å«é¢å¤–çš„è§†è§‰ç‰¹å¾ï¼‰
        if self.img_pos_embed_config is None:
            img_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(
                self.hidden_size, torch.arange(self.actual_img_cond_len))
        else:
            # æ„é€ åŒ…å«é¢å¤–è§†è§‰ç‰¹å¾çš„é…ç½®
            extended_config = OrderedDict(self.img_pos_embed_config)
            if self.use_dinov2_features:
                extended_config["dinov2_cls"] = 1
            if self.use_depth_features:
                extended_config["depth_cls"] = 1
                
            img_cond_pos_embed = get_multimodal_cond_pos_embed(
                embed_dim=self.hidden_size,
                mm_cond_lens=extended_config,
                embed_modality=False
            )
        self.img_cond_pos_embed.data.copy_(
            torch.from_numpy(img_cond_pos_embed).float().unsqueeze(0))

        # æ—¶é—´æ­¥å’Œé¢‘ç‡åµŒå…¥å™¨åˆå§‹åŒ–
        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[2].weight, std=0.02)
        
        # ğŸ†• å¤šæ¨¡æ€æŠ•å½±å™¨åˆå§‹åŒ–
        for projector_name, projector in self.extra_visual_projectors.items():
            for module in projector:
                if isinstance(module, nn.Linear):
                    nn.init.xavier_uniform_(module.weight, gain=0.5)
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
            print(f"   - {projector_name}æŠ•å½±å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # ğŸ†• èåˆå±‚åˆå§‹åŒ–
        if hasattr(self, 'visual_fusion_gate'):
            for module in self.visual_fusion_gate:
                if isinstance(module, nn.Linear):
                    nn.init.xavier_uniform_(module.weight, gain=0.1)
                    if module.bias is not None:
                        nn.init.constant_(module.bias, 0)
        
        # æœ€ç»ˆå±‚é›¶åˆå§‹åŒ–
        nn.init.constant_(self.final_layer.ffn_final.fc2.weight, 0)
        nn.init.constant_(self.final_layer.ffn_final.fc2.bias, 0)
        
        # è½¬æ¢åˆ°æŒ‡å®šæ•°æ®ç±»å‹
        self.to(self.dtype)

    def fuse_visual_features(self, img_c, dinov2_features=None, depth_features=None):
        """
        èåˆå¤šæ¨¡æ€è§†è§‰ç‰¹å¾åˆ°å›¾åƒæ¡ä»¶ä¸­
        
        Args:
            img_c: (B, N_siglip, D) SigLIPç‰¹å¾
            dinov2_features: (B, 1, D) DINOv2 CLS tokenç‰¹å¾
            depth_features: (B, 1, D) DepthAnything CLS tokenç‰¹å¾
            
        Returns:
            fused_img_c: (B, N_total, D) èåˆåçš„è§†è§‰ç‰¹å¾
        """
        B, N_siglip, D = img_c.shape
        
        # æ”¶é›†æ‰€æœ‰è§†è§‰ç‰¹å¾
        visual_features = [img_c]  # SigLIP patch tokens
        feature_names = ["siglip"]
        
        if dinov2_features is not None and self.use_dinov2_features:
            # æŠ•å½±DINOv2ç‰¹å¾
            projected_dinov2 = self.extra_visual_projectors['dinov2'](dinov2_features)
            visual_features.append(projected_dinov2)
            feature_names.append("dinov2")
            
        if depth_features is not None and self.use_depth_features:
            # æŠ•å½±æ·±åº¦ç‰¹å¾
            projected_depth = self.extra_visual_projectors['depth'](depth_features)
            visual_features.append(projected_depth)
            feature_names.append("depth")
        
        # æ ¹æ®èåˆç­–ç•¥ç»„åˆç‰¹å¾
        if self.visual_fusion_strategy == "concat":
            # ç›´æ¥æ‹¼æ¥ï¼š[SigLIP_patches, DINOv2_cls, Depth_cls]
            fused_features = torch.cat(visual_features, dim=1)
            
        elif self.visual_fusion_strategy == "add":
            # åŠ æƒæ±‚å’Œï¼ˆåªå¯¹ç›¸åŒé•¿åº¦çš„ç‰¹å¾ï¼‰
            # SigLIPä½œä¸ºåŸºç¡€ï¼Œé¢å¤–ç‰¹å¾broadcaståˆ°æ‰€æœ‰ä½ç½®
            fused_features = img_c.clone()
            
            if dinov2_features is not None and self.use_dinov2_features:
                projected_dinov2 = self.extra_visual_projectors['dinov2'](dinov2_features)
                # Broadcaståˆ°æ‰€æœ‰SigLIPä½ç½®
                fused_features += projected_dinov2.expand(-1, N_siglip, -1)
                
            if depth_features is not None and self.use_depth_features:
                projected_depth = self.extra_visual_projectors['depth'](depth_features)
                # Broadcaståˆ°æ‰€æœ‰SigLIPä½ç½®
                fused_features += projected_depth.expand(-1, N_siglip, -1)
                
        elif self.visual_fusion_strategy == "gate":
            # é—¨æ§èåˆ
            # é¦–å…ˆå¯¹æ‰€æœ‰ç‰¹å¾è¿›è¡Œå¹³å‡æ± åŒ–å¾—åˆ°global representation
            global_features = []
            for feat in visual_features:
                global_feat = feat.mean(dim=1, keepdim=True)  # (B, 1, D)
                global_features.append(global_feat)
            
            # è®¡ç®—é—¨æ§æƒé‡
            concat_global = torch.cat(global_features, dim=-1)  # (B, 1, D*num_features)
            gates = self.visual_fusion_gate(concat_global)  # (B, 1, num_features)
            
            # åº”ç”¨é—¨æ§æƒé‡
            weighted_features = []
            for i, feat in enumerate(visual_features):
                weight = gates[:, :, i:i+1]  # (B, 1, 1)
                if feat.shape[1] == 1:  # CLS token
                    weighted_feat = feat * weight
                else:  # patch tokens
                    weighted_feat = feat * weight.expand(-1, feat.shape[1], -1)
                weighted_features.append(weighted_feat)
            
            # æ‹¼æ¥åŠ æƒç‰¹å¾
            fused_features = torch.cat(weighted_features, dim=1)
            
        else:
            raise ValueError(f"æœªæ”¯æŒçš„è§†è§‰èåˆç­–ç•¥: {self.visual_fusion_strategy}")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        expected_len = self.actual_img_cond_len
        actual_len = fused_features.shape[1]
        if actual_len != expected_len:
            print(f"âš ï¸ è§†è§‰ç‰¹å¾é•¿åº¦ä¸åŒ¹é…: {actual_len} vs æœŸæœ›çš„ {expected_len}")
            # è°ƒæ•´åˆ°æœŸæœ›é•¿åº¦
            if actual_len > expected_len:
                fused_features = fused_features[:, :expected_len, :]
            else:
                padding = torch.zeros(B, expected_len - actual_len, D, 
                                    device=fused_features.device, 
                                    dtype=fused_features.dtype)
                fused_features = torch.cat([fused_features, padding], dim=1)
        
        print(f"âœ… è§†è§‰ç‰¹å¾èåˆå®Œæˆ: {feature_names} -> {fused_features.shape}")
        return fused_features

    def forward(self, x, freq, t, lang_c, img_c, lang_mask=None, img_mask=None,
                dinov2_features=None, depth_features=None):
        """
        å‰å‘ä¼ æ’­ï¼šèåˆå¤šæ¨¡æ€è§†è§‰ç‰¹å¾
        
        Args:
            x, freq, t, lang_c: æ ‡å‡†DITè¾“å…¥
            img_c: (B, N_siglip, D) SigLIPå›¾åƒç‰¹å¾
            dinov2_features: (B, 1, D) DINOv2 CLSç‰¹å¾
            depth_features: (B, 1, D) DepthAnything CLSç‰¹å¾
            
        Returns:
            x: (B, horizon, output_dim) æœ€ç»ˆé¢„æµ‹
        """
        # æ—¶é—´æ­¥å’Œé¢‘ç‡åµŒå…¥
        t = self.t_embedder(t).unsqueeze(1)             # (B, 1, D)
        freq = self.freq_embedder(freq).unsqueeze(1)    # (B, 1, D)
        
        # å¤„ç†æ—¶é—´æ­¥å¹¿æ’­
        if t.shape[0] == 1:
            t = t.expand(x.shape[0], -1, -1)
        x = torch.cat([t, freq, x], dim=1)               # (B, T+2, D)
        
        # ğŸ†• èåˆå¤šæ¨¡æ€è§†è§‰ç‰¹å¾
        fused_img_c = self.fuse_visual_features(img_c, dinov2_features, depth_features)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.x_pos_embed
        lang_c = lang_c + self.lang_cond_pos_embed[:, :lang_c.shape[1]]
        fused_img_c = fused_img_c + self.img_cond_pos_embed[:, :fused_img_c.shape[1]]

        # å‰å‘ä¼ æ’­é€šè¿‡transformerå—
        conds = [lang_c, fused_img_c]
        masks = [lang_mask, img_mask]
        for i, block in enumerate(self.blocks):
            c, mask = conds[i%2], masks[i%2]
            x = block(x, c, mask)                       # (B, T+2, D)

        # æœ€ç»ˆè¾“å‡ºå±‚
        x = self.final_layer(x)                         # (B, T+2, output_dim)

        # åªä¿ç•™åŠ¨ä½œtoken
        x = x[:, -self.horizon:]
        
        return x