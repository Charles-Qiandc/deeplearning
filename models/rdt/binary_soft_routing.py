# models/rdt/binary_soft_routing.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple
import numpy as np


class BinaryLabelSoftRouter(nn.Module):
    """
    åŸºäºå…³é”®æ—¶é—´æ®µäºŒå…ƒæ ‡ç­¾çš„è½¯è·¯ç”±æƒé‡åˆ†é…å™¨
    
    æ ¸å¿ƒæ€è·¯ï¼š
    - å…³é”®æ—¶é—´æ®µ(1): [å…¨å±€25%, æ·±åº¦75%] - åå‘ç²¾ç¡®æ“ä½œ
    - éå…³é”®æ—¶é—´æ®µ(0): [å…¨å±€75%, æ·±åº¦25%] - åå‘åœºæ™¯ç†è§£
    """
    
    def __init__(self,
                 action_dim: int = 2048,
                 hidden_dim: int = 256,
                 # åŸºç¡€æƒé‡é…ç½®
                 critical_global_weight: float = 0.25,      # å…³é”®æ—¶é—´æ®µå…¨å±€æƒé‡
                 critical_depth_weight: float = 0.75,       # å…³é”®æ—¶é—´æ®µæ·±åº¦æƒé‡
                 non_critical_global_weight: float = 0.75,  # éå…³é”®æ—¶é—´æ®µå…¨å±€æƒé‡
                 non_critical_depth_weight: float = 0.25,   # éå…³é”®æ—¶é—´æ®µæ·±åº¦æƒé‡
                 # å¾®è°ƒå’Œå¹³æ»‘å‚æ•°
                 enable_neural_adjustment: bool = True,     # æ˜¯å¦å¯ç”¨ç¥ç»ç½‘ç»œå¾®è°ƒ
                 adjustment_strength: float = 0.1,          # å¾®è°ƒå¼ºåº¦
                 temporal_smoothing: float = 0.9,           # æ—¶åºå¹³æ»‘ç³»æ•°
                 temperature: float = 1.0):                 # softmaxæ¸©åº¦
        super().__init__()
        
        self.action_dim = action_dim
        self.hidden_dim = hidden_dim
        
        # åŸºç¡€æƒé‡é…ç½®
        self.critical_global_weight = critical_global_weight
        self.critical_depth_weight = critical_depth_weight
        self.non_critical_global_weight = non_critical_global_weight
        self.non_critical_depth_weight = non_critical_depth_weight
        
        # ç¡®ä¿æƒé‡å’Œä¸º1
        assert abs(critical_global_weight + critical_depth_weight - 1.0) < 1e-6
        assert abs(non_critical_global_weight + non_critical_depth_weight - 1.0) < 1e-6
        
        # å¾®è°ƒå’Œå¹³æ»‘å‚æ•°
        self.enable_neural_adjustment = enable_neural_adjustment
        self.adjustment_strength = adjustment_strength
        self.temporal_smoothing = temporal_smoothing
        self.temperature = nn.Parameter(torch.tensor(temperature))
        
        # å¯é€‰ï¼šç¥ç»ç½‘ç»œå¾®è°ƒå™¨
        if enable_neural_adjustment:
            self.weight_adjuster = nn.Sequential(
                nn.Linear(action_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.LayerNorm(hidden_dim // 2),
                nn.GELU(),
                nn.Linear(hidden_dim // 2, 2),  # è¾“å‡ºè°ƒæ•´å€¼ [global_adj, depth_adj]
                nn.Tanh()  # è¾“å‡ºèŒƒå›´ [-1, 1]
            )
            self._initialize_adjuster()
        
        # å­˜å‚¨ä¸Šä¸€æ—¶é—´æ­¥çš„æƒé‡ç”¨äºå¹³æ»‘
        self.register_buffer('prev_weights', torch.zeros(1, 1, 2))
        
    def _initialize_adjuster(self):
        """åˆå§‹åŒ–æƒé‡å¾®è°ƒå™¨"""
        for module in self.weight_adjuster:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight, gain=0.1)  # å°åˆå§‹åŒ–
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def get_base_weights(self, critical_labels: torch.Tensor) -> torch.Tensor:
        """
        æ ¹æ®äºŒå…ƒæ ‡ç­¾è·å–åŸºç¡€æƒé‡
        
        Args:
            critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼Œ0/1
            
        Returns:
            base_weights: (B, T, 2) åŸºç¡€æƒé‡ [global_weight, depth_weight]
        """
        B, T = critical_labels.shape
        device = critical_labels.device
        dtype = critical_labels.dtype if critical_labels.dtype.is_floating_point else torch.float32
        
        # åˆ›å»ºæƒé‡å¼ é‡
        weights = torch.zeros(B, T, 2, device=device, dtype=dtype)
        
        # å…³é”®æ—¶é—´æ®µæƒé‡ (critical_labels == 1)
        critical_mask = critical_labels.bool()
        weights[critical_mask, 0] = self.critical_global_weight    # å…¨å±€æƒé‡
        weights[critical_mask, 1] = self.critical_depth_weight     # æ·±åº¦æƒé‡
        
        # éå…³é”®æ—¶é—´æ®µæƒé‡ (critical_labels == 0)
        non_critical_mask = ~critical_mask
        weights[non_critical_mask, 0] = self.non_critical_global_weight  # å…¨å±€æƒé‡
        weights[non_critical_mask, 1] = self.non_critical_depth_weight   # æ·±åº¦æƒé‡
        
        return weights
    
    def apply_neural_adjustment(self, 
                              base_weights: torch.Tensor, 
                              action_tokens: torch.Tensor) -> torch.Tensor:
        """
        ä½¿ç”¨ç¥ç»ç½‘ç»œå¾®è°ƒæƒé‡
        
        Args:
            base_weights: (B, T, 2) åŸºç¡€æƒé‡
            action_tokens: (B, T, action_dim) åŠ¨ä½œtokens
            
        Returns:
            adjusted_weights: (B, T, 2) å¾®è°ƒåçš„æƒé‡
        """
        if not self.enable_neural_adjustment:
            return base_weights
        
        B, T, _ = action_tokens.shape
        
        # è®¡ç®—æƒé‡è°ƒæ•´å€¼
        flat_tokens = action_tokens.view(B * T, -1)
        adjustments = self.weight_adjuster(flat_tokens)  # (B*T, 2)
        adjustments = adjustments.view(B, T, 2)          # (B, T, 2)
        
        # åº”ç”¨è°ƒæ•´å¼ºåº¦
        adjustments = adjustments * self.adjustment_strength
        
        # è°ƒæ•´æƒé‡
        adjusted_weights = base_weights + adjustments
        
        # é‡æ–°å½’ä¸€åŒ–ç¡®ä¿æƒé‡å’Œä¸º1
        adjusted_weights = F.softmax(adjusted_weights / torch.clamp(self.temperature, min=0.1), dim=-1)
        
        return adjusted_weights
    
    def apply_temporal_smoothing(self, 
                               current_weights: torch.Tensor,
                               is_first_batch: bool = False) -> torch.Tensor:
        """
        åº”ç”¨æ—¶åºå¹³æ»‘
        
        Args:
            current_weights: (B, T, 2) å½“å‰æƒé‡
            is_first_batch: æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªbatch
            
        Returns:
            smoothed_weights: (B, T, 2) å¹³æ»‘åçš„æƒé‡
        """
        if is_first_batch or self.temporal_smoothing <= 0:
            # ç¬¬ä¸€ä¸ªbatchæˆ–ä¸å¯ç”¨å¹³æ»‘
            self.prev_weights = current_weights[:, -1:, :].detach().clone()
            return current_weights
        
        B, T, _ = current_weights.shape
        
        # åˆ›å»ºå¹³æ»‘æƒé‡
        smoothed_weights = current_weights.clone()
        
        # å¯¹ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥åº”ç”¨å¹³æ»‘
        if self.prev_weights.shape[0] == B:  # æ‰¹æ¬¡å¤§å°åŒ¹é…
            smoothed_weights[:, 0:1, :] = (
                self.temporal_smoothing * self.prev_weights + 
                (1 - self.temporal_smoothing) * current_weights[:, 0:1, :]
            )
        
        # å¯¹åºåˆ—å†…éƒ¨åº”ç”¨å¹³æ»‘
        for t in range(1, T):
            smoothed_weights[:, t:t+1, :] = (
                self.temporal_smoothing * smoothed_weights[:, t-1:t, :] + 
                (1 - self.temporal_smoothing) * current_weights[:, t:t+1, :]
            )
        
        # æ›´æ–°prev_weights
        self.prev_weights = smoothed_weights[:, -1:, :].detach().clone()
        
        return smoothed_weights
    
    def forward(self, 
                critical_labels: torch.Tensor, 
                action_tokens: Optional[torch.Tensor] = None,
                is_first_batch: bool = False) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­ï¼šç”Ÿæˆè½¯è·¯ç”±æƒé‡
        
        Args:
            critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾
            action_tokens: (B, T, action_dim) å¯é€‰çš„åŠ¨ä½œtokensç”¨äºå¾®è°ƒ
            is_first_batch: æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªbatch
            
        Returns:
            DictåŒ…å«:
            - routing_weights: (B, T, 2) æœ€ç»ˆè·¯ç”±æƒé‡
            - base_weights: (B, T, 2) åŸºç¡€æƒé‡
            - adjusted_weights: (B, T, 2) å¾®è°ƒåæƒé‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            - statistics: å„ç§ç»Ÿè®¡ä¿¡æ¯
        """
        B, T = critical_labels.shape
        device = critical_labels.device
        
        # 1. è·å–åŸºç¡€æƒé‡
        base_weights = self.get_base_weights(critical_labels)
        
        # 2. ç¥ç»ç½‘ç»œå¾®è°ƒï¼ˆå¯é€‰ï¼‰
        if self.enable_neural_adjustment and action_tokens is not None:
            adjusted_weights = self.apply_neural_adjustment(base_weights, action_tokens)
        else:
            adjusted_weights = base_weights
        
        # 3. æ—¶åºå¹³æ»‘
        final_weights = self.apply_temporal_smoothing(adjusted_weights, is_first_batch)
        
        # 4. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        critical_mask = critical_labels.bool()
        non_critical_mask = ~critical_mask
        
        statistics = {
            'critical_ratio': critical_mask.float().mean().item(),
            'avg_global_weight': final_weights[:, :, 0].mean().item(),
            'avg_depth_weight': final_weights[:, :, 1].mean().item(),
            'weight_std_global': final_weights[:, :, 0].std().item(),
            'weight_std_depth': final_weights[:, :, 1].std().item(),
            'temperature': torch.clamp(self.temperature, min=0.1).item(),
        }
        
        # åˆ†ç±»ç»Ÿè®¡
        if critical_mask.any():
            critical_weights = final_weights[critical_mask]
            statistics.update({
                'critical_avg_global': critical_weights[:, 0].mean().item(),
                'critical_avg_depth': critical_weights[:, 1].mean().item(),
            })
        
        if non_critical_mask.any():
            non_critical_weights = final_weights[non_critical_mask]
            statistics.update({
                'non_critical_avg_global': non_critical_weights[:, 0].mean().item(),
                'non_critical_avg_depth': non_critical_weights[:, 1].mean().item(),
            })
        
        # å¾®è°ƒç»Ÿè®¡
        if self.enable_neural_adjustment and action_tokens is not None:
            weight_drift = torch.norm(adjusted_weights - base_weights, dim=-1).mean()
            statistics['weight_drift'] = weight_drift.item()
        
        return {
            'routing_weights': final_weights,
            'base_weights': base_weights,
            'adjusted_weights': adjusted_weights,
            'statistics': statistics
        }


class SimpleDualTeacherModel(nn.Module):
    """
    ç®€åŒ–çš„åŒæ•™å¸ˆå¯¹é½æ¨¡å‹
    
    é›†æˆè½¯è·¯ç”±æœºåˆ¶ï¼Œå®ç°åŠ¨æ€æƒé‡åˆ†é…çš„è§†è§‰å¯¹é½
    """
    
    def __init__(self,
                 action_dim: int = 2048,
                 dinov2_dim: int = 1024,
                 depth_dim: int = 1024,
                 router_config: Dict = None):
        """
        åˆå§‹åŒ–åŒæ•™å¸ˆæ¨¡å‹
        
        Args:
            action_dim: åŠ¨ä½œtokenç»´åº¦
            dinov2_dim: DINOv2ç‰¹å¾ç»´åº¦
            depth_dim: æ·±åº¦ç‰¹å¾ç»´åº¦
            router_config: è·¯ç”±å™¨é…ç½®
        """
        super().__init__()
        
        self.action_dim = action_dim
        self.dinov2_dim = dinov2_dim
        self.depth_dim = depth_dim
        
        # åˆ›å»ºè½¯è·¯ç”±å™¨
        if router_config is None:
            router_config = {
                'action_dim': action_dim,
                'critical_global_weight': 0.25,
                'critical_depth_weight': 0.75,
                'non_critical_global_weight': 0.75,
                'non_critical_depth_weight': 0.25,
                'enable_neural_adjustment': True,
                'temporal_smoothing': 0.9,
            }
        
        self.soft_router = BinaryLabelSoftRouter(**router_config)
        
        # ç‰¹å¾æŠ•å½±å™¨
        self.dinov2_projector = nn.Sequential(
            nn.Linear(dinov2_dim, (dinov2_dim + action_dim) // 2),
            nn.LayerNorm((dinov2_dim + action_dim) // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear((dinov2_dim + action_dim) // 2, action_dim),
            nn.LayerNorm(action_dim),
        )
        
        self.depth_projector = nn.Sequential(
            nn.Linear(depth_dim, (depth_dim + action_dim) // 2),
            nn.LayerNorm((depth_dim + action_dim) // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear((depth_dim + action_dim) // 2, action_dim),
            nn.LayerNorm(action_dim),
        )
        
        # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°ç”¨äºå¯¹æ¯”å­¦ä¹ 
        self.alignment_temperature = nn.Parameter(torch.tensor(0.07))
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for module in [self.dinov2_projector, self.depth_projector]:
            for layer in module:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight, gain=0.5)
                    if layer.bias is not None:
                        nn.init.constant_(layer.bias, 0)
    
    def compute_alignment_loss(self,
                             action_tokens: torch.Tensor,
                             dinov2_features: torch.Tensor,
                             depth_features: torch.Tensor,
                             routing_weights: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        è®¡ç®—åŠ æƒå¯¹é½æŸå¤±
        
        Args:
            action_tokens: (B, T, action_dim) åŠ¨ä½œtokens
            dinov2_features: (B, dinov2_dim) DINOv2å…¨å±€ç‰¹å¾
            depth_features: (B, depth_dim) æ·±åº¦ç‰¹å¾
            routing_weights: (B, T, 2) è·¯ç”±æƒé‡ [global_weight, depth_weight]
            
        Returns:
            æŸå¤±å­—å…¸
        """
        B, T, _ = action_tokens.shape
        
        # æŠ•å½±è§†è§‰ç‰¹å¾åˆ°åŠ¨ä½œç©ºé—´
        projected_dinov2 = self.dinov2_projector(dinov2_features)  # (B, action_dim)
        projected_depth = self.depth_projector(depth_features)     # (B, action_dim)
        
        # æ‰©å±•åˆ°æ—¶é—´ç»´åº¦
        projected_dinov2_expanded = projected_dinov2.unsqueeze(1).expand(-1, T, -1)
        projected_depth_expanded = projected_depth.unsqueeze(1).expand(-1, T, -1)
        
        # L2å½’ä¸€åŒ–
        action_norm = F.normalize(action_tokens, p=2, dim=-1)
        dinov2_norm = F.normalize(projected_dinov2_expanded, p=2, dim=-1)
        depth_norm = F.normalize(projected_depth_expanded, p=2, dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        global_similarity = torch.sum(action_norm * dinov2_norm, dim=-1)  # (B, T)
        depth_similarity = torch.sum(action_norm * depth_norm, dim=-1)    # (B, T)
        
        # æ¸©åº¦ç¼©æ”¾
        temp = torch.clamp(self.alignment_temperature, min=0.01)
        global_similarity_scaled = global_similarity / temp
        depth_similarity_scaled = depth_similarity / temp
        
        # è½¬æ¢ä¸ºæŸå¤±ï¼ˆæœ€å¤§åŒ–ç›¸ä¼¼åº¦ = æœ€å°åŒ–è´Ÿç›¸ä¼¼åº¦ï¼‰
        global_loss = 1.0 - global_similarity  # (B, T)
        depth_loss = 1.0 - depth_similarity    # (B, T)
        
        # åº”ç”¨è·¯ç”±æƒé‡
        weighted_global_loss = routing_weights[:, :, 0] * global_loss  # (B, T)
        weighted_depth_loss = routing_weights[:, :, 1] * depth_loss    # (B, T)
        
        # æ€»æŸå¤±
        total_alignment_loss = (weighted_global_loss + weighted_depth_loss).mean()
        
        # é¢å¤–çš„å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.compute_contrastive_loss(
            action_norm, dinov2_norm, depth_norm, routing_weights
        )
        
        # ç»„åˆæŸå¤±
        combined_loss = total_alignment_loss + 0.1 * contrastive_loss
        
        return {
            'total_loss': combined_loss,
            'alignment_loss': total_alignment_loss,
            'contrastive_loss': contrastive_loss,
            'global_loss_raw': global_loss.mean(),
            'depth_loss_raw': depth_loss.mean(),
            'weighted_global_loss': weighted_global_loss.mean(),
            'weighted_depth_loss': weighted_depth_loss.mean(),
            'global_similarity_avg': global_similarity.mean(),
            'depth_similarity_avg': depth_similarity.mean(),
            'alignment_temperature': temp.item(),
        }
    
    def compute_contrastive_loss(self,
                               action_norm: torch.Tensor,
                               dinov2_norm: torch.Tensor,
                               depth_norm: torch.Tensor,
                               routing_weights: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Args:
            action_norm: (B, T, D) å½’ä¸€åŒ–çš„åŠ¨ä½œç‰¹å¾
            dinov2_norm: (B, T, D) å½’ä¸€åŒ–çš„DINOv2ç‰¹å¾
            depth_norm: (B, T, D) å½’ä¸€åŒ–çš„æ·±åº¦ç‰¹å¾
            routing_weights: (B, T, 2) è·¯ç”±æƒé‡
            
        Returns:
            contrastive_loss: æ ‡é‡æŸå¤±
        """
        B, T, D = action_norm.shape
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡æ•°æ®ç±»å‹ä¸€è‡´
        target_dtype = action_norm.dtype
        dinov2_norm = dinov2_norm.to(dtype=target_dtype)
        depth_norm = depth_norm.to(dtype=target_dtype)
        routing_weights = routing_weights.to(dtype=target_dtype)
        
        # è®¡ç®—åŠ æƒçš„è§†è§‰è¡¨ç¤º
        weighted_visual = (
            routing_weights[:, :, 0:1] * dinov2_norm + 
            routing_weights[:, :, 1:2] * depth_norm
        )  # (B, T, D)
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ reshape è€Œä¸æ˜¯ viewï¼Œå¹¶ç¡®ä¿å¼ é‡è¿ç»­æ€§å’Œæ•°æ®ç±»å‹ä¸€è‡´
        action_flat = action_norm.contiguous().reshape(-1, D)  # (B*T, D)
        visual_flat = weighted_visual.contiguous().reshape(-1, D)  # (B*T, D)
        
        # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
        action_flat = action_flat.to(dtype=target_dtype)
        visual_flat = visual_flat.to(dtype=target_dtype)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(action_flat, visual_flat.transpose(-2, -1))  # (B*T, B*T)
        
        # å¯¹è§’çº¿ä¸ºæ­£æ ·æœ¬ï¼Œå…¶ä»–ä¸ºè´Ÿæ ·æœ¬
        labels = torch.arange(B * T, device=action_norm.device)
        
        # ç®€åŒ–çš„å¯¹æ¯”æŸå¤±ï¼šåªä½¿ç”¨å¯¹è§’çº¿æ­£æ ·æœ¬
        positive_sim = torch.diag(sim_matrix)  # (B*T,)
        
        # è®¡ç®—æ¯è¡Œçš„log-sum-expä½œä¸ºå½’ä¸€åŒ–é¡¹
        logsumexp_sim = torch.logsumexp(sim_matrix, dim=-1)  # (B*T,)
        
        # å¯¹æ¯”æŸå¤±ï¼š-log(exp(pos) / sum(exp(all)))
        contrastive_loss = -positive_sim + logsumexp_sim
        contrastive_loss = contrastive_loss.mean()
        
        return contrastive_loss
    
    def forward(self,
                action_tokens: torch.Tensor,
                dinov2_features: torch.Tensor,
                depth_features: torch.Tensor,
                critical_labels: torch.Tensor,
                is_first_batch: bool = False) -> Dict[str, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            action_tokens: (B, T, action_dim) åŠ¨ä½œtokens
            dinov2_features: (B, dinov2_dim) DINOv2ç‰¹å¾
            depth_features: (B, depth_dim) æ·±åº¦ç‰¹å¾
            critical_labels: (B, T) å…³é”®æ—¶é—´æ®µæ ‡ç­¾
            is_first_batch: æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªbatch
            
        Returns:
            å®Œæ•´ç»“æœå­—å…¸
        """
        # 1. ç”Ÿæˆè·¯ç”±æƒé‡
        routing_results = self.soft_router(
            critical_labels, 
            action_tokens, 
            is_first_batch
        )
        
        routing_weights = routing_results['routing_weights']
        
        # 2. è®¡ç®—å¯¹é½æŸå¤±
        alignment_results = self.compute_alignment_loss(
            action_tokens,
            dinov2_features,
            depth_features,
            routing_weights
        )
        
        # 3. åˆå¹¶ç»“æœ
        results = {
            **alignment_results,
            'routing_weights': routing_weights,
            'router_statistics': routing_results['statistics'],
            'base_weights': routing_results['base_weights'],
        }
        
        return results


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•åŸºäºå…³é”®æ—¶é—´æ®µçš„è½¯è·¯ç”±åŒæ•™å¸ˆæ¡†æ¶")
    
    # è®¾ç½®å‚æ•°
    B, T = 4, 64
    action_dim = 2048
    dinov2_dim = 1024
    depth_dim = 1024
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®...")
    action_tokens = torch.randn(B, T, action_dim)
    dinov2_features = torch.randn(B, dinov2_dim)
    depth_features = torch.randn(B, depth_dim)
    
    # åˆ›å»ºå…³é”®æ—¶é—´æ®µæ ‡ç­¾ï¼ˆæ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼‰
    critical_labels = torch.zeros(B, T, dtype=torch.long)
    for b in range(B):
        # æ¯ä¸ªåºåˆ—éšæœºé€‰æ‹©30%çš„æ—¶é—´æ­¥ä½œä¸ºå…³é”®æ—¶é—´æ®µ
        num_critical = int(T * 0.3)
        critical_indices = torch.randperm(T)[:num_critical]
        critical_labels[b, critical_indices] = 1
    
    print(f"   - åŠ¨ä½œtokens: {action_tokens.shape}")
    print(f"   - DINOv2ç‰¹å¾: {dinov2_features.shape}")
    print(f"   - æ·±åº¦ç‰¹å¾: {depth_features.shape}")
    print(f"   - å…³é”®æ—¶é—´æ®µæ ‡ç­¾: {critical_labels.shape}")
    print(f"   - å…³é”®æ—¶é—´æ®µæ¯”ä¾‹: {critical_labels.float().mean().item():.3f}")
    
    # æµ‹è¯•è½¯è·¯ç”±å™¨
    print("\nğŸ”€ æµ‹è¯•è½¯è·¯ç”±å™¨...")
    router_config = {
        'action_dim': action_dim,
        'critical_global_weight': 0.25,      # å…³é”®æ—¶é—´æ®µï¼šå…¨å±€25%
        'critical_depth_weight': 0.75,       # å…³é”®æ—¶é—´æ®µï¼šæ·±åº¦75%
        'non_critical_global_weight': 0.75,  # éå…³é”®æ—¶é—´æ®µï¼šå…¨å±€75%
        'non_critical_depth_weight': 0.25,   # éå…³é”®æ—¶é—´æ®µï¼šæ·±åº¦25%
        'enable_neural_adjustment': True,
        'temporal_smoothing': 0.9,
    }
    
    soft_router = BinaryLabelSoftRouter(**router_config)
    routing_results = soft_router(critical_labels, action_tokens, is_first_batch=True)
    
    print("âœ… è½¯è·¯ç”±å™¨æµ‹è¯•ç»“æœ:")
    print(f"   - è·¯ç”±æƒé‡å½¢çŠ¶: {routing_results['routing_weights'].shape}")
    stats = routing_results['statistics']
    print(f"   - å¹³å‡å…¨å±€æƒé‡: {stats['avg_global_weight']:.3f}")
    print(f"   - å¹³å‡æ·±åº¦æƒé‡: {stats['avg_depth_weight']:.3f}")
    if 'critical_avg_global' in stats:
        print(f"   - å…³é”®æ—¶é—´æ®µå…¨å±€æƒé‡: {stats['critical_avg_global']:.3f}")
        print(f"   - å…³é”®æ—¶é—´æ®µæ·±åº¦æƒé‡: {stats['critical_avg_depth']:.3f}")
    if 'non_critical_avg_global' in stats:
        print(f"   - éå…³é”®æ—¶é—´æ®µå…¨å±€æƒé‡: {stats['non_critical_avg_global']:.3f}")
        print(f"   - éå…³é”®æ—¶é—´æ®µæ·±åº¦æƒé‡: {stats['non_critical_avg_depth']:.3f}")
    
    # æµ‹è¯•å®Œæ•´åŒæ•™å¸ˆæ¨¡å‹
    print("\nğŸ¯ æµ‹è¯•å®Œæ•´åŒæ•™å¸ˆæ¨¡å‹...")
    dual_teacher_model = SimpleDualTeacherModel(
        action_dim=action_dim,
        dinov2_dim=dinov2_dim,
        depth_dim=depth_dim,
        router_config=router_config
    )
    
    results = dual_teacher_model(
        action_tokens,
        dinov2_features,
        depth_features,
        critical_labels,
        is_first_batch=True
    )
    
    print("âœ… åŒæ•™å¸ˆæ¨¡å‹æµ‹è¯•ç»“æœ:")
    print(f"   - æ€»æŸå¤±: {results['total_loss'].item():.4f}")
    print(f"   - å¯¹é½æŸå¤±: {results['alignment_loss'].item():.4f}")
    print(f"   - å¯¹æ¯”æŸå¤±: {results['contrastive_loss'].item():.4f}")
    print(f"   - å…¨å±€ç›¸ä¼¼åº¦: {results['global_similarity_avg'].item():.4f}")
    print(f"   - æ·±åº¦ç›¸ä¼¼åº¦: {results['depth_similarity_avg'].item():.4f}")
    print(f"   - åŠ æƒå…¨å±€æŸå¤±: {results['weighted_global_loss'].item():.4f}")
    print(f"   - åŠ æƒæ·±åº¦æŸå¤±: {results['weighted_depth_loss'].item():.4f}")
    
    # éªŒè¯æƒé‡åˆ†é…æ˜¯å¦ç¬¦åˆé¢„æœŸ
    print("\nğŸ“ˆ éªŒè¯æƒé‡åˆ†é…ç­–ç•¥...")
    routing_weights = results['routing_weights']
    critical_mask = critical_labels.bool()
    non_critical_mask = ~critical_mask
    
    if critical_mask.any():
        critical_weights = routing_weights[critical_mask]
        expected_global = router_config['critical_global_weight']
        expected_depth = router_config['critical_depth_weight']
        actual_global = critical_weights[:, 0].mean().item()
        actual_depth = critical_weights[:, 1].mean().item()
        
        print(f"   å…³é”®æ—¶é—´æ®µæƒé‡éªŒè¯:")
        print(f"     - æœŸæœ›å…¨å±€æƒé‡: {expected_global:.3f}, å®é™…: {actual_global:.3f}")
        print(f"     - æœŸæœ›æ·±åº¦æƒé‡: {expected_depth:.3f}, å®é™…: {actual_depth:.3f}")
        print(f"     - å…¨å±€æƒé‡è¯¯å·®: {abs(actual_global - expected_global):.3f}")
        print(f"     - æ·±åº¦æƒé‡è¯¯å·®: {abs(actual_depth - expected_depth):.3f}")
    
    if non_critical_mask.any():
        non_critical_weights = routing_weights[non_critical_mask]
        expected_global = router_config['non_critical_global_weight']
        expected_depth = router_config['non_critical_depth_weight']
        actual_global = non_critical_weights[:, 0].mean().item()
        actual_depth = non_critical_weights[:, 1].mean().item()
        
        print(f"   éå…³é”®æ—¶é—´æ®µæƒé‡éªŒè¯:")
        print(f"     - æœŸæœ›å…¨å±€æƒé‡: {expected_global:.3f}, å®é™…: {actual_global:.3f}")
        print(f"     - æœŸæœ›æ·±åº¦æƒé‡: {expected_depth:.3f}, å®é™…: {actual_depth:.3f}")
        print(f"     - å…¨å±€æƒé‡è¯¯å·®: {abs(actual_global - expected_global):.3f}")
        print(f"     - æ·±åº¦æƒé‡è¯¯å·®: {abs(actual_depth - expected_depth):.3f}")
    
    print("\nğŸŠ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è½¯è·¯ç”±åŒæ•™å¸ˆæ¡†æ¶è¿è¡Œæ­£å¸¸ã€‚")
    print("\nğŸ“‹ æ¡†æ¶ç‰¹ç‚¹æ€»ç»“:")
    print("   âœ“ åŸºäºå…³é”®æ—¶é—´æ®µæ ‡ç­¾çš„è§„åˆ™é©±åŠ¨æƒé‡åˆ†é…")
    print("   âœ“ å¯é€‰çš„ç¥ç»ç½‘ç»œå¾®è°ƒæœºåˆ¶")
    print("   âœ“ æ—¶åºå¹³æ»‘é˜²æ­¢æƒé‡çªå˜")
    print("   âœ“ å¯¹æ¯”å­¦ä¹ å¢å¼ºç‰¹å¾å¯¹é½")
    print("   âœ“ å®Œæ•´çš„ç»Ÿè®¡ä¿¡æ¯å’Œå¯è§£é‡Šæ€§")
    print("   âœ“ æ˜“äºé›†æˆåˆ°ç°æœ‰RDTæ¡†æ¶")