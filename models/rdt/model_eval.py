# models/rdt/model_eval.py - ä¸“é—¨ç”¨äºè¯„æµ‹çš„RDTæ¨¡å‹

from collections import OrderedDict
import torch
import torch.nn as nn

from models.rdt.blocks import (FinalLayer, RDTBlock, TimestepEmbedder,
                               get_1d_sincos_pos_embed_from_grid,
                               get_multimodal_cond_pos_embed)


class RDTEval(nn.Module):
    """
    ä¸“é—¨ç”¨äºè¯„æµ‹çš„RDTæ¨¡å‹
    ç§»é™¤äº†æ‰€æœ‰è®­ç»ƒæ—¶çš„REPAå¯¹é½ç»„ä»¶ï¼Œç¡®ä¿æ¨ç†æ—¶çš„çº¯å‡€æ€§
    """

    def __init__(
        self,
        output_dim=128,
        horizon=64,
        hidden_size=2048,
        depth=28,
        num_heads=32,
        max_lang_cond_len=1024,
        img_cond_len=4096,
        lang_pos_embed_config=None,
        img_pos_embed_config=None,
        dtype=torch.bfloat16,
    ):
        super().__init__()
        self.horizon = horizon
        self.hidden_size = hidden_size
        self.max_lang_cond_len = max_lang_cond_len
        self.img_cond_len = img_cond_len
        self.dtype = dtype
        self.lang_pos_embed_config = lang_pos_embed_config
        self.img_pos_embed_config = img_pos_embed_config

        print("ğŸ”§ åˆ›å»ºè¯„æµ‹ä¸“ç”¨RDTæ¨¡å‹ (æ— REPAç»„ä»¶)")

        # åµŒå…¥å™¨ç»„ä»¶
        self.t_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        self.freq_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        
        # ä½ç½®ç¼–ç å‚æ•°
        self.x_pos_embed = nn.Parameter(torch.zeros(1, horizon+3, hidden_size))
        self.lang_cond_pos_embed = nn.Parameter(torch.zeros(1, max_lang_cond_len, hidden_size))
        self.img_cond_pos_embed = nn.Parameter(torch.zeros(1, img_cond_len, hidden_size))

        # Transformerå—
        self.blocks = nn.ModuleList([
            RDTBlock(hidden_size, num_heads) for _ in range(depth)
        ])
        
        # æœ€ç»ˆå±‚
        self.final_layer = FinalLayer(hidden_size, output_dim)
        self.initialize_weights()

    def initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # åŸºç¡€æƒé‡åˆå§‹åŒ–
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        self.apply(_basic_init)

        # ä½ç½®ç¼–ç åˆå§‹åŒ–
        x_pos_embed = get_multimodal_cond_pos_embed(
            embed_dim=self.hidden_size,
            mm_cond_lens=OrderedDict([
                ('timestep', 1),
                ('ctrl_freq', 1),
                ('state', 1),
                ('action', self.horizon),
            ])
        )
        self.x_pos_embed.data.copy_(torch.from_numpy(x_pos_embed).float().unsqueeze(0))

        # è¯­è¨€æ¡ä»¶ä½ç½®ç¼–ç 
        if self.lang_pos_embed_config is None:
            lang_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(
                self.hidden_size, torch.arange(self.max_lang_cond_len))
        else:
            lang_cond_pos_embed = get_multimodal_cond_pos_embed(
                embed_dim=self.hidden_size,
                mm_cond_lens=OrderedDict(self.lang_pos_embed_config),
                embed_modality=False
            )
        self.lang_cond_pos_embed.data.copy_(
            torch.from_numpy(lang_cond_pos_embed).float().unsqueeze(0))
        
        # å›¾åƒæ¡ä»¶ä½ç½®ç¼–ç 
        if self.img_pos_embed_config is None:
            img_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(
                self.hidden_size, torch.arange(self.img_cond_len))
        else:
            img_cond_pos_embed = get_multimodal_cond_pos_embed(
                embed_dim=self.hidden_size,
                mm_cond_lens=OrderedDict(self.img_pos_embed_config),
                embed_modality=False
            )
        self.img_cond_pos_embed.data.copy_(
            torch.from_numpy(img_cond_pos_embed).float().unsqueeze(0))

        # æ—¶é—´æ­¥å’Œé¢‘ç‡åµŒå…¥å™¨åˆå§‹åŒ–
        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[2].weight, std=0.02)
        
        # ğŸ”§ æ¡ä»¶æ€§åˆå§‹åŒ–REPAç»„ä»¶
        if self.enable_repa_loss and not self.eval_mode:
            # DINOv2æŠ•å½±å™¨åˆå§‹åŒ–
            if hasattr(self, 'dinov2_to_action_projector'):
                for module in self.dinov2_to_action_projector:
                    if isinstance(module, nn.Linear):
                        nn.init.xavier_uniform_(module.weight, gain=0.5)
                        if module.bias is not None:
                            nn.init.constant_(module.bias, 0)
            
            # DepthæŠ•å½±å™¨åˆå§‹åŒ–
            if hasattr(self, 'depth_to_action_projector'):
                for module in self.depth_to_action_projector:
                    if isinstance(module, nn.Linear):
                        nn.init.xavier_uniform_(module.weight, gain=0.5)
                        if module.bias is not None:
                            nn.init.constant_(module.bias, 0)
        
        # è·¯ç”±ç½‘ç»œåˆå§‹åŒ–
        if self.use_dual_teachers and self.enable_repa_loss and not self.eval_mode:
            if hasattr(self, 'routing_network'):
                for module in self.routing_network:
                    if isinstance(module, nn.Linear):
                        nn.init.xavier_uniform_(module.weight, gain=0.5)
                        if module.bias is not None:
                            nn.init.constant_(module.bias, 0)
        
        # æœ€ç»ˆå±‚é›¶åˆå§‹åŒ–
        nn.init.constant_(self.final_layer.ffn_final.fc2.weight, 0)
        nn.init.constant_(self.final_layer.ffn_final.fc2.bias, 0)
        
        # è½¬æ¢åˆ°æŒ‡å®šæ•°æ®ç±»å‹
        self.to(self.dtype)

    def forward(self, x, freq, t, lang_c, img_c, lang_mask=None, img_mask=None):
        """
        å‰å‘ä¼ æ’­ï¼Œæ”¯æŒè®­ç»ƒæ¨¡å¼å’Œè¯„æµ‹æ¨¡å¼
        """
        # æ—¶é—´æ­¥å’Œé¢‘ç‡åµŒå…¥
        t = self.t_embedder(t).unsqueeze(1)             # (B, 1, D)
        freq = self.freq_embedder(freq).unsqueeze(1)    # (B, 1, D)
        
        # å¤„ç†æ—¶é—´æ­¥å¹¿æ’­
        if t.shape[0] == 1:
            t = t.expand(x.shape[0], -1, -1)
        x = torch.cat([t, freq, x], dim=1)               # (B, T+2, D)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.x_pos_embed
        lang_c = lang_c + self.lang_cond_pos_embed[:, :lang_c.shape[1]]
        img_c = img_c + self.img_cond_pos_embed

        # å­˜å‚¨ä¸­é—´æ¿€æ´»ç”¨äºREPAæŸå¤±ï¼ˆåªåœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
        intermediate_activations = {}
        
        # å‰å‘ä¼ æ’­é€šè¿‡transformerå—
        conds = [lang_c, img_c]
        masks = [lang_mask, img_mask]
        for i, block in enumerate(self.blocks):
            c, mask = conds[i%2], masks[i%2]
            x = block(x, c, mask)                       # (B, T+2, D)
            
            # ğŸ”§ åªåœ¨è®­ç»ƒæ¨¡å¼ä¸”å¯ç”¨REPAæ—¶æå–ä¸­é—´æ¿€æ´»
            if (not self.eval_mode and 
                self.enable_repa_loss and 
                i == self.repa_activation_layer):
                # æå–åŠ¨ä½œéƒ¨åˆ† (å»é™¤å‰ç¼€: timestep, freq, state)
                action_tokens = x[:, -self.horizon:, :]  # (B, horizon, hidden_size)
                intermediate_activations['action_tokens_for_repa'] = action_tokens
                
                # è®¡ç®—è·¯ç”±æƒé‡ï¼ˆå¦‚æœå¯ç”¨åŒæ•™å¸ˆæ¨¡å¼ï¼‰
                if self.use_dual_teachers and hasattr(self, 'routing_network'):
                    routing_logits = self.routing_network(action_tokens)  # (B, T, 2)
                    # åº”ç”¨æ¸©åº¦ç¼©æ”¾çš„softmax
                    routing_weights = torch.softmax(
                        routing_logits / torch.clamp(self.routing_temperature, min=0.1), 
                        dim=-1
                    )
                    intermediate_activations['routing_weights'] = routing_weights
                    intermediate_activations['routing_logits'] = routing_logits

        # æœ€ç»ˆè¾“å‡ºå±‚
        x = self.final_layer(x)                         # (B, T+2, output_dim)

        # åªä¿ç•™åŠ¨ä½œtoken
        x = x[:, -self.horizon:]
        
        return x, intermediate_activations

    def set_eval_mode(self, eval_mode=True):
        """è®¾ç½®è¯„æµ‹æ¨¡å¼"""
        self.eval_mode = eval_mode
        if eval_mode:
            print("ğŸ”§ RDTæ¨¡å‹åˆ‡æ¢åˆ°è¯„æµ‹æ¨¡å¼")
        else:
            print("ğŸ”§ RDTæ¨¡å‹åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼").img_cond_len))
        else:
            img_cond_pos_embed = get_multimodal_cond_pos_embed(
                embed_dim=self.hidden_size,
                mm_cond_lens=OrderedDict(self.img_pos_embed_config),
                embed_modality=False
            )
        self.img_cond_pos_embed.data.copy_(
            torch.from_numpy(img_cond_pos_embed).float().unsqueeze(0))

        # æ—¶é—´æ­¥å’Œé¢‘ç‡åµŒå…¥å™¨åˆå§‹åŒ–
        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[0].weight, std=0.02)
        nn.init.normal_(self.freq_embedder.mlp[2].weight, std=0.02)
        
        # æœ€ç»ˆå±‚é›¶åˆå§‹åŒ–
        nn.init.constant_(self.final_layer.ffn_final.fc2.weight, 0)
        nn.init.constant_(self.final_layer.ffn_final.fc2.bias, 0)
        
        # è½¬æ¢åˆ°æŒ‡å®šæ•°æ®ç±»å‹
        self.to(self.dtype)

    def forward(self, x, freq, t, lang_c, img_c, lang_mask=None, img_mask=None):
        """
        çº¯å‡€çš„å‰å‘ä¼ æ’­ï¼Œåªè¿”å›åŠ¨ä½œé¢„æµ‹
        """
        # æ—¶é—´æ­¥å’Œé¢‘ç‡åµŒå…¥
        t = self.t_embedder(t).unsqueeze(1)             # (B, 1, D)
        freq = self.freq_embedder(freq).unsqueeze(1)    # (B, 1, D)
        
        # å¤„ç†æ—¶é—´æ­¥å¹¿æ’­
        if t.shape[0] == 1:
            t = t.expand(x.shape[0], -1, -1)
        x = torch.cat([t, freq, x], dim=1)               # (B, T+2, D)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.x_pos_embed
        lang_c = lang_c + self.lang_cond_pos_embed[:, :lang_c.shape[1]]
        img_c = img_c + self.img_cond_pos_embed

        # å‰å‘ä¼ æ’­é€šè¿‡transformerå— - çº¯å‡€ç‰ˆæœ¬
        conds = [lang_c, img_c]
        masks = [lang_mask, img_mask]
        for i, block in enumerate(self.blocks):
            c, mask = conds[i%2], masks[i%2]
            x = block(x, c, mask)                       # (B, T+2, D)

        # æœ€ç»ˆè¾“å‡ºå±‚
        x = self.final_layer(x)                         # (B, T+2, output_dim)

        # åªä¿ç•™åŠ¨ä½œtokenï¼Œè¿”å›ç®€å•æ ¼å¼
        x = x[:, -self.horizon:]
        
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œè¿”å›ç©ºçš„ä¸­é—´æ¿€æ´»
        return x, {}


# ä¿®æ”¹åŸå§‹RDTæ¨¡å‹ï¼Œæ·»åŠ è¯„æµ‹æ¨¡å¼
class RDT(nn.Module):
    """
    åŸå§‹RDTæ¨¡å‹ï¼Œæ·»åŠ è¯„æµ‹æ¨¡å¼æ”¯æŒ
    """

    def __init__(
        self,
        output_dim=128,
        horizon=64,
        hidden_size=2048,
        depth=28,
        num_heads=32,
        max_lang_cond_len=1024,
        img_cond_len=4096,
        lang_pos_embed_config=None,
        img_pos_embed_config=None,
        dtype=torch.bfloat16,
        # REPAç›¸å…³å‚æ•°
        enable_repa_loss=True,
        repa_activation_layer=21,
        dinov2_feature_dim=1024,
        depth_feature_dim=1024,
        # åŒæ•™å¸ˆè·¯ç”±å‚æ•°
        use_dual_teachers=False,
        routing_hidden_dim=512,
        # ğŸ†• è¯„æµ‹æ¨¡å¼å‚æ•°
        eval_mode=False,
    ):
        super().__init__()
        self.horizon = horizon
        self.hidden_size = hidden_size
        self.max_lang_cond_len = max_lang_cond_len
        self.img_cond_len = img_cond_len
        self.dtype = dtype
        self.lang_pos_embed_config = lang_pos_embed_config
        self.img_pos_embed_config = img_pos_embed_config
        
        # ğŸ†• è¯„æµ‹æ¨¡å¼é…ç½®
        self.eval_mode = eval_mode
        if eval_mode:
            print("ğŸ”§ RDTæ¨¡å‹åˆå§‹åŒ–ä¸ºè¯„æµ‹æ¨¡å¼ (ç¦ç”¨REPA)")
            enable_repa_loss = False
            use_dual_teachers = False
        
        # REPAé…ç½®
        self.enable_repa_loss = enable_repa_loss
        self.repa_activation_layer = repa_activation_layer - 1 if enable_repa_loss else -1
        self.dinov2_feature_dim = dinov2_feature_dim
        self.depth_feature_dim = depth_feature_dim
        
        # åŒæ•™å¸ˆè·¯ç”±é…ç½®
        self.use_dual_teachers = use_dual_teachers

        # åµŒå…¥å™¨ç»„ä»¶
        self.t_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        self.freq_embedder = TimestepEmbedder(hidden_size, dtype=dtype)
        
        # ä½ç½®ç¼–ç å‚æ•°
        self.x_pos_embed = nn.Parameter(torch.zeros(1, horizon+3, hidden_size))
        self.lang_cond_pos_embed = nn.Parameter(torch.zeros(1, max_lang_cond_len, hidden_size))
        self.img_cond_pos_embed = nn.Parameter(torch.zeros(1, img_cond_len, hidden_size))

        # Transformerå—
        self.blocks = nn.ModuleList([
            RDTBlock(hidden_size, num_heads) for _ in range(depth)
        ])
        
        # ğŸ”§ æ¡ä»¶æ€§åˆ›å»ºREPAç›¸å…³ç»„ä»¶
        if self.enable_repa_loss and not eval_mode:
            # DINOv2å…¨å±€ç‰¹å¾æŠ•å½±å™¨
            self.dinov2_to_action_projector = nn.Sequential(
                nn.Linear(dinov2_feature_dim, (dinov2_feature_dim + hidden_size) // 2),
                nn.LayerNorm((dinov2_feature_dim + hidden_size) // 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear((dinov2_feature_dim + hidden_size) // 2, hidden_size),
                nn.LayerNorm(hidden_size),
            )
            
            # DepthAnythingV2æ·±åº¦ç‰¹å¾æŠ•å½±å™¨
            self.depth_to_action_projector = nn.Sequential(
                nn.Linear(depth_feature_dim, (depth_feature_dim + hidden_size) // 2),
                nn.LayerNorm((depth_feature_dim + hidden_size) // 2),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear((depth_feature_dim + hidden_size) // 2, hidden_size),
                nn.LayerNorm(hidden_size),
            )
        
        # åŒæ•™å¸ˆè·¯ç”±ç½‘ç»œ
        if self.use_dual_teachers and self.enable_repa_loss and not eval_mode:
            self.routing_network = nn.Sequential(
                nn.Linear(hidden_size, routing_hidden_dim),
                nn.LayerNorm(routing_hidden_dim),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(routing_hidden_dim, routing_hidden_dim),
                nn.LayerNorm(routing_hidden_dim),
                nn.GELU(),
                nn.Linear(routing_hidden_dim, 2),
            )
            
            # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°
            self.routing_temperature = nn.Parameter(torch.tensor(1.0))
        
        # æœ€ç»ˆå±‚
        self.final_layer = FinalLayer(hidden_size, output_dim)
        self.initialize_weights()

    def initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        # åŸºç¡€æƒé‡åˆå§‹åŒ–
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        self.apply(_basic_init)

        # ä½ç½®ç¼–ç åˆå§‹åŒ–
        x_pos_embed = get_multimodal_cond_pos_embed(
            embed_dim=self.hidden_size,
            mm_cond_lens=OrderedDict([
                ('timestep', 1),
                ('ctrl_freq', 1),
                ('state', 1),
                ('action', self.horizon),
            ])
        )
        self.x_pos_embed.data.copy_(torch.from_numpy(x_pos_embed).float().unsqueeze(0))

        # è¯­è¨€æ¡ä»¶ä½ç½®ç¼–ç 
        if self.lang_pos_embed_config is None:
            lang_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(
                self.hidden_size, torch.arange(self.max_lang_cond_len))
        else:
            lang_cond_pos_embed = get_multimodal_cond_pos_embed(
                embed_dim=self.hidden_size,
                mm_cond_lens=OrderedDict(self.lang_pos_embed_config),
                embed_modality=False
            )
        self.lang_cond_pos_embed.data.copy_(
            torch.from_numpy(lang_cond_pos_embed).float().unsqueeze(0))
        
        # å›¾åƒæ¡ä»¶ä½ç½®ç¼–ç 
        if self.img_pos_embed_config is None:
            img_cond_pos_embed = get_1d_sincos_pos_embed_from_grid(
                self.hidden_size, torch.arange(self